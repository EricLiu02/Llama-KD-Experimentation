{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.3316788321167885,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0023357664233576644,
      "grad_norm": 9.76717758178711,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.3263,
      "step": 1
    },
    {
      "epoch": 0.004671532846715329,
      "grad_norm": NaN,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.2879,
      "step": 2
    },
    {
      "epoch": 0.0070072992700729924,
      "grad_norm": 9.933398246765137,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.4199,
      "step": 3
    },
    {
      "epoch": 0.009343065693430658,
      "grad_norm": NaN,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.3573,
      "step": 4
    },
    {
      "epoch": 0.01167883211678832,
      "grad_norm": 8.508368492126465,
      "learning_rate": 6e-06,
      "loss": 2.3047,
      "step": 5
    },
    {
      "epoch": 0.014014598540145985,
      "grad_norm": 8.60690975189209,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.4392,
      "step": 6
    },
    {
      "epoch": 0.01635036496350365,
      "grad_norm": 8.965044975280762,
      "learning_rate": 1e-05,
      "loss": 2.2099,
      "step": 7
    },
    {
      "epoch": 0.018686131386861315,
      "grad_norm": 9.193801879882812,
      "learning_rate": 1.2e-05,
      "loss": 2.4131,
      "step": 8
    },
    {
      "epoch": 0.021021897810218976,
      "grad_norm": 10.027884483337402,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 2.3123,
      "step": 9
    },
    {
      "epoch": 0.02335766423357664,
      "grad_norm": 8.260871887207031,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.2102,
      "step": 10
    },
    {
      "epoch": 0.025693430656934305,
      "grad_norm": 7.53739595413208,
      "learning_rate": 1.8e-05,
      "loss": 1.9717,
      "step": 11
    },
    {
      "epoch": 0.02802919708029197,
      "grad_norm": 8.725610733032227,
      "learning_rate": 2e-05,
      "loss": 2.0262,
      "step": 12
    },
    {
      "epoch": 0.030364963503649634,
      "grad_norm": 8.109869956970215,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 2.2024,
      "step": 13
    },
    {
      "epoch": 0.0327007299270073,
      "grad_norm": 7.234158992767334,
      "learning_rate": 2.4e-05,
      "loss": 1.9096,
      "step": 14
    },
    {
      "epoch": 0.035036496350364967,
      "grad_norm": 7.191513538360596,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.7055,
      "step": 15
    },
    {
      "epoch": 0.03737226277372263,
      "grad_norm": 5.687394142150879,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.9948,
      "step": 16
    },
    {
      "epoch": 0.039708029197080295,
      "grad_norm": 5.609055519104004,
      "learning_rate": 3e-05,
      "loss": 1.7378,
      "step": 17
    },
    {
      "epoch": 0.04204379562043795,
      "grad_norm": 5.479762077331543,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.6213,
      "step": 18
    },
    {
      "epoch": 0.04437956204379562,
      "grad_norm": 4.654578685760498,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.5124,
      "step": 19
    },
    {
      "epoch": 0.04671532846715328,
      "grad_norm": 4.10599422454834,
      "learning_rate": 3.6e-05,
      "loss": 1.3159,
      "step": 20
    },
    {
      "epoch": 0.049051094890510946,
      "grad_norm": 4.380311965942383,
      "learning_rate": 3.8e-05,
      "loss": 1.2948,
      "step": 21
    },
    {
      "epoch": 0.05138686131386861,
      "grad_norm": 3.8523054122924805,
      "learning_rate": 4e-05,
      "loss": 1.4535,
      "step": 22
    },
    {
      "epoch": 0.053722627737226275,
      "grad_norm": 4.26491117477417,
      "learning_rate": 4.2e-05,
      "loss": 1.2584,
      "step": 23
    },
    {
      "epoch": 0.05605839416058394,
      "grad_norm": 3.7212021350860596,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.4821,
      "step": 24
    },
    {
      "epoch": 0.058394160583941604,
      "grad_norm": 4.034579277038574,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.2296,
      "step": 25
    },
    {
      "epoch": 0.06072992700729927,
      "grad_norm": 3.416475534439087,
      "learning_rate": 4.8e-05,
      "loss": 1.2752,
      "step": 26
    },
    {
      "epoch": 0.06306569343065693,
      "grad_norm": 4.776378631591797,
      "learning_rate": 5e-05,
      "loss": 1.029,
      "step": 27
    },
    {
      "epoch": 0.0654014598540146,
      "grad_norm": 4.425699234008789,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 1.178,
      "step": 28
    },
    {
      "epoch": 0.06773722627737226,
      "grad_norm": 4.365467548370361,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 0.8316,
      "step": 29
    },
    {
      "epoch": 0.07007299270072993,
      "grad_norm": 5.5824875831604,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 1.498,
      "step": 30
    },
    {
      "epoch": 0.07240875912408759,
      "grad_norm": 5.130398273468018,
      "learning_rate": 5.8e-05,
      "loss": 0.9368,
      "step": 31
    },
    {
      "epoch": 0.07474452554744526,
      "grad_norm": 6.309416770935059,
      "learning_rate": 6e-05,
      "loss": 1.1143,
      "step": 32
    },
    {
      "epoch": 0.07708029197080292,
      "grad_norm": 7.914453029632568,
      "learning_rate": 6.2e-05,
      "loss": 1.1405,
      "step": 33
    },
    {
      "epoch": 0.07941605839416059,
      "grad_norm": 5.542115688323975,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.9615,
      "step": 34
    },
    {
      "epoch": 0.08175182481751825,
      "grad_norm": 4.751394748687744,
      "learning_rate": 6.6e-05,
      "loss": 0.4923,
      "step": 35
    },
    {
      "epoch": 0.0840875912408759,
      "grad_norm": 5.898655414581299,
      "learning_rate": 6.800000000000001e-05,
      "loss": 1.0004,
      "step": 36
    },
    {
      "epoch": 0.08642335766423358,
      "grad_norm": 5.27031135559082,
      "learning_rate": 7e-05,
      "loss": 0.6363,
      "step": 37
    },
    {
      "epoch": 0.08875912408759123,
      "grad_norm": 11.979598045349121,
      "learning_rate": 7.2e-05,
      "loss": 1.3032,
      "step": 38
    },
    {
      "epoch": 0.0910948905109489,
      "grad_norm": 10.768462181091309,
      "learning_rate": 7.4e-05,
      "loss": 0.9158,
      "step": 39
    },
    {
      "epoch": 0.09343065693430656,
      "grad_norm": 4.440024375915527,
      "learning_rate": 7.6e-05,
      "loss": 0.6689,
      "step": 40
    },
    {
      "epoch": 0.09576642335766423,
      "grad_norm": 10.976005554199219,
      "learning_rate": 7.800000000000001e-05,
      "loss": 1.242,
      "step": 41
    },
    {
      "epoch": 0.09810218978102189,
      "grad_norm": 12.807488441467285,
      "learning_rate": 8e-05,
      "loss": 1.4006,
      "step": 42
    },
    {
      "epoch": 0.10043795620437956,
      "grad_norm": 12.16834831237793,
      "learning_rate": 8.2e-05,
      "loss": 1.0982,
      "step": 43
    },
    {
      "epoch": 0.10277372262773722,
      "grad_norm": 9.45134162902832,
      "learning_rate": 8.4e-05,
      "loss": 0.8025,
      "step": 44
    },
    {
      "epoch": 0.10510948905109489,
      "grad_norm": 9.13866901397705,
      "learning_rate": 8.6e-05,
      "loss": 0.9271,
      "step": 45
    },
    {
      "epoch": 0.10744525547445255,
      "grad_norm": 11.3162260055542,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.7856,
      "step": 46
    },
    {
      "epoch": 0.10978102189781022,
      "grad_norm": 15.61104679107666,
      "learning_rate": 9e-05,
      "loss": 0.8954,
      "step": 47
    },
    {
      "epoch": 0.11211678832116788,
      "grad_norm": 17.97649383544922,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.197,
      "step": 48
    },
    {
      "epoch": 0.11445255474452555,
      "grad_norm": 10.265968322753906,
      "learning_rate": 9.4e-05,
      "loss": 1.3696,
      "step": 49
    },
    {
      "epoch": 0.11678832116788321,
      "grad_norm": 8.953907012939453,
      "learning_rate": 9.6e-05,
      "loss": 1.0353,
      "step": 50
    },
    {
      "epoch": 0.11912408759124088,
      "grad_norm": 5.833664894104004,
      "learning_rate": 9.8e-05,
      "loss": 0.7492,
      "step": 51
    },
    {
      "epoch": 0.12145985401459854,
      "grad_norm": 7.175143718719482,
      "learning_rate": 0.0001,
      "loss": 0.8271,
      "step": 52
    },
    {
      "epoch": 0.12379562043795621,
      "grad_norm": 8.340107917785645,
      "learning_rate": 0.00010200000000000001,
      "loss": 1.1485,
      "step": 53
    },
    {
      "epoch": 0.12613138686131387,
      "grad_norm": 6.0470709800720215,
      "learning_rate": 0.00010400000000000001,
      "loss": 1.0915,
      "step": 54
    },
    {
      "epoch": 0.12846715328467154,
      "grad_norm": 6.645877361297607,
      "learning_rate": 0.00010600000000000002,
      "loss": 0.9332,
      "step": 55
    },
    {
      "epoch": 0.1308029197080292,
      "grad_norm": 4.835856914520264,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.4492,
      "step": 56
    },
    {
      "epoch": 0.13313868613138685,
      "grad_norm": 7.812933444976807,
      "learning_rate": 0.00011000000000000002,
      "loss": 1.0645,
      "step": 57
    },
    {
      "epoch": 0.13547445255474452,
      "grad_norm": 10.470976829528809,
      "learning_rate": 0.00011200000000000001,
      "loss": 1.2669,
      "step": 58
    },
    {
      "epoch": 0.1378102189781022,
      "grad_norm": 8.349514961242676,
      "learning_rate": 0.00011399999999999999,
      "loss": 1.2171,
      "step": 59
    },
    {
      "epoch": 0.14014598540145987,
      "grad_norm": 4.965411186218262,
      "learning_rate": 0.000116,
      "loss": 0.9432,
      "step": 60
    },
    {
      "epoch": 0.1424817518248175,
      "grad_norm": 5.7750043869018555,
      "learning_rate": 0.000118,
      "loss": 0.9212,
      "step": 61
    },
    {
      "epoch": 0.14481751824817518,
      "grad_norm": 6.971693992614746,
      "learning_rate": 0.00012,
      "loss": 1.1003,
      "step": 62
    },
    {
      "epoch": 0.14715328467153285,
      "grad_norm": 6.595733642578125,
      "learning_rate": 0.000122,
      "loss": 0.8723,
      "step": 63
    },
    {
      "epoch": 0.14948905109489052,
      "grad_norm": 6.896487712860107,
      "learning_rate": 0.000124,
      "loss": 1.0114,
      "step": 64
    },
    {
      "epoch": 0.15182481751824817,
      "grad_norm": 6.1925458908081055,
      "learning_rate": 0.000126,
      "loss": 0.8337,
      "step": 65
    },
    {
      "epoch": 0.15416058394160584,
      "grad_norm": 6.249763011932373,
      "learning_rate": 0.00012800000000000002,
      "loss": 1.1553,
      "step": 66
    },
    {
      "epoch": 0.1564963503649635,
      "grad_norm": 5.088858127593994,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.811,
      "step": 67
    },
    {
      "epoch": 0.15883211678832118,
      "grad_norm": 7.436530113220215,
      "learning_rate": 0.000132,
      "loss": 0.9339,
      "step": 68
    },
    {
      "epoch": 0.16116788321167883,
      "grad_norm": 6.778558254241943,
      "learning_rate": 0.000134,
      "loss": 0.9762,
      "step": 69
    },
    {
      "epoch": 0.1635036496350365,
      "grad_norm": 7.676880359649658,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.7444,
      "step": 70
    },
    {
      "epoch": 0.16583941605839417,
      "grad_norm": 9.037859916687012,
      "learning_rate": 0.000138,
      "loss": 0.9417,
      "step": 71
    },
    {
      "epoch": 0.1681751824817518,
      "grad_norm": 7.1711835861206055,
      "learning_rate": 0.00014,
      "loss": 0.7338,
      "step": 72
    },
    {
      "epoch": 0.17051094890510948,
      "grad_norm": 10.760369300842285,
      "learning_rate": 0.000142,
      "loss": 1.2352,
      "step": 73
    },
    {
      "epoch": 0.17284671532846715,
      "grad_norm": 13.642167091369629,
      "learning_rate": 0.000144,
      "loss": 1.3979,
      "step": 74
    },
    {
      "epoch": 0.17518248175182483,
      "grad_norm": 7.608661651611328,
      "learning_rate": 0.000146,
      "loss": 0.7229,
      "step": 75
    },
    {
      "epoch": 0.17751824817518247,
      "grad_norm": 5.433183670043945,
      "learning_rate": 0.000148,
      "loss": 0.9487,
      "step": 76
    },
    {
      "epoch": 0.17985401459854014,
      "grad_norm": 10.913125991821289,
      "learning_rate": 0.00015000000000000001,
      "loss": 1.1137,
      "step": 77
    },
    {
      "epoch": 0.1821897810218978,
      "grad_norm": 6.373745441436768,
      "learning_rate": 0.000152,
      "loss": 0.7081,
      "step": 78
    },
    {
      "epoch": 0.18452554744525548,
      "grad_norm": 4.796586036682129,
      "learning_rate": 0.000154,
      "loss": 0.7377,
      "step": 79
    },
    {
      "epoch": 0.18686131386861313,
      "grad_norm": 8.88888168334961,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.8333,
      "step": 80
    },
    {
      "epoch": 0.1891970802919708,
      "grad_norm": 8.389791488647461,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.7865,
      "step": 81
    },
    {
      "epoch": 0.19153284671532847,
      "grad_norm": 8.081562995910645,
      "learning_rate": 0.00016,
      "loss": 0.9762,
      "step": 82
    },
    {
      "epoch": 0.19386861313868614,
      "grad_norm": 14.985672950744629,
      "learning_rate": 0.000162,
      "loss": 1.3983,
      "step": 83
    },
    {
      "epoch": 0.19620437956204378,
      "grad_norm": 7.66095495223999,
      "learning_rate": 0.000164,
      "loss": 0.8917,
      "step": 84
    },
    {
      "epoch": 0.19854014598540146,
      "grad_norm": 7.894149303436279,
      "learning_rate": 0.000166,
      "loss": 1.1757,
      "step": 85
    },
    {
      "epoch": 0.20087591240875913,
      "grad_norm": 13.073752403259277,
      "learning_rate": 0.000168,
      "loss": 0.9258,
      "step": 86
    },
    {
      "epoch": 0.2032116788321168,
      "grad_norm": 7.305069923400879,
      "learning_rate": 0.00017,
      "loss": 1.1833,
      "step": 87
    },
    {
      "epoch": 0.20554744525547444,
      "grad_norm": 8.712993621826172,
      "learning_rate": 0.000172,
      "loss": 1.4098,
      "step": 88
    },
    {
      "epoch": 0.2078832116788321,
      "grad_norm": 7.470459938049316,
      "learning_rate": 0.000174,
      "loss": 0.8284,
      "step": 89
    },
    {
      "epoch": 0.21021897810218979,
      "grad_norm": 5.62532901763916,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.6869,
      "step": 90
    },
    {
      "epoch": 0.21255474452554746,
      "grad_norm": 5.659195899963379,
      "learning_rate": 0.00017800000000000002,
      "loss": 1.0071,
      "step": 91
    },
    {
      "epoch": 0.2148905109489051,
      "grad_norm": 5.023138046264648,
      "learning_rate": 0.00018,
      "loss": 0.9592,
      "step": 92
    },
    {
      "epoch": 0.21722627737226277,
      "grad_norm": 7.538593292236328,
      "learning_rate": 0.000182,
      "loss": 0.7795,
      "step": 93
    },
    {
      "epoch": 0.21956204379562044,
      "grad_norm": 6.767273902893066,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.1355,
      "step": 94
    },
    {
      "epoch": 0.22189781021897811,
      "grad_norm": 9.686171531677246,
      "learning_rate": 0.00018600000000000002,
      "loss": 1.0693,
      "step": 95
    },
    {
      "epoch": 0.22423357664233576,
      "grad_norm": 5.524252891540527,
      "learning_rate": 0.000188,
      "loss": 0.7764,
      "step": 96
    },
    {
      "epoch": 0.22656934306569343,
      "grad_norm": 4.117249488830566,
      "learning_rate": 0.00019,
      "loss": 0.8022,
      "step": 97
    },
    {
      "epoch": 0.2289051094890511,
      "grad_norm": 4.576656341552734,
      "learning_rate": 0.000192,
      "loss": 0.7795,
      "step": 98
    },
    {
      "epoch": 0.23124087591240877,
      "grad_norm": 5.930695056915283,
      "learning_rate": 0.000194,
      "loss": 0.9582,
      "step": 99
    },
    {
      "epoch": 0.23357664233576642,
      "grad_norm": 5.217170715332031,
      "learning_rate": 0.000196,
      "loss": 0.7543,
      "step": 100
    },
    {
      "epoch": 0.2359124087591241,
      "grad_norm": 9.055098533630371,
      "learning_rate": 0.00019800000000000002,
      "loss": 0.9395,
      "step": 101
    },
    {
      "epoch": 0.23824817518248176,
      "grad_norm": 6.4407057762146,
      "learning_rate": 0.0002,
      "loss": 0.6923,
      "step": 102
    },
    {
      "epoch": 0.24058394160583943,
      "grad_norm": 8.897411346435547,
      "learning_rate": 0.0001998310810810811,
      "loss": 1.0289,
      "step": 103
    },
    {
      "epoch": 0.24291970802919707,
      "grad_norm": 6.931639194488525,
      "learning_rate": 0.00019966216216216217,
      "loss": 0.8029,
      "step": 104
    },
    {
      "epoch": 0.24525547445255474,
      "grad_norm": 9.338128089904785,
      "learning_rate": 0.00019949324324324325,
      "loss": 0.6929,
      "step": 105
    },
    {
      "epoch": 0.24759124087591242,
      "grad_norm": 7.234033107757568,
      "learning_rate": 0.00019932432432432433,
      "loss": 0.9377,
      "step": 106
    },
    {
      "epoch": 0.24992700729927006,
      "grad_norm": 7.623779773712158,
      "learning_rate": 0.0001991554054054054,
      "loss": 1.0171,
      "step": 107
    },
    {
      "epoch": 0.25226277372262773,
      "grad_norm": 8.869211196899414,
      "learning_rate": 0.0001989864864864865,
      "loss": 1.1983,
      "step": 108
    },
    {
      "epoch": 0.2545985401459854,
      "grad_norm": 8.804340362548828,
      "learning_rate": 0.00019881756756756757,
      "loss": 1.4408,
      "step": 109
    },
    {
      "epoch": 0.2569343065693431,
      "grad_norm": 5.016317367553711,
      "learning_rate": 0.00019864864864864865,
      "loss": 0.2605,
      "step": 110
    },
    {
      "epoch": 0.2592700729927007,
      "grad_norm": 14.56962776184082,
      "learning_rate": 0.00019847972972972974,
      "loss": 1.1174,
      "step": 111
    },
    {
      "epoch": 0.2616058394160584,
      "grad_norm": 6.767775058746338,
      "learning_rate": 0.00019831081081081082,
      "loss": 0.9632,
      "step": 112
    },
    {
      "epoch": 0.26394160583941606,
      "grad_norm": 6.974334716796875,
      "learning_rate": 0.0001981418918918919,
      "loss": 1.2598,
      "step": 113
    },
    {
      "epoch": 0.2662773722627737,
      "grad_norm": 8.877135276794434,
      "learning_rate": 0.000197972972972973,
      "loss": 0.886,
      "step": 114
    },
    {
      "epoch": 0.2686131386861314,
      "grad_norm": 5.912685871124268,
      "learning_rate": 0.00019780405405405406,
      "loss": 0.8804,
      "step": 115
    },
    {
      "epoch": 0.27094890510948905,
      "grad_norm": 6.718365669250488,
      "learning_rate": 0.00019763513513513514,
      "loss": 0.8349,
      "step": 116
    },
    {
      "epoch": 0.2732846715328467,
      "grad_norm": 5.90163516998291,
      "learning_rate": 0.00019746621621621622,
      "loss": 0.5554,
      "step": 117
    },
    {
      "epoch": 0.2756204379562044,
      "grad_norm": 9.349019050598145,
      "learning_rate": 0.0001972972972972973,
      "loss": 1.0039,
      "step": 118
    },
    {
      "epoch": 0.27795620437956203,
      "grad_norm": 9.165647506713867,
      "learning_rate": 0.00019712837837837838,
      "loss": 0.9879,
      "step": 119
    },
    {
      "epoch": 0.28029197080291973,
      "grad_norm": 9.26611328125,
      "learning_rate": 0.00019695945945945946,
      "loss": 1.029,
      "step": 120
    },
    {
      "epoch": 0.2826277372262774,
      "grad_norm": 8.024558067321777,
      "learning_rate": 0.00019679054054054057,
      "loss": 1.2021,
      "step": 121
    },
    {
      "epoch": 0.284963503649635,
      "grad_norm": 6.188562870025635,
      "learning_rate": 0.00019662162162162162,
      "loss": 0.9071,
      "step": 122
    },
    {
      "epoch": 0.2872992700729927,
      "grad_norm": 10.846186637878418,
      "learning_rate": 0.0001964527027027027,
      "loss": 1.163,
      "step": 123
    },
    {
      "epoch": 0.28963503649635036,
      "grad_norm": 8.796976089477539,
      "learning_rate": 0.0001962837837837838,
      "loss": 0.8749,
      "step": 124
    },
    {
      "epoch": 0.291970802919708,
      "grad_norm": 10.872875213623047,
      "learning_rate": 0.0001961148648648649,
      "loss": 0.9021,
      "step": 125
    },
    {
      "epoch": 0.2943065693430657,
      "grad_norm": 6.358268737792969,
      "learning_rate": 0.00019594594594594594,
      "loss": 0.9134,
      "step": 126
    },
    {
      "epoch": 0.29664233576642335,
      "grad_norm": 8.116446495056152,
      "learning_rate": 0.00019577702702702703,
      "loss": 1.0335,
      "step": 127
    },
    {
      "epoch": 0.29897810218978105,
      "grad_norm": 7.334012508392334,
      "learning_rate": 0.00019560810810810813,
      "loss": 0.847,
      "step": 128
    },
    {
      "epoch": 0.3013138686131387,
      "grad_norm": 7.877804279327393,
      "learning_rate": 0.0001954391891891892,
      "loss": 1.2797,
      "step": 129
    },
    {
      "epoch": 0.30364963503649633,
      "grad_norm": 8.457817077636719,
      "learning_rate": 0.00019527027027027027,
      "loss": 1.028,
      "step": 130
    },
    {
      "epoch": 0.30598540145985403,
      "grad_norm": 6.420352458953857,
      "learning_rate": 0.00019510135135135138,
      "loss": 0.6641,
      "step": 131
    },
    {
      "epoch": 0.3083211678832117,
      "grad_norm": 6.714930057525635,
      "learning_rate": 0.00019493243243243246,
      "loss": 1.1209,
      "step": 132
    },
    {
      "epoch": 0.3106569343065693,
      "grad_norm": 5.645082950592041,
      "learning_rate": 0.0001947635135135135,
      "loss": 0.7638,
      "step": 133
    },
    {
      "epoch": 0.312992700729927,
      "grad_norm": 5.287239074707031,
      "learning_rate": 0.00019459459459459462,
      "loss": 0.782,
      "step": 134
    },
    {
      "epoch": 0.31532846715328466,
      "grad_norm": 4.4870805740356445,
      "learning_rate": 0.0001944256756756757,
      "loss": 0.5925,
      "step": 135
    },
    {
      "epoch": 0.31766423357664236,
      "grad_norm": 5.80476713180542,
      "learning_rate": 0.00019425675675675675,
      "loss": 0.7599,
      "step": 136
    },
    {
      "epoch": 0.32,
      "grad_norm": 7.054404258728027,
      "learning_rate": 0.00019408783783783783,
      "loss": 1.1151,
      "step": 137
    },
    {
      "epoch": 0.32233576642335765,
      "grad_norm": 6.7628655433654785,
      "learning_rate": 0.00019391891891891894,
      "loss": 0.8809,
      "step": 138
    },
    {
      "epoch": 0.32467153284671535,
      "grad_norm": 6.111440658569336,
      "learning_rate": 0.00019375000000000002,
      "loss": 0.8498,
      "step": 139
    },
    {
      "epoch": 0.327007299270073,
      "grad_norm": 10.185044288635254,
      "learning_rate": 0.00019358108108108107,
      "loss": 1.0391,
      "step": 140
    },
    {
      "epoch": 0.32934306569343064,
      "grad_norm": 5.051753520965576,
      "learning_rate": 0.00019341216216216218,
      "loss": 0.8515,
      "step": 141
    },
    {
      "epoch": 0.33167883211678834,
      "grad_norm": 6.770346641540527,
      "learning_rate": 0.00019324324324324326,
      "loss": 1.0253,
      "step": 142
    },
    {
      "epoch": 0.334014598540146,
      "grad_norm": 6.221109390258789,
      "learning_rate": 0.00019307432432432434,
      "loss": 0.6879,
      "step": 143
    },
    {
      "epoch": 0.3363503649635036,
      "grad_norm": 11.058600425720215,
      "learning_rate": 0.0001929054054054054,
      "loss": 0.793,
      "step": 144
    },
    {
      "epoch": 0.3386861313868613,
      "grad_norm": 7.7143707275390625,
      "learning_rate": 0.0001927364864864865,
      "loss": 0.6352,
      "step": 145
    },
    {
      "epoch": 0.34102189781021897,
      "grad_norm": 5.915372848510742,
      "learning_rate": 0.00019256756756756758,
      "loss": 0.7971,
      "step": 146
    },
    {
      "epoch": 0.34335766423357666,
      "grad_norm": 7.288167953491211,
      "learning_rate": 0.00019239864864864864,
      "loss": 0.8487,
      "step": 147
    },
    {
      "epoch": 0.3456934306569343,
      "grad_norm": 5.024692058563232,
      "learning_rate": 0.00019222972972972975,
      "loss": 0.7772,
      "step": 148
    },
    {
      "epoch": 0.34802919708029195,
      "grad_norm": 5.397745609283447,
      "learning_rate": 0.00019206081081081083,
      "loss": 0.8004,
      "step": 149
    },
    {
      "epoch": 0.35036496350364965,
      "grad_norm": 5.1174116134643555,
      "learning_rate": 0.0001918918918918919,
      "loss": 0.393,
      "step": 150
    },
    {
      "epoch": 0.3527007299270073,
      "grad_norm": 3.7918858528137207,
      "learning_rate": 0.000191722972972973,
      "loss": 0.6238,
      "step": 151
    },
    {
      "epoch": 0.35503649635036494,
      "grad_norm": 5.994518280029297,
      "learning_rate": 0.00019155405405405407,
      "loss": 0.7079,
      "step": 152
    },
    {
      "epoch": 0.35737226277372264,
      "grad_norm": 4.451195240020752,
      "learning_rate": 0.00019138513513513515,
      "loss": 0.6434,
      "step": 153
    },
    {
      "epoch": 0.3597080291970803,
      "grad_norm": 10.143388748168945,
      "learning_rate": 0.0001912162162162162,
      "loss": 0.9351,
      "step": 154
    },
    {
      "epoch": 0.362043795620438,
      "grad_norm": 6.0896124839782715,
      "learning_rate": 0.0001910472972972973,
      "loss": 0.7868,
      "step": 155
    },
    {
      "epoch": 0.3643795620437956,
      "grad_norm": 7.168572425842285,
      "learning_rate": 0.0001908783783783784,
      "loss": 0.7065,
      "step": 156
    },
    {
      "epoch": 0.36671532846715327,
      "grad_norm": 18.372459411621094,
      "learning_rate": 0.00019070945945945947,
      "loss": 2.0762,
      "step": 157
    },
    {
      "epoch": 0.36905109489051097,
      "grad_norm": 10.109853744506836,
      "learning_rate": 0.00019054054054054055,
      "loss": 0.891,
      "step": 158
    },
    {
      "epoch": 0.3713868613138686,
      "grad_norm": 7.8495659828186035,
      "learning_rate": 0.00019037162162162163,
      "loss": 1.1769,
      "step": 159
    },
    {
      "epoch": 0.37372262773722625,
      "grad_norm": 10.027345657348633,
      "learning_rate": 0.00019020270270270271,
      "loss": 0.8991,
      "step": 160
    },
    {
      "epoch": 0.37605839416058395,
      "grad_norm": 5.7884602546691895,
      "learning_rate": 0.0001900337837837838,
      "loss": 1.1326,
      "step": 161
    },
    {
      "epoch": 0.3783941605839416,
      "grad_norm": 5.5775837898254395,
      "learning_rate": 0.00018986486486486487,
      "loss": 1.3149,
      "step": 162
    },
    {
      "epoch": 0.3807299270072993,
      "grad_norm": 3.6914334297180176,
      "learning_rate": 0.00018969594594594596,
      "loss": 0.5384,
      "step": 163
    },
    {
      "epoch": 0.38306569343065694,
      "grad_norm": 5.164587020874023,
      "learning_rate": 0.00018952702702702704,
      "loss": 0.7099,
      "step": 164
    },
    {
      "epoch": 0.3854014598540146,
      "grad_norm": 5.736057281494141,
      "learning_rate": 0.00018935810810810812,
      "loss": 0.7099,
      "step": 165
    },
    {
      "epoch": 0.3877372262773723,
      "grad_norm": 5.7522101402282715,
      "learning_rate": 0.0001891891891891892,
      "loss": 0.9778,
      "step": 166
    },
    {
      "epoch": 0.3900729927007299,
      "grad_norm": 3.4977025985717773,
      "learning_rate": 0.00018902027027027028,
      "loss": 0.6505,
      "step": 167
    },
    {
      "epoch": 0.39240875912408757,
      "grad_norm": 7.347283840179443,
      "learning_rate": 0.00018885135135135136,
      "loss": 1.4042,
      "step": 168
    },
    {
      "epoch": 0.39474452554744527,
      "grad_norm": 7.347468376159668,
      "learning_rate": 0.00018868243243243244,
      "loss": 0.7796,
      "step": 169
    },
    {
      "epoch": 0.3970802919708029,
      "grad_norm": 9.90487003326416,
      "learning_rate": 0.00018851351351351352,
      "loss": 0.7585,
      "step": 170
    },
    {
      "epoch": 0.3994160583941606,
      "grad_norm": 8.078313827514648,
      "learning_rate": 0.00018834459459459463,
      "loss": 0.9114,
      "step": 171
    },
    {
      "epoch": 0.40175182481751825,
      "grad_norm": 4.924508094787598,
      "learning_rate": 0.00018817567567567568,
      "loss": 1.1475,
      "step": 172
    },
    {
      "epoch": 0.4040875912408759,
      "grad_norm": 7.172532558441162,
      "learning_rate": 0.00018800675675675676,
      "loss": 1.0826,
      "step": 173
    },
    {
      "epoch": 0.4064233576642336,
      "grad_norm": 5.172428131103516,
      "learning_rate": 0.00018783783783783784,
      "loss": 0.8076,
      "step": 174
    },
    {
      "epoch": 0.40875912408759124,
      "grad_norm": 6.745422840118408,
      "learning_rate": 0.00018766891891891892,
      "loss": 0.7352,
      "step": 175
    },
    {
      "epoch": 0.4110948905109489,
      "grad_norm": 10.078023910522461,
      "learning_rate": 0.0001875,
      "loss": 0.8279,
      "step": 176
    },
    {
      "epoch": 0.4134306569343066,
      "grad_norm": 6.1680121421813965,
      "learning_rate": 0.00018733108108108108,
      "loss": 0.669,
      "step": 177
    },
    {
      "epoch": 0.4157664233576642,
      "grad_norm": 7.203061103820801,
      "learning_rate": 0.0001871621621621622,
      "loss": 1.0645,
      "step": 178
    },
    {
      "epoch": 0.41810218978102187,
      "grad_norm": 6.270352363586426,
      "learning_rate": 0.00018699324324324325,
      "loss": 1.0771,
      "step": 179
    },
    {
      "epoch": 0.42043795620437957,
      "grad_norm": 7.831157684326172,
      "learning_rate": 0.00018682432432432433,
      "loss": 0.7996,
      "step": 180
    },
    {
      "epoch": 0.4227737226277372,
      "grad_norm": 6.860781192779541,
      "learning_rate": 0.0001866554054054054,
      "loss": 0.998,
      "step": 181
    },
    {
      "epoch": 0.4251094890510949,
      "grad_norm": 7.368956089019775,
      "learning_rate": 0.0001864864864864865,
      "loss": 0.7725,
      "step": 182
    },
    {
      "epoch": 0.42744525547445256,
      "grad_norm": 7.807478427886963,
      "learning_rate": 0.00018631756756756757,
      "loss": 0.8673,
      "step": 183
    },
    {
      "epoch": 0.4297810218978102,
      "grad_norm": 4.579418659210205,
      "learning_rate": 0.00018614864864864865,
      "loss": 0.7306,
      "step": 184
    },
    {
      "epoch": 0.4321167883211679,
      "grad_norm": 6.845966339111328,
      "learning_rate": 0.00018597972972972976,
      "loss": 1.1914,
      "step": 185
    },
    {
      "epoch": 0.43445255474452554,
      "grad_norm": 11.325923919677734,
      "learning_rate": 0.0001858108108108108,
      "loss": 0.7132,
      "step": 186
    },
    {
      "epoch": 0.4367883211678832,
      "grad_norm": 9.796159744262695,
      "learning_rate": 0.0001856418918918919,
      "loss": 0.842,
      "step": 187
    },
    {
      "epoch": 0.4391240875912409,
      "grad_norm": 10.890971183776855,
      "learning_rate": 0.000185472972972973,
      "loss": 1.1087,
      "step": 188
    },
    {
      "epoch": 0.44145985401459853,
      "grad_norm": 4.55857515335083,
      "learning_rate": 0.00018530405405405408,
      "loss": 0.7255,
      "step": 189
    },
    {
      "epoch": 0.44379562043795623,
      "grad_norm": 8.275824546813965,
      "learning_rate": 0.00018513513513513513,
      "loss": 0.8712,
      "step": 190
    },
    {
      "epoch": 0.44613138686131387,
      "grad_norm": 8.487767219543457,
      "learning_rate": 0.0001849662162162162,
      "loss": 1.0917,
      "step": 191
    },
    {
      "epoch": 0.4484671532846715,
      "grad_norm": 7.717424392700195,
      "learning_rate": 0.00018479729729729732,
      "loss": 0.9892,
      "step": 192
    },
    {
      "epoch": 0.4508029197080292,
      "grad_norm": 6.149257659912109,
      "learning_rate": 0.00018462837837837837,
      "loss": 0.877,
      "step": 193
    },
    {
      "epoch": 0.45313868613138686,
      "grad_norm": 6.252261638641357,
      "learning_rate": 0.00018445945945945946,
      "loss": 0.9551,
      "step": 194
    },
    {
      "epoch": 0.4554744525547445,
      "grad_norm": 3.983856439590454,
      "learning_rate": 0.00018429054054054056,
      "loss": 0.5765,
      "step": 195
    },
    {
      "epoch": 0.4578102189781022,
      "grad_norm": 9.997297286987305,
      "learning_rate": 0.00018412162162162164,
      "loss": 0.7578,
      "step": 196
    },
    {
      "epoch": 0.46014598540145984,
      "grad_norm": 5.603611469268799,
      "learning_rate": 0.0001839527027027027,
      "loss": 1.1738,
      "step": 197
    },
    {
      "epoch": 0.46248175182481754,
      "grad_norm": 8.280428886413574,
      "learning_rate": 0.0001837837837837838,
      "loss": 0.8793,
      "step": 198
    },
    {
      "epoch": 0.4648175182481752,
      "grad_norm": 6.784937381744385,
      "learning_rate": 0.00018361486486486489,
      "loss": 0.8694,
      "step": 199
    },
    {
      "epoch": 0.46715328467153283,
      "grad_norm": 7.550201416015625,
      "learning_rate": 0.00018344594594594594,
      "loss": 0.6676,
      "step": 200
    },
    {
      "epoch": 0.46948905109489053,
      "grad_norm": 7.951096057891846,
      "learning_rate": 0.00018327702702702702,
      "loss": 1.3596,
      "step": 201
    },
    {
      "epoch": 0.4718248175182482,
      "grad_norm": 9.186018943786621,
      "learning_rate": 0.00018310810810810813,
      "loss": 1.2706,
      "step": 202
    },
    {
      "epoch": 0.4741605839416058,
      "grad_norm": 4.871204376220703,
      "learning_rate": 0.0001829391891891892,
      "loss": 0.7022,
      "step": 203
    },
    {
      "epoch": 0.4764963503649635,
      "grad_norm": 5.399316787719727,
      "learning_rate": 0.00018277027027027026,
      "loss": 0.8087,
      "step": 204
    },
    {
      "epoch": 0.47883211678832116,
      "grad_norm": 6.477816104888916,
      "learning_rate": 0.00018260135135135137,
      "loss": 0.7937,
      "step": 205
    },
    {
      "epoch": 0.48116788321167886,
      "grad_norm": 6.4984450340271,
      "learning_rate": 0.00018243243243243245,
      "loss": 0.9979,
      "step": 206
    },
    {
      "epoch": 0.4835036496350365,
      "grad_norm": 8.913121223449707,
      "learning_rate": 0.00018226351351351353,
      "loss": 0.7981,
      "step": 207
    },
    {
      "epoch": 0.48583941605839415,
      "grad_norm": 7.515475273132324,
      "learning_rate": 0.0001820945945945946,
      "loss": 0.9245,
      "step": 208
    },
    {
      "epoch": 0.48817518248175185,
      "grad_norm": 5.051977634429932,
      "learning_rate": 0.0001819256756756757,
      "loss": 0.5655,
      "step": 209
    },
    {
      "epoch": 0.4905109489051095,
      "grad_norm": 7.855276107788086,
      "learning_rate": 0.00018175675675675677,
      "loss": 0.9896,
      "step": 210
    },
    {
      "epoch": 0.49284671532846713,
      "grad_norm": 8.824816703796387,
      "learning_rate": 0.00018158783783783783,
      "loss": 0.7908,
      "step": 211
    },
    {
      "epoch": 0.49518248175182483,
      "grad_norm": 8.008698463439941,
      "learning_rate": 0.00018141891891891893,
      "loss": 0.8653,
      "step": 212
    },
    {
      "epoch": 0.4975182481751825,
      "grad_norm": 4.864081859588623,
      "learning_rate": 0.00018125000000000001,
      "loss": 0.7948,
      "step": 213
    },
    {
      "epoch": 0.4998540145985401,
      "grad_norm": 12.952052116394043,
      "learning_rate": 0.0001810810810810811,
      "loss": 1.0148,
      "step": 214
    },
    {
      "epoch": 0.5021897810218978,
      "grad_norm": 6.782063961029053,
      "learning_rate": 0.00018091216216216218,
      "loss": 0.95,
      "step": 215
    },
    {
      "epoch": 0.5045255474452555,
      "grad_norm": 7.568595886230469,
      "learning_rate": 0.00018074324324324326,
      "loss": 0.672,
      "step": 216
    },
    {
      "epoch": 0.5068613138686131,
      "grad_norm": 4.8678178787231445,
      "learning_rate": 0.00018057432432432434,
      "loss": 0.6524,
      "step": 217
    },
    {
      "epoch": 0.5091970802919707,
      "grad_norm": 8.519765853881836,
      "learning_rate": 0.0001804054054054054,
      "loss": 1.2237,
      "step": 218
    },
    {
      "epoch": 0.5115328467153285,
      "grad_norm": 11.88292121887207,
      "learning_rate": 0.0001802364864864865,
      "loss": 1.2592,
      "step": 219
    },
    {
      "epoch": 0.5138686131386861,
      "grad_norm": 5.470284938812256,
      "learning_rate": 0.00018006756756756758,
      "loss": 0.6473,
      "step": 220
    },
    {
      "epoch": 0.5162043795620438,
      "grad_norm": 7.077305316925049,
      "learning_rate": 0.00017989864864864866,
      "loss": 0.7643,
      "step": 221
    },
    {
      "epoch": 0.5185401459854014,
      "grad_norm": 6.918737888336182,
      "learning_rate": 0.00017972972972972974,
      "loss": 0.5367,
      "step": 222
    },
    {
      "epoch": 0.5208759124087591,
      "grad_norm": 7.401599407196045,
      "learning_rate": 0.00017956081081081082,
      "loss": 0.7267,
      "step": 223
    },
    {
      "epoch": 0.5232116788321168,
      "grad_norm": 8.486220359802246,
      "learning_rate": 0.0001793918918918919,
      "loss": 0.6337,
      "step": 224
    },
    {
      "epoch": 0.5255474452554745,
      "grad_norm": 20.685081481933594,
      "learning_rate": 0.00017922297297297298,
      "loss": 0.9759,
      "step": 225
    },
    {
      "epoch": 0.5278832116788321,
      "grad_norm": 13.061893463134766,
      "learning_rate": 0.00017905405405405406,
      "loss": 0.9329,
      "step": 226
    },
    {
      "epoch": 0.5302189781021898,
      "grad_norm": 9.72519302368164,
      "learning_rate": 0.00017888513513513514,
      "loss": 0.9816,
      "step": 227
    },
    {
      "epoch": 0.5325547445255474,
      "grad_norm": 9.00983715057373,
      "learning_rate": 0.00017871621621621622,
      "loss": 1.3165,
      "step": 228
    },
    {
      "epoch": 0.5348905109489052,
      "grad_norm": 8.176162719726562,
      "learning_rate": 0.0001785472972972973,
      "loss": 0.6555,
      "step": 229
    },
    {
      "epoch": 0.5372262773722628,
      "grad_norm": 4.326785564422607,
      "learning_rate": 0.00017837837837837839,
      "loss": 0.6892,
      "step": 230
    },
    {
      "epoch": 0.5395620437956204,
      "grad_norm": 10.554978370666504,
      "learning_rate": 0.00017820945945945947,
      "loss": 1.0114,
      "step": 231
    },
    {
      "epoch": 0.5418978102189781,
      "grad_norm": 5.989541053771973,
      "learning_rate": 0.00017804054054054055,
      "loss": 0.8565,
      "step": 232
    },
    {
      "epoch": 0.5442335766423357,
      "grad_norm": 8.618585586547852,
      "learning_rate": 0.00017787162162162163,
      "loss": 0.6432,
      "step": 233
    },
    {
      "epoch": 0.5465693430656934,
      "grad_norm": 5.997701644897461,
      "learning_rate": 0.0001777027027027027,
      "loss": 0.7218,
      "step": 234
    },
    {
      "epoch": 0.5489051094890511,
      "grad_norm": 7.321142673492432,
      "learning_rate": 0.00017753378378378382,
      "loss": 1.0598,
      "step": 235
    },
    {
      "epoch": 0.5512408759124088,
      "grad_norm": 4.576878547668457,
      "learning_rate": 0.00017736486486486487,
      "loss": 0.5024,
      "step": 236
    },
    {
      "epoch": 0.5535766423357664,
      "grad_norm": 7.182010650634766,
      "learning_rate": 0.00017719594594594595,
      "loss": 0.7857,
      "step": 237
    },
    {
      "epoch": 0.5559124087591241,
      "grad_norm": 7.499270915985107,
      "learning_rate": 0.00017702702702702703,
      "loss": 1.1323,
      "step": 238
    },
    {
      "epoch": 0.5582481751824817,
      "grad_norm": 8.689299583435059,
      "learning_rate": 0.0001768581081081081,
      "loss": 1.1335,
      "step": 239
    },
    {
      "epoch": 0.5605839416058395,
      "grad_norm": 5.17193603515625,
      "learning_rate": 0.0001766891891891892,
      "loss": 0.4839,
      "step": 240
    },
    {
      "epoch": 0.5629197080291971,
      "grad_norm": 8.735675811767578,
      "learning_rate": 0.00017652027027027027,
      "loss": 0.8988,
      "step": 241
    },
    {
      "epoch": 0.5652554744525548,
      "grad_norm": 7.43065071105957,
      "learning_rate": 0.00017635135135135138,
      "loss": 0.9226,
      "step": 242
    },
    {
      "epoch": 0.5675912408759124,
      "grad_norm": 5.490161895751953,
      "learning_rate": 0.00017618243243243243,
      "loss": 0.9966,
      "step": 243
    },
    {
      "epoch": 0.56992700729927,
      "grad_norm": 7.090993404388428,
      "learning_rate": 0.00017601351351351351,
      "loss": 1.0016,
      "step": 244
    },
    {
      "epoch": 0.5722627737226277,
      "grad_norm": 3.55344557762146,
      "learning_rate": 0.00017584459459459462,
      "loss": 0.7438,
      "step": 245
    },
    {
      "epoch": 0.5745985401459854,
      "grad_norm": 6.823838233947754,
      "learning_rate": 0.00017567567567567568,
      "loss": 0.8278,
      "step": 246
    },
    {
      "epoch": 0.5769343065693431,
      "grad_norm": 3.7239747047424316,
      "learning_rate": 0.00017550675675675676,
      "loss": 0.6253,
      "step": 247
    },
    {
      "epoch": 0.5792700729927007,
      "grad_norm": 5.685385227203369,
      "learning_rate": 0.00017533783783783784,
      "loss": 0.7159,
      "step": 248
    },
    {
      "epoch": 0.5816058394160584,
      "grad_norm": 7.284355163574219,
      "learning_rate": 0.00017516891891891894,
      "loss": 0.8958,
      "step": 249
    },
    {
      "epoch": 0.583941605839416,
      "grad_norm": 6.769615650177002,
      "learning_rate": 0.000175,
      "loss": 1.1116,
      "step": 250
    },
    {
      "epoch": 0.5862773722627738,
      "grad_norm": 5.0462212562561035,
      "learning_rate": 0.00017483108108108108,
      "loss": 0.7842,
      "step": 251
    },
    {
      "epoch": 0.5886131386861314,
      "grad_norm": 9.280317306518555,
      "learning_rate": 0.0001746621621621622,
      "loss": 0.8448,
      "step": 252
    },
    {
      "epoch": 0.590948905109489,
      "grad_norm": 5.857150554656982,
      "learning_rate": 0.00017449324324324327,
      "loss": 0.8661,
      "step": 253
    },
    {
      "epoch": 0.5932846715328467,
      "grad_norm": 7.021173477172852,
      "learning_rate": 0.00017432432432432432,
      "loss": 0.6199,
      "step": 254
    },
    {
      "epoch": 0.5956204379562043,
      "grad_norm": 7.9153828620910645,
      "learning_rate": 0.0001741554054054054,
      "loss": 0.8851,
      "step": 255
    },
    {
      "epoch": 0.5979562043795621,
      "grad_norm": 7.113411903381348,
      "learning_rate": 0.0001739864864864865,
      "loss": 0.8817,
      "step": 256
    },
    {
      "epoch": 0.6002919708029197,
      "grad_norm": 8.672229766845703,
      "learning_rate": 0.00017381756756756756,
      "loss": 0.9125,
      "step": 257
    },
    {
      "epoch": 0.6026277372262774,
      "grad_norm": 6.525421142578125,
      "learning_rate": 0.00017364864864864864,
      "loss": 0.8732,
      "step": 258
    },
    {
      "epoch": 0.604963503649635,
      "grad_norm": 9.126590728759766,
      "learning_rate": 0.00017347972972972975,
      "loss": 1.1359,
      "step": 259
    },
    {
      "epoch": 0.6072992700729927,
      "grad_norm": 6.255928039550781,
      "learning_rate": 0.00017331081081081083,
      "loss": 0.9102,
      "step": 260
    },
    {
      "epoch": 0.6096350364963503,
      "grad_norm": 7.980057239532471,
      "learning_rate": 0.00017314189189189189,
      "loss": 0.8507,
      "step": 261
    },
    {
      "epoch": 0.6119708029197081,
      "grad_norm": 4.8253278732299805,
      "learning_rate": 0.000172972972972973,
      "loss": 0.8229,
      "step": 262
    },
    {
      "epoch": 0.6143065693430657,
      "grad_norm": 5.306454658508301,
      "learning_rate": 0.00017280405405405407,
      "loss": 0.9239,
      "step": 263
    },
    {
      "epoch": 0.6166423357664234,
      "grad_norm": 6.088669300079346,
      "learning_rate": 0.00017263513513513513,
      "loss": 0.568,
      "step": 264
    },
    {
      "epoch": 0.618978102189781,
      "grad_norm": 6.465721130371094,
      "learning_rate": 0.0001724662162162162,
      "loss": 0.7942,
      "step": 265
    },
    {
      "epoch": 0.6213138686131386,
      "grad_norm": 8.688125610351562,
      "learning_rate": 0.00017229729729729732,
      "loss": 1.384,
      "step": 266
    },
    {
      "epoch": 0.6236496350364964,
      "grad_norm": 5.229392051696777,
      "learning_rate": 0.0001721283783783784,
      "loss": 0.7895,
      "step": 267
    },
    {
      "epoch": 0.625985401459854,
      "grad_norm": 7.516857624053955,
      "learning_rate": 0.00017195945945945945,
      "loss": 0.7055,
      "step": 268
    },
    {
      "epoch": 0.6283211678832117,
      "grad_norm": 7.79861307144165,
      "learning_rate": 0.00017179054054054056,
      "loss": 1.236,
      "step": 269
    },
    {
      "epoch": 0.6306569343065693,
      "grad_norm": 6.08745813369751,
      "learning_rate": 0.00017162162162162164,
      "loss": 0.8224,
      "step": 270
    },
    {
      "epoch": 0.632992700729927,
      "grad_norm": 5.55251407623291,
      "learning_rate": 0.00017145270270270272,
      "loss": 0.9449,
      "step": 271
    },
    {
      "epoch": 0.6353284671532847,
      "grad_norm": 4.175854206085205,
      "learning_rate": 0.0001712837837837838,
      "loss": 0.8865,
      "step": 272
    },
    {
      "epoch": 0.6376642335766424,
      "grad_norm": 4.975333213806152,
      "learning_rate": 0.00017111486486486488,
      "loss": 1.2677,
      "step": 273
    },
    {
      "epoch": 0.64,
      "grad_norm": 3.028534173965454,
      "learning_rate": 0.00017094594594594596,
      "loss": 0.6574,
      "step": 274
    },
    {
      "epoch": 0.6423357664233577,
      "grad_norm": 4.715255260467529,
      "learning_rate": 0.00017077702702702701,
      "loss": 1.1611,
      "step": 275
    },
    {
      "epoch": 0.6446715328467153,
      "grad_norm": 4.393507480621338,
      "learning_rate": 0.00017060810810810812,
      "loss": 0.7634,
      "step": 276
    },
    {
      "epoch": 0.6470072992700729,
      "grad_norm": 7.324012279510498,
      "learning_rate": 0.0001704391891891892,
      "loss": 1.0548,
      "step": 277
    },
    {
      "epoch": 0.6493430656934307,
      "grad_norm": 4.038607597351074,
      "learning_rate": 0.00017027027027027028,
      "loss": 0.8227,
      "step": 278
    },
    {
      "epoch": 0.6516788321167883,
      "grad_norm": 6.0033040046691895,
      "learning_rate": 0.00017010135135135136,
      "loss": 1.0804,
      "step": 279
    },
    {
      "epoch": 0.654014598540146,
      "grad_norm": 4.98785400390625,
      "learning_rate": 0.00016993243243243244,
      "loss": 0.7015,
      "step": 280
    },
    {
      "epoch": 0.6563503649635036,
      "grad_norm": 5.445507526397705,
      "learning_rate": 0.00016976351351351353,
      "loss": 0.9446,
      "step": 281
    },
    {
      "epoch": 0.6586861313868613,
      "grad_norm": 6.65606164932251,
      "learning_rate": 0.0001695945945945946,
      "loss": 1.0668,
      "step": 282
    },
    {
      "epoch": 0.661021897810219,
      "grad_norm": 4.1828155517578125,
      "learning_rate": 0.00016942567567567569,
      "loss": 0.9106,
      "step": 283
    },
    {
      "epoch": 0.6633576642335767,
      "grad_norm": 5.656001091003418,
      "learning_rate": 0.00016925675675675677,
      "loss": 0.8617,
      "step": 284
    },
    {
      "epoch": 0.6656934306569343,
      "grad_norm": 5.887742042541504,
      "learning_rate": 0.00016908783783783785,
      "loss": 0.5795,
      "step": 285
    },
    {
      "epoch": 0.668029197080292,
      "grad_norm": 6.032003402709961,
      "learning_rate": 0.00016891891891891893,
      "loss": 1.0344,
      "step": 286
    },
    {
      "epoch": 0.6703649635036496,
      "grad_norm": 5.492703437805176,
      "learning_rate": 0.00016875,
      "loss": 0.672,
      "step": 287
    },
    {
      "epoch": 0.6727007299270072,
      "grad_norm": 3.2970850467681885,
      "learning_rate": 0.0001685810810810811,
      "loss": 0.966,
      "step": 288
    },
    {
      "epoch": 0.675036496350365,
      "grad_norm": 6.699465274810791,
      "learning_rate": 0.00016841216216216217,
      "loss": 0.6205,
      "step": 289
    },
    {
      "epoch": 0.6773722627737226,
      "grad_norm": 7.343521595001221,
      "learning_rate": 0.00016824324324324325,
      "loss": 0.8969,
      "step": 290
    },
    {
      "epoch": 0.6797080291970803,
      "grad_norm": 7.261515140533447,
      "learning_rate": 0.00016807432432432433,
      "loss": 0.9129,
      "step": 291
    },
    {
      "epoch": 0.6820437956204379,
      "grad_norm": 6.892227649688721,
      "learning_rate": 0.0001679054054054054,
      "loss": 1.1318,
      "step": 292
    },
    {
      "epoch": 0.6843795620437956,
      "grad_norm": 7.926883697509766,
      "learning_rate": 0.0001677364864864865,
      "loss": 0.8271,
      "step": 293
    },
    {
      "epoch": 0.6867153284671533,
      "grad_norm": 4.8008012771606445,
      "learning_rate": 0.00016756756756756757,
      "loss": 0.7046,
      "step": 294
    },
    {
      "epoch": 0.689051094890511,
      "grad_norm": 4.793013095855713,
      "learning_rate": 0.00016739864864864865,
      "loss": 0.5709,
      "step": 295
    },
    {
      "epoch": 0.6913868613138686,
      "grad_norm": 5.09293794631958,
      "learning_rate": 0.00016722972972972973,
      "loss": 1.0127,
      "step": 296
    },
    {
      "epoch": 0.6937226277372263,
      "grad_norm": 4.682775497436523,
      "learning_rate": 0.00016706081081081082,
      "loss": 0.8423,
      "step": 297
    },
    {
      "epoch": 0.6960583941605839,
      "grad_norm": 4.691804885864258,
      "learning_rate": 0.0001668918918918919,
      "loss": 0.6599,
      "step": 298
    },
    {
      "epoch": 0.6983941605839417,
      "grad_norm": 8.835844039916992,
      "learning_rate": 0.000166722972972973,
      "loss": 0.9323,
      "step": 299
    },
    {
      "epoch": 0.7007299270072993,
      "grad_norm": 6.146981716156006,
      "learning_rate": 0.00016655405405405406,
      "loss": 0.6158,
      "step": 300
    },
    {
      "epoch": 0.703065693430657,
      "grad_norm": 5.380880832672119,
      "learning_rate": 0.00016638513513513514,
      "loss": 0.5139,
      "step": 301
    },
    {
      "epoch": 0.7054014598540146,
      "grad_norm": 8.264299392700195,
      "learning_rate": 0.00016621621621621622,
      "loss": 1.0692,
      "step": 302
    },
    {
      "epoch": 0.7077372262773722,
      "grad_norm": 6.496553897857666,
      "learning_rate": 0.0001660472972972973,
      "loss": 0.6987,
      "step": 303
    },
    {
      "epoch": 0.7100729927007299,
      "grad_norm": 9.955521583557129,
      "learning_rate": 0.00016587837837837838,
      "loss": 0.696,
      "step": 304
    },
    {
      "epoch": 0.7124087591240876,
      "grad_norm": 16.92171287536621,
      "learning_rate": 0.00016570945945945946,
      "loss": 0.8457,
      "step": 305
    },
    {
      "epoch": 0.7147445255474453,
      "grad_norm": 6.554502010345459,
      "learning_rate": 0.00016554054054054057,
      "loss": 0.8135,
      "step": 306
    },
    {
      "epoch": 0.7170802919708029,
      "grad_norm": 9.270560264587402,
      "learning_rate": 0.00016537162162162162,
      "loss": 0.5735,
      "step": 307
    },
    {
      "epoch": 0.7194160583941606,
      "grad_norm": 7.268645763397217,
      "learning_rate": 0.0001652027027027027,
      "loss": 1.198,
      "step": 308
    },
    {
      "epoch": 0.7217518248175182,
      "grad_norm": 9.993047714233398,
      "learning_rate": 0.0001650337837837838,
      "loss": 0.7896,
      "step": 309
    },
    {
      "epoch": 0.724087591240876,
      "grad_norm": 6.460206508636475,
      "learning_rate": 0.00016486486486486486,
      "loss": 0.7623,
      "step": 310
    },
    {
      "epoch": 0.7264233576642336,
      "grad_norm": 7.854243755340576,
      "learning_rate": 0.00016469594594594594,
      "loss": 0.9919,
      "step": 311
    },
    {
      "epoch": 0.7287591240875912,
      "grad_norm": 4.489086627960205,
      "learning_rate": 0.00016452702702702702,
      "loss": 0.7164,
      "step": 312
    },
    {
      "epoch": 0.7310948905109489,
      "grad_norm": 7.015450954437256,
      "learning_rate": 0.00016435810810810813,
      "loss": 0.823,
      "step": 313
    },
    {
      "epoch": 0.7334306569343065,
      "grad_norm": 4.799137115478516,
      "learning_rate": 0.00016418918918918919,
      "loss": 0.9381,
      "step": 314
    },
    {
      "epoch": 0.7357664233576642,
      "grad_norm": 5.323049545288086,
      "learning_rate": 0.00016402027027027027,
      "loss": 0.8213,
      "step": 315
    },
    {
      "epoch": 0.7381021897810219,
      "grad_norm": 7.019449710845947,
      "learning_rate": 0.00016385135135135137,
      "loss": 1.0483,
      "step": 316
    },
    {
      "epoch": 0.7404379562043796,
      "grad_norm": 5.833044528961182,
      "learning_rate": 0.00016368243243243246,
      "loss": 0.8337,
      "step": 317
    },
    {
      "epoch": 0.7427737226277372,
      "grad_norm": 5.048829078674316,
      "learning_rate": 0.0001635135135135135,
      "loss": 0.7274,
      "step": 318
    },
    {
      "epoch": 0.7451094890510949,
      "grad_norm": 5.442798137664795,
      "learning_rate": 0.00016334459459459462,
      "loss": 0.6042,
      "step": 319
    },
    {
      "epoch": 0.7474452554744525,
      "grad_norm": 4.064268589019775,
      "learning_rate": 0.0001631756756756757,
      "loss": 0.6147,
      "step": 320
    },
    {
      "epoch": 0.7497810218978103,
      "grad_norm": 4.270107746124268,
      "learning_rate": 0.00016300675675675675,
      "loss": 0.7816,
      "step": 321
    },
    {
      "epoch": 0.7521167883211679,
      "grad_norm": 5.698283672332764,
      "learning_rate": 0.00016283783783783783,
      "loss": 0.8064,
      "step": 322
    },
    {
      "epoch": 0.7544525547445255,
      "grad_norm": 6.7690958976745605,
      "learning_rate": 0.00016266891891891894,
      "loss": 0.8092,
      "step": 323
    },
    {
      "epoch": 0.7567883211678832,
      "grad_norm": 4.075102806091309,
      "learning_rate": 0.00016250000000000002,
      "loss": 0.6885,
      "step": 324
    },
    {
      "epoch": 0.7591240875912408,
      "grad_norm": 6.659181118011475,
      "learning_rate": 0.00016233108108108107,
      "loss": 0.8739,
      "step": 325
    },
    {
      "epoch": 0.7614598540145986,
      "grad_norm": 5.9917497634887695,
      "learning_rate": 0.00016216216216216218,
      "loss": 0.7213,
      "step": 326
    },
    {
      "epoch": 0.7637956204379562,
      "grad_norm": 4.441450119018555,
      "learning_rate": 0.00016199324324324326,
      "loss": 0.7009,
      "step": 327
    },
    {
      "epoch": 0.7661313868613139,
      "grad_norm": 9.212785720825195,
      "learning_rate": 0.00016182432432432432,
      "loss": 1.3656,
      "step": 328
    },
    {
      "epoch": 0.7684671532846715,
      "grad_norm": 10.078849792480469,
      "learning_rate": 0.0001616554054054054,
      "loss": 0.7179,
      "step": 329
    },
    {
      "epoch": 0.7708029197080292,
      "grad_norm": 10.91972827911377,
      "learning_rate": 0.0001614864864864865,
      "loss": 1.0507,
      "step": 330
    },
    {
      "epoch": 0.7731386861313868,
      "grad_norm": 7.4001078605651855,
      "learning_rate": 0.00016131756756756758,
      "loss": 0.7725,
      "step": 331
    },
    {
      "epoch": 0.7754744525547446,
      "grad_norm": 6.0486626625061035,
      "learning_rate": 0.00016114864864864864,
      "loss": 0.7395,
      "step": 332
    },
    {
      "epoch": 0.7778102189781022,
      "grad_norm": 7.311551570892334,
      "learning_rate": 0.00016097972972972975,
      "loss": 1.087,
      "step": 333
    },
    {
      "epoch": 0.7801459854014599,
      "grad_norm": 10.66413402557373,
      "learning_rate": 0.00016081081081081083,
      "loss": 1.2797,
      "step": 334
    },
    {
      "epoch": 0.7824817518248175,
      "grad_norm": 6.228906631469727,
      "learning_rate": 0.0001606418918918919,
      "loss": 0.8993,
      "step": 335
    },
    {
      "epoch": 0.7848175182481751,
      "grad_norm": 3.7792913913726807,
      "learning_rate": 0.000160472972972973,
      "loss": 0.5535,
      "step": 336
    },
    {
      "epoch": 0.7871532846715329,
      "grad_norm": 5.10250997543335,
      "learning_rate": 0.00016030405405405407,
      "loss": 0.7973,
      "step": 337
    },
    {
      "epoch": 0.7894890510948905,
      "grad_norm": 3.457181930541992,
      "learning_rate": 0.00016013513513513515,
      "loss": 0.6189,
      "step": 338
    },
    {
      "epoch": 0.7918248175182482,
      "grad_norm": 5.377668857574463,
      "learning_rate": 0.0001599662162162162,
      "loss": 0.6027,
      "step": 339
    },
    {
      "epoch": 0.7941605839416058,
      "grad_norm": 6.977315425872803,
      "learning_rate": 0.0001597972972972973,
      "loss": 0.6975,
      "step": 340
    },
    {
      "epoch": 0.7964963503649635,
      "grad_norm": 6.955214977264404,
      "learning_rate": 0.0001596283783783784,
      "loss": 1.271,
      "step": 341
    },
    {
      "epoch": 0.7988321167883212,
      "grad_norm": 6.99512243270874,
      "learning_rate": 0.00015945945945945947,
      "loss": 1.1319,
      "step": 342
    },
    {
      "epoch": 0.8011678832116789,
      "grad_norm": 5.151858806610107,
      "learning_rate": 0.00015929054054054055,
      "loss": 0.4887,
      "step": 343
    },
    {
      "epoch": 0.8035036496350365,
      "grad_norm": 11.7971830368042,
      "learning_rate": 0.00015912162162162163,
      "loss": 0.8569,
      "step": 344
    },
    {
      "epoch": 0.8058394160583942,
      "grad_norm": 5.387551307678223,
      "learning_rate": 0.0001589527027027027,
      "loss": 0.6216,
      "step": 345
    },
    {
      "epoch": 0.8081751824817518,
      "grad_norm": 9.573450088500977,
      "learning_rate": 0.0001587837837837838,
      "loss": 1.392,
      "step": 346
    },
    {
      "epoch": 0.8105109489051094,
      "grad_norm": 4.430175304412842,
      "learning_rate": 0.00015861486486486487,
      "loss": 0.8176,
      "step": 347
    },
    {
      "epoch": 0.8128467153284672,
      "grad_norm": 6.214131832122803,
      "learning_rate": 0.00015844594594594595,
      "loss": 0.507,
      "step": 348
    },
    {
      "epoch": 0.8151824817518248,
      "grad_norm": 7.961795330047607,
      "learning_rate": 0.00015827702702702704,
      "loss": 0.9719,
      "step": 349
    },
    {
      "epoch": 0.8175182481751825,
      "grad_norm": 6.70178747177124,
      "learning_rate": 0.00015810810810810812,
      "loss": 0.6853,
      "step": 350
    },
    {
      "epoch": 0.8198540145985401,
      "grad_norm": 6.289235591888428,
      "learning_rate": 0.0001579391891891892,
      "loss": 1.1924,
      "step": 351
    },
    {
      "epoch": 0.8221897810218978,
      "grad_norm": 7.149569511413574,
      "learning_rate": 0.00015777027027027028,
      "loss": 0.8455,
      "step": 352
    },
    {
      "epoch": 0.8245255474452555,
      "grad_norm": 4.02401876449585,
      "learning_rate": 0.00015760135135135136,
      "loss": 1.1547,
      "step": 353
    },
    {
      "epoch": 0.8268613138686132,
      "grad_norm": 5.4473185539245605,
      "learning_rate": 0.00015743243243243244,
      "loss": 0.9227,
      "step": 354
    },
    {
      "epoch": 0.8291970802919708,
      "grad_norm": 8.248241424560547,
      "learning_rate": 0.00015726351351351352,
      "loss": 0.9317,
      "step": 355
    },
    {
      "epoch": 0.8315328467153285,
      "grad_norm": 4.032402515411377,
      "learning_rate": 0.0001570945945945946,
      "loss": 0.732,
      "step": 356
    },
    {
      "epoch": 0.8338686131386861,
      "grad_norm": 4.60549259185791,
      "learning_rate": 0.00015692567567567568,
      "loss": 0.9608,
      "step": 357
    },
    {
      "epoch": 0.8362043795620437,
      "grad_norm": 4.3628411293029785,
      "learning_rate": 0.00015675675675675676,
      "loss": 0.6747,
      "step": 358
    },
    {
      "epoch": 0.8385401459854015,
      "grad_norm": 6.763551235198975,
      "learning_rate": 0.00015658783783783784,
      "loss": 0.7684,
      "step": 359
    },
    {
      "epoch": 0.8408759124087591,
      "grad_norm": 6.007474899291992,
      "learning_rate": 0.00015641891891891892,
      "loss": 0.607,
      "step": 360
    },
    {
      "epoch": 0.8432116788321168,
      "grad_norm": 6.087243556976318,
      "learning_rate": 0.00015625,
      "loss": 1.2015,
      "step": 361
    },
    {
      "epoch": 0.8455474452554744,
      "grad_norm": 8.502033233642578,
      "learning_rate": 0.00015608108108108108,
      "loss": 0.867,
      "step": 362
    },
    {
      "epoch": 0.8478832116788321,
      "grad_norm": 4.433197021484375,
      "learning_rate": 0.0001559121621621622,
      "loss": 0.9245,
      "step": 363
    },
    {
      "epoch": 0.8502189781021898,
      "grad_norm": 7.1652512550354,
      "learning_rate": 0.00015574324324324325,
      "loss": 0.591,
      "step": 364
    },
    {
      "epoch": 0.8525547445255475,
      "grad_norm": 7.019618511199951,
      "learning_rate": 0.00015557432432432433,
      "loss": 1.1106,
      "step": 365
    },
    {
      "epoch": 0.8548905109489051,
      "grad_norm": 4.165480613708496,
      "learning_rate": 0.0001554054054054054,
      "loss": 0.9148,
      "step": 366
    },
    {
      "epoch": 0.8572262773722628,
      "grad_norm": 7.7417778968811035,
      "learning_rate": 0.0001552364864864865,
      "loss": 0.8469,
      "step": 367
    },
    {
      "epoch": 0.8595620437956204,
      "grad_norm": 8.0143404006958,
      "learning_rate": 0.00015506756756756757,
      "loss": 0.4893,
      "step": 368
    },
    {
      "epoch": 0.8618978102189782,
      "grad_norm": 4.316339015960693,
      "learning_rate": 0.00015489864864864865,
      "loss": 0.9265,
      "step": 369
    },
    {
      "epoch": 0.8642335766423358,
      "grad_norm": 7.385110855102539,
      "learning_rate": 0.00015472972972972976,
      "loss": 0.8126,
      "step": 370
    },
    {
      "epoch": 0.8665693430656934,
      "grad_norm": 5.231488227844238,
      "learning_rate": 0.0001545608108108108,
      "loss": 0.592,
      "step": 371
    },
    {
      "epoch": 0.8689051094890511,
      "grad_norm": 6.054327964782715,
      "learning_rate": 0.0001543918918918919,
      "loss": 0.6435,
      "step": 372
    },
    {
      "epoch": 0.8712408759124087,
      "grad_norm": 11.147046089172363,
      "learning_rate": 0.000154222972972973,
      "loss": 1.0894,
      "step": 373
    },
    {
      "epoch": 0.8735766423357664,
      "grad_norm": 6.036360740661621,
      "learning_rate": 0.00015405405405405405,
      "loss": 0.7502,
      "step": 374
    },
    {
      "epoch": 0.8759124087591241,
      "grad_norm": 6.561180591583252,
      "learning_rate": 0.00015388513513513513,
      "loss": 0.8006,
      "step": 375
    },
    {
      "epoch": 0.8782481751824818,
      "grad_norm": 12.836221694946289,
      "learning_rate": 0.0001537162162162162,
      "loss": 1.2646,
      "step": 376
    },
    {
      "epoch": 0.8805839416058394,
      "grad_norm": 4.795337200164795,
      "learning_rate": 0.00015354729729729732,
      "loss": 0.6364,
      "step": 377
    },
    {
      "epoch": 0.8829197080291971,
      "grad_norm": 5.505937099456787,
      "learning_rate": 0.00015337837837837837,
      "loss": 0.7103,
      "step": 378
    },
    {
      "epoch": 0.8852554744525547,
      "grad_norm": 4.98874044418335,
      "learning_rate": 0.00015320945945945945,
      "loss": 0.7259,
      "step": 379
    },
    {
      "epoch": 0.8875912408759125,
      "grad_norm": 5.42160701751709,
      "learning_rate": 0.00015304054054054056,
      "loss": 0.966,
      "step": 380
    },
    {
      "epoch": 0.8899270072992701,
      "grad_norm": 6.994714260101318,
      "learning_rate": 0.00015287162162162164,
      "loss": 0.7391,
      "step": 381
    },
    {
      "epoch": 0.8922627737226277,
      "grad_norm": 2.8942959308624268,
      "learning_rate": 0.0001527027027027027,
      "loss": 0.5394,
      "step": 382
    },
    {
      "epoch": 0.8945985401459854,
      "grad_norm": 3.9224424362182617,
      "learning_rate": 0.0001525337837837838,
      "loss": 0.3641,
      "step": 383
    },
    {
      "epoch": 0.896934306569343,
      "grad_norm": 5.543798446655273,
      "learning_rate": 0.00015236486486486489,
      "loss": 0.8011,
      "step": 384
    },
    {
      "epoch": 0.8992700729927007,
      "grad_norm": 4.648947715759277,
      "learning_rate": 0.00015219594594594594,
      "loss": 0.8672,
      "step": 385
    },
    {
      "epoch": 0.9016058394160584,
      "grad_norm": 9.867694854736328,
      "learning_rate": 0.00015202702702702702,
      "loss": 1.6051,
      "step": 386
    },
    {
      "epoch": 0.9039416058394161,
      "grad_norm": 7.496989727020264,
      "learning_rate": 0.00015185810810810813,
      "loss": 0.8033,
      "step": 387
    },
    {
      "epoch": 0.9062773722627737,
      "grad_norm": 4.73930549621582,
      "learning_rate": 0.0001516891891891892,
      "loss": 0.7754,
      "step": 388
    },
    {
      "epoch": 0.9086131386861314,
      "grad_norm": 5.197925090789795,
      "learning_rate": 0.00015152027027027026,
      "loss": 0.7721,
      "step": 389
    },
    {
      "epoch": 0.910948905109489,
      "grad_norm": 9.855984687805176,
      "learning_rate": 0.00015135135135135137,
      "loss": 1.163,
      "step": 390
    },
    {
      "epoch": 0.9132846715328468,
      "grad_norm": 10.318248748779297,
      "learning_rate": 0.00015118243243243245,
      "loss": 0.9304,
      "step": 391
    },
    {
      "epoch": 0.9156204379562044,
      "grad_norm": 5.263314723968506,
      "learning_rate": 0.0001510135135135135,
      "loss": 0.7539,
      "step": 392
    },
    {
      "epoch": 0.917956204379562,
      "grad_norm": 5.445123672485352,
      "learning_rate": 0.0001508445945945946,
      "loss": 0.7521,
      "step": 393
    },
    {
      "epoch": 0.9202919708029197,
      "grad_norm": 8.494091987609863,
      "learning_rate": 0.0001506756756756757,
      "loss": 0.9523,
      "step": 394
    },
    {
      "epoch": 0.9226277372262773,
      "grad_norm": 7.38707160949707,
      "learning_rate": 0.00015050675675675677,
      "loss": 0.8536,
      "step": 395
    },
    {
      "epoch": 0.9249635036496351,
      "grad_norm": 3.4259514808654785,
      "learning_rate": 0.00015033783783783783,
      "loss": 0.739,
      "step": 396
    },
    {
      "epoch": 0.9272992700729927,
      "grad_norm": 4.587640762329102,
      "learning_rate": 0.00015016891891891893,
      "loss": 0.7037,
      "step": 397
    },
    {
      "epoch": 0.9296350364963504,
      "grad_norm": 4.836366176605225,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.7398,
      "step": 398
    },
    {
      "epoch": 0.931970802919708,
      "grad_norm": 7.333074569702148,
      "learning_rate": 0.0001498310810810811,
      "loss": 0.563,
      "step": 399
    },
    {
      "epoch": 0.9343065693430657,
      "grad_norm": 2.705052137374878,
      "learning_rate": 0.00014966216216216218,
      "loss": 0.5028,
      "step": 400
    },
    {
      "epoch": 0.9366423357664233,
      "grad_norm": 7.058990001678467,
      "learning_rate": 0.00014949324324324326,
      "loss": 0.5227,
      "step": 401
    },
    {
      "epoch": 0.9389781021897811,
      "grad_norm": 5.633650779724121,
      "learning_rate": 0.00014932432432432434,
      "loss": 0.7267,
      "step": 402
    },
    {
      "epoch": 0.9413138686131387,
      "grad_norm": 8.744504928588867,
      "learning_rate": 0.0001491554054054054,
      "loss": 1.2343,
      "step": 403
    },
    {
      "epoch": 0.9436496350364963,
      "grad_norm": 6.471978664398193,
      "learning_rate": 0.0001489864864864865,
      "loss": 1.2886,
      "step": 404
    },
    {
      "epoch": 0.945985401459854,
      "grad_norm": 7.438263893127441,
      "learning_rate": 0.00014881756756756758,
      "loss": 1.517,
      "step": 405
    },
    {
      "epoch": 0.9483211678832116,
      "grad_norm": 5.617314338684082,
      "learning_rate": 0.00014864864864864866,
      "loss": 0.7766,
      "step": 406
    },
    {
      "epoch": 0.9506569343065694,
      "grad_norm": 8.536684036254883,
      "learning_rate": 0.00014847972972972974,
      "loss": 0.6426,
      "step": 407
    },
    {
      "epoch": 0.952992700729927,
      "grad_norm": 5.9136128425598145,
      "learning_rate": 0.00014831081081081082,
      "loss": 1.1024,
      "step": 408
    },
    {
      "epoch": 0.9553284671532847,
      "grad_norm": 4.662235260009766,
      "learning_rate": 0.0001481418918918919,
      "loss": 0.7308,
      "step": 409
    },
    {
      "epoch": 0.9576642335766423,
      "grad_norm": 4.224222660064697,
      "learning_rate": 0.00014797297297297298,
      "loss": 0.5269,
      "step": 410
    },
    {
      "epoch": 0.96,
      "grad_norm": 8.386307716369629,
      "learning_rate": 0.00014780405405405406,
      "loss": 1.1271,
      "step": 411
    },
    {
      "epoch": 0.9623357664233577,
      "grad_norm": 4.710031509399414,
      "learning_rate": 0.00014763513513513514,
      "loss": 0.817,
      "step": 412
    },
    {
      "epoch": 0.9646715328467154,
      "grad_norm": 4.970576286315918,
      "learning_rate": 0.00014746621621621622,
      "loss": 0.727,
      "step": 413
    },
    {
      "epoch": 0.967007299270073,
      "grad_norm": 5.313754081726074,
      "learning_rate": 0.0001472972972972973,
      "loss": 0.8984,
      "step": 414
    },
    {
      "epoch": 0.9693430656934306,
      "grad_norm": 4.149856090545654,
      "learning_rate": 0.00014712837837837838,
      "loss": 0.4877,
      "step": 415
    },
    {
      "epoch": 0.9716788321167883,
      "grad_norm": 3.205772876739502,
      "learning_rate": 0.00014695945945945947,
      "loss": 0.4857,
      "step": 416
    },
    {
      "epoch": 0.9740145985401459,
      "grad_norm": 7.293856143951416,
      "learning_rate": 0.00014679054054054055,
      "loss": 1.0866,
      "step": 417
    },
    {
      "epoch": 0.9763503649635037,
      "grad_norm": 9.064064979553223,
      "learning_rate": 0.00014662162162162163,
      "loss": 1.0911,
      "step": 418
    },
    {
      "epoch": 0.9786861313868613,
      "grad_norm": 3.4795987606048584,
      "learning_rate": 0.0001464527027027027,
      "loss": 0.3807,
      "step": 419
    },
    {
      "epoch": 0.981021897810219,
      "grad_norm": 6.142076015472412,
      "learning_rate": 0.0001462837837837838,
      "loss": 0.6725,
      "step": 420
    },
    {
      "epoch": 0.9833576642335766,
      "grad_norm": 2.777956962585449,
      "learning_rate": 0.00014611486486486487,
      "loss": 0.5403,
      "step": 421
    },
    {
      "epoch": 0.9856934306569343,
      "grad_norm": 5.993048667907715,
      "learning_rate": 0.00014594594594594595,
      "loss": 1.0097,
      "step": 422
    },
    {
      "epoch": 0.988029197080292,
      "grad_norm": 3.2484207153320312,
      "learning_rate": 0.00014577702702702703,
      "loss": 0.6419,
      "step": 423
    },
    {
      "epoch": 0.9903649635036497,
      "grad_norm": 6.651515483856201,
      "learning_rate": 0.0001456081081081081,
      "loss": 1.1121,
      "step": 424
    },
    {
      "epoch": 0.9927007299270073,
      "grad_norm": 6.509943008422852,
      "learning_rate": 0.0001454391891891892,
      "loss": 0.815,
      "step": 425
    },
    {
      "epoch": 0.995036496350365,
      "grad_norm": 7.837801456451416,
      "learning_rate": 0.00014527027027027027,
      "loss": 0.7943,
      "step": 426
    },
    {
      "epoch": 0.9973722627737226,
      "grad_norm": 8.747801780700684,
      "learning_rate": 0.00014510135135135138,
      "loss": 0.9447,
      "step": 427
    },
    {
      "epoch": 0.9997080291970802,
      "grad_norm": 6.669861316680908,
      "learning_rate": 0.00014493243243243243,
      "loss": 0.9619,
      "step": 428
    },
    {
      "epoch": 1.0,
      "grad_norm": NaN,
      "learning_rate": 0.00014493243243243243,
      "loss": 1.8854,
      "step": 429
    },
    {
      "epoch": 1.0023357664233576,
      "grad_norm": 4.596429824829102,
      "learning_rate": 0.00014476351351351351,
      "loss": 0.673,
      "step": 430
    },
    {
      "epoch": 1.0046715328467153,
      "grad_norm": 3.2578349113464355,
      "learning_rate": 0.00014459459459459462,
      "loss": 0.4461,
      "step": 431
    },
    {
      "epoch": 1.007007299270073,
      "grad_norm": 6.035192966461182,
      "learning_rate": 0.00014442567567567568,
      "loss": 0.8358,
      "step": 432
    },
    {
      "epoch": 1.0093430656934306,
      "grad_norm": 7.7041192054748535,
      "learning_rate": 0.00014425675675675676,
      "loss": 0.736,
      "step": 433
    },
    {
      "epoch": 1.0116788321167882,
      "grad_norm": 2.7834715843200684,
      "learning_rate": 0.00014408783783783784,
      "loss": 0.2913,
      "step": 434
    },
    {
      "epoch": 1.014014598540146,
      "grad_norm": 5.460861682891846,
      "learning_rate": 0.00014391891891891894,
      "loss": 0.5329,
      "step": 435
    },
    {
      "epoch": 1.0163503649635037,
      "grad_norm": 4.689500331878662,
      "learning_rate": 0.00014375,
      "loss": 0.631,
      "step": 436
    },
    {
      "epoch": 1.0186861313868614,
      "grad_norm": 4.221080780029297,
      "learning_rate": 0.00014358108108108108,
      "loss": 0.459,
      "step": 437
    },
    {
      "epoch": 1.021021897810219,
      "grad_norm": 3.923269271850586,
      "learning_rate": 0.00014341216216216219,
      "loss": 0.4163,
      "step": 438
    },
    {
      "epoch": 1.0233576642335767,
      "grad_norm": 7.384091377258301,
      "learning_rate": 0.00014324324324324324,
      "loss": 0.682,
      "step": 439
    },
    {
      "epoch": 1.0256934306569343,
      "grad_norm": 2.5215537548065186,
      "learning_rate": 0.00014307432432432432,
      "loss": 0.2115,
      "step": 440
    },
    {
      "epoch": 1.028029197080292,
      "grad_norm": 6.305704116821289,
      "learning_rate": 0.0001429054054054054,
      "loss": 1.1165,
      "step": 441
    },
    {
      "epoch": 1.0303649635036496,
      "grad_norm": 2.6440882682800293,
      "learning_rate": 0.0001427364864864865,
      "loss": 0.419,
      "step": 442
    },
    {
      "epoch": 1.0327007299270072,
      "grad_norm": 6.110527992248535,
      "learning_rate": 0.00014256756756756756,
      "loss": 0.4654,
      "step": 443
    },
    {
      "epoch": 1.0350364963503649,
      "grad_norm": 6.3476972579956055,
      "learning_rate": 0.00014239864864864864,
      "loss": 0.7646,
      "step": 444
    },
    {
      "epoch": 1.0373722627737227,
      "grad_norm": 5.20393180847168,
      "learning_rate": 0.00014222972972972975,
      "loss": 0.5101,
      "step": 445
    },
    {
      "epoch": 1.0397080291970804,
      "grad_norm": 10.946528434753418,
      "learning_rate": 0.00014206081081081083,
      "loss": 0.3878,
      "step": 446
    },
    {
      "epoch": 1.042043795620438,
      "grad_norm": 4.48175048828125,
      "learning_rate": 0.00014189189189189188,
      "loss": 0.5331,
      "step": 447
    },
    {
      "epoch": 1.0443795620437957,
      "grad_norm": 2.1287660598754883,
      "learning_rate": 0.000141722972972973,
      "loss": 0.2982,
      "step": 448
    },
    {
      "epoch": 1.0467153284671533,
      "grad_norm": 9.292010307312012,
      "learning_rate": 0.00014155405405405407,
      "loss": 0.488,
      "step": 449
    },
    {
      "epoch": 1.049051094890511,
      "grad_norm": 6.593799114227295,
      "learning_rate": 0.00014138513513513513,
      "loss": 0.9113,
      "step": 450
    },
    {
      "epoch": 1.0513868613138686,
      "grad_norm": 8.146626472473145,
      "learning_rate": 0.0001412162162162162,
      "loss": 0.7571,
      "step": 451
    },
    {
      "epoch": 1.0537226277372262,
      "grad_norm": 7.582242965698242,
      "learning_rate": 0.00014104729729729731,
      "loss": 0.5726,
      "step": 452
    },
    {
      "epoch": 1.056058394160584,
      "grad_norm": 3.0189568996429443,
      "learning_rate": 0.0001408783783783784,
      "loss": 0.1564,
      "step": 453
    },
    {
      "epoch": 1.0583941605839415,
      "grad_norm": 5.976013660430908,
      "learning_rate": 0.00014070945945945945,
      "loss": 0.2897,
      "step": 454
    },
    {
      "epoch": 1.0607299270072992,
      "grad_norm": 7.406803607940674,
      "learning_rate": 0.00014054054054054056,
      "loss": 0.5669,
      "step": 455
    },
    {
      "epoch": 1.0630656934306568,
      "grad_norm": 4.953707218170166,
      "learning_rate": 0.00014037162162162164,
      "loss": 0.4983,
      "step": 456
    },
    {
      "epoch": 1.0654014598540147,
      "grad_norm": 8.327093124389648,
      "learning_rate": 0.0001402027027027027,
      "loss": 0.7327,
      "step": 457
    },
    {
      "epoch": 1.0677372262773723,
      "grad_norm": 5.874166965484619,
      "learning_rate": 0.0001400337837837838,
      "loss": 0.5521,
      "step": 458
    },
    {
      "epoch": 1.07007299270073,
      "grad_norm": 5.781249523162842,
      "learning_rate": 0.00013986486486486488,
      "loss": 0.3172,
      "step": 459
    },
    {
      "epoch": 1.0724087591240876,
      "grad_norm": 1.7059407234191895,
      "learning_rate": 0.00013969594594594596,
      "loss": 0.1562,
      "step": 460
    },
    {
      "epoch": 1.0747445255474453,
      "grad_norm": 3.594942808151245,
      "learning_rate": 0.000139527027027027,
      "loss": 0.6232,
      "step": 461
    },
    {
      "epoch": 1.077080291970803,
      "grad_norm": 4.3159098625183105,
      "learning_rate": 0.00013935810810810812,
      "loss": 0.6608,
      "step": 462
    },
    {
      "epoch": 1.0794160583941605,
      "grad_norm": 22.95301055908203,
      "learning_rate": 0.0001391891891891892,
      "loss": 0.9593,
      "step": 463
    },
    {
      "epoch": 1.0817518248175182,
      "grad_norm": 4.162599563598633,
      "learning_rate": 0.00013902027027027028,
      "loss": 0.121,
      "step": 464
    },
    {
      "epoch": 1.0840875912408758,
      "grad_norm": 10.367852210998535,
      "learning_rate": 0.00013885135135135136,
      "loss": 0.6493,
      "step": 465
    },
    {
      "epoch": 1.0864233576642335,
      "grad_norm": 20.248218536376953,
      "learning_rate": 0.00013868243243243244,
      "loss": 0.9463,
      "step": 466
    },
    {
      "epoch": 1.0887591240875913,
      "grad_norm": 6.749633312225342,
      "learning_rate": 0.00013851351351351352,
      "loss": 0.6067,
      "step": 467
    },
    {
      "epoch": 1.091094890510949,
      "grad_norm": 2.3994061946868896,
      "learning_rate": 0.0001383445945945946,
      "loss": 0.2359,
      "step": 468
    },
    {
      "epoch": 1.0934306569343066,
      "grad_norm": 5.598981857299805,
      "learning_rate": 0.00013817567567567569,
      "loss": 0.5659,
      "step": 469
    },
    {
      "epoch": 1.0957664233576643,
      "grad_norm": 9.786368370056152,
      "learning_rate": 0.00013800675675675677,
      "loss": 0.4118,
      "step": 470
    },
    {
      "epoch": 1.098102189781022,
      "grad_norm": 14.289093017578125,
      "learning_rate": 0.00013783783783783785,
      "loss": 0.8315,
      "step": 471
    },
    {
      "epoch": 1.1004379562043796,
      "grad_norm": 14.615381240844727,
      "learning_rate": 0.00013766891891891893,
      "loss": 1.4323,
      "step": 472
    },
    {
      "epoch": 1.1027737226277372,
      "grad_norm": 17.676340103149414,
      "learning_rate": 0.0001375,
      "loss": 0.5521,
      "step": 473
    },
    {
      "epoch": 1.1051094890510949,
      "grad_norm": 16.377269744873047,
      "learning_rate": 0.0001373310810810811,
      "loss": 0.4428,
      "step": 474
    },
    {
      "epoch": 1.1074452554744525,
      "grad_norm": 17.224184036254883,
      "learning_rate": 0.00013716216216216217,
      "loss": 0.7749,
      "step": 475
    },
    {
      "epoch": 1.1097810218978101,
      "grad_norm": 6.155167579650879,
      "learning_rate": 0.00013699324324324325,
      "loss": 0.8031,
      "step": 476
    },
    {
      "epoch": 1.1121167883211678,
      "grad_norm": 4.363621234893799,
      "learning_rate": 0.00013682432432432433,
      "loss": 0.3698,
      "step": 477
    },
    {
      "epoch": 1.1144525547445256,
      "grad_norm": 4.149532318115234,
      "learning_rate": 0.0001366554054054054,
      "loss": 0.3866,
      "step": 478
    },
    {
      "epoch": 1.1167883211678833,
      "grad_norm": 12.887531280517578,
      "learning_rate": 0.0001364864864864865,
      "loss": 1.0287,
      "step": 479
    },
    {
      "epoch": 1.119124087591241,
      "grad_norm": 6.879570007324219,
      "learning_rate": 0.00013631756756756757,
      "loss": 0.6054,
      "step": 480
    },
    {
      "epoch": 1.1214598540145986,
      "grad_norm": 4.282322883605957,
      "learning_rate": 0.00013614864864864865,
      "loss": 0.3176,
      "step": 481
    },
    {
      "epoch": 1.1237956204379562,
      "grad_norm": 14.222006797790527,
      "learning_rate": 0.00013597972972972973,
      "loss": 0.5924,
      "step": 482
    },
    {
      "epoch": 1.1261313868613139,
      "grad_norm": 10.988564491271973,
      "learning_rate": 0.00013581081081081081,
      "loss": 0.7525,
      "step": 483
    },
    {
      "epoch": 1.1284671532846715,
      "grad_norm": 7.435821056365967,
      "learning_rate": 0.0001356418918918919,
      "loss": 0.8651,
      "step": 484
    },
    {
      "epoch": 1.1308029197080292,
      "grad_norm": 3.873873710632324,
      "learning_rate": 0.00013547297297297298,
      "loss": 0.3953,
      "step": 485
    },
    {
      "epoch": 1.1331386861313868,
      "grad_norm": 7.190780162811279,
      "learning_rate": 0.00013530405405405406,
      "loss": 0.6423,
      "step": 486
    },
    {
      "epoch": 1.1354744525547444,
      "grad_norm": 4.3324713706970215,
      "learning_rate": 0.00013513513513513514,
      "loss": 0.458,
      "step": 487
    },
    {
      "epoch": 1.1378102189781023,
      "grad_norm": 8.550889015197754,
      "learning_rate": 0.00013496621621621622,
      "loss": 0.6977,
      "step": 488
    },
    {
      "epoch": 1.14014598540146,
      "grad_norm": 7.610562801361084,
      "learning_rate": 0.0001347972972972973,
      "loss": 0.5115,
      "step": 489
    },
    {
      "epoch": 1.1424817518248176,
      "grad_norm": 7.584954738616943,
      "learning_rate": 0.00013462837837837838,
      "loss": 0.7274,
      "step": 490
    },
    {
      "epoch": 1.1448175182481752,
      "grad_norm": 4.137876033782959,
      "learning_rate": 0.00013445945945945946,
      "loss": 0.2165,
      "step": 491
    },
    {
      "epoch": 1.1471532846715329,
      "grad_norm": 7.245903015136719,
      "learning_rate": 0.00013429054054054057,
      "loss": 0.7752,
      "step": 492
    },
    {
      "epoch": 1.1494890510948905,
      "grad_norm": 6.079737663269043,
      "learning_rate": 0.00013412162162162162,
      "loss": 0.8652,
      "step": 493
    },
    {
      "epoch": 1.1518248175182482,
      "grad_norm": 2.861290216445923,
      "learning_rate": 0.0001339527027027027,
      "loss": 0.1895,
      "step": 494
    },
    {
      "epoch": 1.1541605839416058,
      "grad_norm": 5.558864593505859,
      "learning_rate": 0.0001337837837837838,
      "loss": 0.6506,
      "step": 495
    },
    {
      "epoch": 1.1564963503649635,
      "grad_norm": 6.49865198135376,
      "learning_rate": 0.00013361486486486486,
      "loss": 0.3905,
      "step": 496
    },
    {
      "epoch": 1.158832116788321,
      "grad_norm": 3.49749755859375,
      "learning_rate": 0.00013344594594594594,
      "loss": 0.4793,
      "step": 497
    },
    {
      "epoch": 1.1611678832116787,
      "grad_norm": 5.642049312591553,
      "learning_rate": 0.00013327702702702702,
      "loss": 0.5271,
      "step": 498
    },
    {
      "epoch": 1.1635036496350364,
      "grad_norm": 5.05645751953125,
      "learning_rate": 0.00013310810810810813,
      "loss": 0.4729,
      "step": 499
    },
    {
      "epoch": 1.1658394160583943,
      "grad_norm": 9.839338302612305,
      "learning_rate": 0.00013293918918918919,
      "loss": 0.3773,
      "step": 500
    },
    {
      "epoch": 1.168175182481752,
      "grad_norm": 9.435601234436035,
      "learning_rate": 0.00013277027027027027,
      "loss": 0.5436,
      "step": 501
    },
    {
      "epoch": 1.1705109489051095,
      "grad_norm": 4.66444730758667,
      "learning_rate": 0.00013260135135135137,
      "loss": 0.3634,
      "step": 502
    },
    {
      "epoch": 1.1728467153284672,
      "grad_norm": 6.245996475219727,
      "learning_rate": 0.00013243243243243243,
      "loss": 0.5678,
      "step": 503
    },
    {
      "epoch": 1.1751824817518248,
      "grad_norm": 3.6440765857696533,
      "learning_rate": 0.0001322635135135135,
      "loss": 0.1499,
      "step": 504
    },
    {
      "epoch": 1.1775182481751825,
      "grad_norm": 6.120613098144531,
      "learning_rate": 0.00013209459459459462,
      "loss": 0.6688,
      "step": 505
    },
    {
      "epoch": 1.1798540145985401,
      "grad_norm": 7.7360005378723145,
      "learning_rate": 0.0001319256756756757,
      "loss": 0.7003,
      "step": 506
    },
    {
      "epoch": 1.1821897810218978,
      "grad_norm": 11.048473358154297,
      "learning_rate": 0.00013175675675675675,
      "loss": 0.3141,
      "step": 507
    },
    {
      "epoch": 1.1845255474452554,
      "grad_norm": 5.13901424407959,
      "learning_rate": 0.00013158783783783783,
      "loss": 0.379,
      "step": 508
    },
    {
      "epoch": 1.186861313868613,
      "grad_norm": 14.139236450195312,
      "learning_rate": 0.00013141891891891894,
      "loss": 0.5678,
      "step": 509
    },
    {
      "epoch": 1.189197080291971,
      "grad_norm": 8.951922416687012,
      "learning_rate": 0.00013125000000000002,
      "loss": 0.493,
      "step": 510
    },
    {
      "epoch": 1.1915328467153286,
      "grad_norm": 5.621096134185791,
      "learning_rate": 0.00013108108108108107,
      "loss": 0.8823,
      "step": 511
    },
    {
      "epoch": 1.1938686131386862,
      "grad_norm": 5.18740701675415,
      "learning_rate": 0.00013091216216216218,
      "loss": 0.4749,
      "step": 512
    },
    {
      "epoch": 1.1962043795620438,
      "grad_norm": 6.6067891120910645,
      "learning_rate": 0.00013074324324324326,
      "loss": 0.8615,
      "step": 513
    },
    {
      "epoch": 1.1985401459854015,
      "grad_norm": 5.189572811126709,
      "learning_rate": 0.00013057432432432431,
      "loss": 0.4649,
      "step": 514
    },
    {
      "epoch": 1.2008759124087591,
      "grad_norm": 6.92630672454834,
      "learning_rate": 0.0001304054054054054,
      "loss": 0.5517,
      "step": 515
    },
    {
      "epoch": 1.2032116788321168,
      "grad_norm": 5.696386814117432,
      "learning_rate": 0.0001302364864864865,
      "loss": 0.3863,
      "step": 516
    },
    {
      "epoch": 1.2055474452554744,
      "grad_norm": 5.481411933898926,
      "learning_rate": 0.00013006756756756758,
      "loss": 0.697,
      "step": 517
    },
    {
      "epoch": 1.207883211678832,
      "grad_norm": 9.79636001586914,
      "learning_rate": 0.00012989864864864864,
      "loss": 0.7323,
      "step": 518
    },
    {
      "epoch": 1.2102189781021897,
      "grad_norm": 3.7961249351501465,
      "learning_rate": 0.00012972972972972974,
      "loss": 0.269,
      "step": 519
    },
    {
      "epoch": 1.2125547445255473,
      "grad_norm": 11.167445182800293,
      "learning_rate": 0.00012956081081081083,
      "loss": 0.9927,
      "step": 520
    },
    {
      "epoch": 1.214890510948905,
      "grad_norm": 6.375640869140625,
      "learning_rate": 0.00012939189189189188,
      "loss": 0.6386,
      "step": 521
    },
    {
      "epoch": 1.2172262773722629,
      "grad_norm": 7.117393970489502,
      "learning_rate": 0.000129222972972973,
      "loss": 0.3333,
      "step": 522
    },
    {
      "epoch": 1.2195620437956205,
      "grad_norm": 10.585295677185059,
      "learning_rate": 0.00012905405405405407,
      "loss": 0.9819,
      "step": 523
    },
    {
      "epoch": 1.2218978102189781,
      "grad_norm": 7.209827899932861,
      "learning_rate": 0.00012888513513513515,
      "loss": 0.41,
      "step": 524
    },
    {
      "epoch": 1.2242335766423358,
      "grad_norm": 5.857247829437256,
      "learning_rate": 0.0001287162162162162,
      "loss": 0.8025,
      "step": 525
    },
    {
      "epoch": 1.2265693430656934,
      "grad_norm": 1.712826132774353,
      "learning_rate": 0.0001285472972972973,
      "loss": 0.252,
      "step": 526
    },
    {
      "epoch": 1.228905109489051,
      "grad_norm": 5.99552583694458,
      "learning_rate": 0.0001283783783783784,
      "loss": 0.2464,
      "step": 527
    },
    {
      "epoch": 1.2312408759124087,
      "grad_norm": 5.761125564575195,
      "learning_rate": 0.00012820945945945947,
      "loss": 0.2864,
      "step": 528
    },
    {
      "epoch": 1.2335766423357664,
      "grad_norm": 7.770198822021484,
      "learning_rate": 0.00012804054054054055,
      "loss": 0.4386,
      "step": 529
    },
    {
      "epoch": 1.235912408759124,
      "grad_norm": 6.250432014465332,
      "learning_rate": 0.00012787162162162163,
      "loss": 0.4375,
      "step": 530
    },
    {
      "epoch": 1.2382481751824819,
      "grad_norm": 11.224077224731445,
      "learning_rate": 0.0001277027027027027,
      "loss": 1.1553,
      "step": 531
    },
    {
      "epoch": 1.2405839416058395,
      "grad_norm": 8.284651756286621,
      "learning_rate": 0.0001275337837837838,
      "loss": 0.9381,
      "step": 532
    },
    {
      "epoch": 1.2429197080291972,
      "grad_norm": 8.22350788116455,
      "learning_rate": 0.00012736486486486487,
      "loss": 0.6425,
      "step": 533
    },
    {
      "epoch": 1.2452554744525548,
      "grad_norm": 6.093392848968506,
      "learning_rate": 0.00012719594594594595,
      "loss": 0.4442,
      "step": 534
    },
    {
      "epoch": 1.2475912408759124,
      "grad_norm": 8.044147491455078,
      "learning_rate": 0.00012702702702702703,
      "loss": 0.8491,
      "step": 535
    },
    {
      "epoch": 1.24992700729927,
      "grad_norm": 8.474099159240723,
      "learning_rate": 0.00012685810810810812,
      "loss": 0.4721,
      "step": 536
    },
    {
      "epoch": 1.2522627737226277,
      "grad_norm": 7.7703657150268555,
      "learning_rate": 0.0001266891891891892,
      "loss": 0.4961,
      "step": 537
    },
    {
      "epoch": 1.2545985401459854,
      "grad_norm": 6.2414326667785645,
      "learning_rate": 0.00012652027027027028,
      "loss": 0.8603,
      "step": 538
    },
    {
      "epoch": 1.256934306569343,
      "grad_norm": 8.466490745544434,
      "learning_rate": 0.00012635135135135136,
      "loss": 0.5148,
      "step": 539
    },
    {
      "epoch": 1.2592700729927007,
      "grad_norm": 5.307276725769043,
      "learning_rate": 0.00012618243243243244,
      "loss": 0.4239,
      "step": 540
    },
    {
      "epoch": 1.2616058394160583,
      "grad_norm": 8.225961685180664,
      "learning_rate": 0.00012601351351351352,
      "loss": 0.7664,
      "step": 541
    },
    {
      "epoch": 1.263941605839416,
      "grad_norm": 7.078658103942871,
      "learning_rate": 0.0001258445945945946,
      "loss": 0.3325,
      "step": 542
    },
    {
      "epoch": 1.2662773722627736,
      "grad_norm": 6.680000305175781,
      "learning_rate": 0.00012567567567567568,
      "loss": 0.4104,
      "step": 543
    },
    {
      "epoch": 1.2686131386861315,
      "grad_norm": 4.947274208068848,
      "learning_rate": 0.00012550675675675676,
      "loss": 0.6163,
      "step": 544
    },
    {
      "epoch": 1.270948905109489,
      "grad_norm": 5.724225044250488,
      "learning_rate": 0.00012533783783783784,
      "loss": 0.7347,
      "step": 545
    },
    {
      "epoch": 1.2732846715328467,
      "grad_norm": 4.654164791107178,
      "learning_rate": 0.00012516891891891892,
      "loss": 0.8532,
      "step": 546
    },
    {
      "epoch": 1.2756204379562044,
      "grad_norm": 7.351449489593506,
      "learning_rate": 0.000125,
      "loss": 0.6932,
      "step": 547
    },
    {
      "epoch": 1.277956204379562,
      "grad_norm": 8.429614067077637,
      "learning_rate": 0.00012483108108108108,
      "loss": 0.3477,
      "step": 548
    },
    {
      "epoch": 1.2802919708029197,
      "grad_norm": 6.022015571594238,
      "learning_rate": 0.00012466216216216216,
      "loss": 0.6759,
      "step": 549
    },
    {
      "epoch": 1.2826277372262773,
      "grad_norm": 7.926936626434326,
      "learning_rate": 0.00012449324324324324,
      "loss": 1.0818,
      "step": 550
    },
    {
      "epoch": 1.284963503649635,
      "grad_norm": 8.713315963745117,
      "learning_rate": 0.00012432432432432433,
      "loss": 0.4246,
      "step": 551
    },
    {
      "epoch": 1.2872992700729928,
      "grad_norm": 3.685232400894165,
      "learning_rate": 0.0001241554054054054,
      "loss": 0.3303,
      "step": 552
    },
    {
      "epoch": 1.2896350364963505,
      "grad_norm": 5.442508697509766,
      "learning_rate": 0.00012398648648648649,
      "loss": 0.585,
      "step": 553
    },
    {
      "epoch": 1.2919708029197081,
      "grad_norm": 7.723743438720703,
      "learning_rate": 0.00012381756756756757,
      "loss": 0.7422,
      "step": 554
    },
    {
      "epoch": 1.2943065693430658,
      "grad_norm": 8.75072193145752,
      "learning_rate": 0.00012364864864864865,
      "loss": 0.5162,
      "step": 555
    },
    {
      "epoch": 1.2966423357664234,
      "grad_norm": 4.006641387939453,
      "learning_rate": 0.00012347972972972976,
      "loss": 0.3575,
      "step": 556
    },
    {
      "epoch": 1.298978102189781,
      "grad_norm": 5.04097318649292,
      "learning_rate": 0.0001233108108108108,
      "loss": 0.6785,
      "step": 557
    },
    {
      "epoch": 1.3013138686131387,
      "grad_norm": 4.026482582092285,
      "learning_rate": 0.0001231418918918919,
      "loss": 0.49,
      "step": 558
    },
    {
      "epoch": 1.3036496350364963,
      "grad_norm": 6.560734272003174,
      "learning_rate": 0.000122972972972973,
      "loss": 0.6358,
      "step": 559
    },
    {
      "epoch": 1.305985401459854,
      "grad_norm": 4.035945415496826,
      "learning_rate": 0.00012280405405405405,
      "loss": 0.3497,
      "step": 560
    },
    {
      "epoch": 1.3083211678832116,
      "grad_norm": 5.134778022766113,
      "learning_rate": 0.00012263513513513513,
      "loss": 0.5404,
      "step": 561
    },
    {
      "epoch": 1.3106569343065693,
      "grad_norm": 5.130261421203613,
      "learning_rate": 0.0001224662162162162,
      "loss": 0.7627,
      "step": 562
    },
    {
      "epoch": 1.312992700729927,
      "grad_norm": 6.989902496337891,
      "learning_rate": 0.00012229729729729732,
      "loss": 0.4894,
      "step": 563
    },
    {
      "epoch": 1.3153284671532846,
      "grad_norm": 7.659815788269043,
      "learning_rate": 0.00012212837837837837,
      "loss": 0.9022,
      "step": 564
    },
    {
      "epoch": 1.3176642335766424,
      "grad_norm": 8.289347648620605,
      "learning_rate": 0.00012195945945945945,
      "loss": 0.7692,
      "step": 565
    },
    {
      "epoch": 1.32,
      "grad_norm": 7.619430065155029,
      "learning_rate": 0.00012179054054054055,
      "loss": 0.7521,
      "step": 566
    },
    {
      "epoch": 1.3223357664233577,
      "grad_norm": 9.146449089050293,
      "learning_rate": 0.00012162162162162163,
      "loss": 0.7959,
      "step": 567
    },
    {
      "epoch": 1.3246715328467153,
      "grad_norm": 5.56848669052124,
      "learning_rate": 0.00012145270270270271,
      "loss": 0.4772,
      "step": 568
    },
    {
      "epoch": 1.327007299270073,
      "grad_norm": 9.74183464050293,
      "learning_rate": 0.0001212837837837838,
      "loss": 1.1212,
      "step": 569
    },
    {
      "epoch": 1.3293430656934306,
      "grad_norm": 6.882003307342529,
      "learning_rate": 0.00012111486486486487,
      "loss": 0.4497,
      "step": 570
    },
    {
      "epoch": 1.3316788321167883,
      "grad_norm": 8.591551780700684,
      "learning_rate": 0.00012094594594594595,
      "loss": 0.6373,
      "step": 571
    },
    {
      "epoch": 1.334014598540146,
      "grad_norm": 10.906974792480469,
      "learning_rate": 0.00012077702702702702,
      "loss": 0.5731,
      "step": 572
    },
    {
      "epoch": 1.3363503649635036,
      "grad_norm": 5.393909454345703,
      "learning_rate": 0.00012060810810810813,
      "loss": 0.5455,
      "step": 573
    },
    {
      "epoch": 1.3386861313868614,
      "grad_norm": 4.507833003997803,
      "learning_rate": 0.0001204391891891892,
      "loss": 0.4826,
      "step": 574
    },
    {
      "epoch": 1.341021897810219,
      "grad_norm": 4.79478645324707,
      "learning_rate": 0.00012027027027027027,
      "loss": 0.5918,
      "step": 575
    },
    {
      "epoch": 1.3433576642335767,
      "grad_norm": 5.314598083496094,
      "learning_rate": 0.00012010135135135137,
      "loss": 0.476,
      "step": 576
    },
    {
      "epoch": 1.3456934306569344,
      "grad_norm": 4.324897289276123,
      "learning_rate": 0.00011993243243243244,
      "loss": 0.4039,
      "step": 577
    },
    {
      "epoch": 1.348029197080292,
      "grad_norm": 3.7442586421966553,
      "learning_rate": 0.00011976351351351352,
      "loss": 0.5378,
      "step": 578
    },
    {
      "epoch": 1.3503649635036497,
      "grad_norm": 3.7008166313171387,
      "learning_rate": 0.00011959459459459461,
      "loss": 0.3954,
      "step": 579
    },
    {
      "epoch": 1.3527007299270073,
      "grad_norm": 6.611142635345459,
      "learning_rate": 0.00011942567567567569,
      "loss": 0.358,
      "step": 580
    },
    {
      "epoch": 1.355036496350365,
      "grad_norm": 7.718290328979492,
      "learning_rate": 0.00011925675675675676,
      "loss": 1.0027,
      "step": 581
    },
    {
      "epoch": 1.3573722627737226,
      "grad_norm": 5.317841529846191,
      "learning_rate": 0.00011908783783783784,
      "loss": 0.3435,
      "step": 582
    },
    {
      "epoch": 1.3597080291970802,
      "grad_norm": 6.793213844299316,
      "learning_rate": 0.00011891891891891893,
      "loss": 0.5149,
      "step": 583
    },
    {
      "epoch": 1.3620437956204379,
      "grad_norm": 10.437383651733398,
      "learning_rate": 0.00011875,
      "loss": 0.8374,
      "step": 584
    },
    {
      "epoch": 1.3643795620437955,
      "grad_norm": 11.393327713012695,
      "learning_rate": 0.00011858108108108108,
      "loss": 0.9483,
      "step": 585
    },
    {
      "epoch": 1.3667153284671532,
      "grad_norm": 10.337203025817871,
      "learning_rate": 0.00011841216216216217,
      "loss": 0.5992,
      "step": 586
    },
    {
      "epoch": 1.369051094890511,
      "grad_norm": 5.4259772300720215,
      "learning_rate": 0.00011824324324324326,
      "loss": 0.1964,
      "step": 587
    },
    {
      "epoch": 1.3713868613138687,
      "grad_norm": 5.009902000427246,
      "learning_rate": 0.00011807432432432432,
      "loss": 0.3448,
      "step": 588
    },
    {
      "epoch": 1.3737226277372263,
      "grad_norm": 16.547250747680664,
      "learning_rate": 0.0001179054054054054,
      "loss": 1.2409,
      "step": 589
    },
    {
      "epoch": 1.376058394160584,
      "grad_norm": 7.781597137451172,
      "learning_rate": 0.0001177364864864865,
      "loss": 0.9506,
      "step": 590
    },
    {
      "epoch": 1.3783941605839416,
      "grad_norm": 7.291597366333008,
      "learning_rate": 0.00011756756756756758,
      "loss": 0.3709,
      "step": 591
    },
    {
      "epoch": 1.3807299270072992,
      "grad_norm": 10.970905303955078,
      "learning_rate": 0.00011739864864864864,
      "loss": 0.6648,
      "step": 592
    },
    {
      "epoch": 1.3830656934306569,
      "grad_norm": 2.852044105529785,
      "learning_rate": 0.00011722972972972974,
      "loss": 0.3221,
      "step": 593
    },
    {
      "epoch": 1.3854014598540145,
      "grad_norm": 6.2078118324279785,
      "learning_rate": 0.00011706081081081082,
      "loss": 0.6076,
      "step": 594
    },
    {
      "epoch": 1.3877372262773724,
      "grad_norm": 6.885203838348389,
      "learning_rate": 0.00011689189189189189,
      "loss": 0.4871,
      "step": 595
    },
    {
      "epoch": 1.39007299270073,
      "grad_norm": 6.083620071411133,
      "learning_rate": 0.000116722972972973,
      "loss": 0.4867,
      "step": 596
    },
    {
      "epoch": 1.3924087591240877,
      "grad_norm": 8.372346878051758,
      "learning_rate": 0.00011655405405405406,
      "loss": 0.7881,
      "step": 597
    },
    {
      "epoch": 1.3947445255474453,
      "grad_norm": 5.361161708831787,
      "learning_rate": 0.00011638513513513514,
      "loss": 0.6642,
      "step": 598
    },
    {
      "epoch": 1.397080291970803,
      "grad_norm": 6.176519393920898,
      "learning_rate": 0.00011621621621621621,
      "loss": 0.5222,
      "step": 599
    },
    {
      "epoch": 1.3994160583941606,
      "grad_norm": 9.91791820526123,
      "learning_rate": 0.0001160472972972973,
      "loss": 0.4295,
      "step": 600
    },
    {
      "epoch": 1.4017518248175183,
      "grad_norm": 9.03536605834961,
      "learning_rate": 0.00011587837837837838,
      "loss": 0.8464,
      "step": 601
    },
    {
      "epoch": 1.404087591240876,
      "grad_norm": 4.827908515930176,
      "learning_rate": 0.00011570945945945945,
      "loss": 0.6691,
      "step": 602
    },
    {
      "epoch": 1.4064233576642335,
      "grad_norm": 7.758268356323242,
      "learning_rate": 0.00011554054054054056,
      "loss": 0.9087,
      "step": 603
    },
    {
      "epoch": 1.4087591240875912,
      "grad_norm": 11.46689510345459,
      "learning_rate": 0.00011537162162162163,
      "loss": 0.7001,
      "step": 604
    },
    {
      "epoch": 1.4110948905109488,
      "grad_norm": 4.951912879943848,
      "learning_rate": 0.0001152027027027027,
      "loss": 0.4071,
      "step": 605
    },
    {
      "epoch": 1.4134306569343065,
      "grad_norm": 8.214978218078613,
      "learning_rate": 0.0001150337837837838,
      "loss": 0.7959,
      "step": 606
    },
    {
      "epoch": 1.4157664233576641,
      "grad_norm": 11.517888069152832,
      "learning_rate": 0.00011486486486486487,
      "loss": 1.0541,
      "step": 607
    },
    {
      "epoch": 1.4181021897810218,
      "grad_norm": 9.37918758392334,
      "learning_rate": 0.00011469594594594595,
      "loss": 0.8273,
      "step": 608
    },
    {
      "epoch": 1.4204379562043796,
      "grad_norm": 5.188201904296875,
      "learning_rate": 0.00011452702702702703,
      "loss": 0.4362,
      "step": 609
    },
    {
      "epoch": 1.4227737226277373,
      "grad_norm": 3.3232510089874268,
      "learning_rate": 0.00011435810810810812,
      "loss": 0.4231,
      "step": 610
    },
    {
      "epoch": 1.425109489051095,
      "grad_norm": 3.896658182144165,
      "learning_rate": 0.00011418918918918919,
      "loss": 0.5295,
      "step": 611
    },
    {
      "epoch": 1.4274452554744526,
      "grad_norm": 4.096184253692627,
      "learning_rate": 0.00011402027027027027,
      "loss": 0.4921,
      "step": 612
    },
    {
      "epoch": 1.4297810218978102,
      "grad_norm": 5.328663349151611,
      "learning_rate": 0.00011385135135135137,
      "loss": 0.5597,
      "step": 613
    },
    {
      "epoch": 1.4321167883211678,
      "grad_norm": 5.52100944519043,
      "learning_rate": 0.00011368243243243245,
      "loss": 0.6832,
      "step": 614
    },
    {
      "epoch": 1.4344525547445255,
      "grad_norm": 5.970470428466797,
      "learning_rate": 0.00011351351351351351,
      "loss": 0.4664,
      "step": 615
    },
    {
      "epoch": 1.4367883211678831,
      "grad_norm": 4.896577835083008,
      "learning_rate": 0.00011334459459459461,
      "loss": 0.5476,
      "step": 616
    },
    {
      "epoch": 1.439124087591241,
      "grad_norm": 10.44189167022705,
      "learning_rate": 0.00011317567567567569,
      "loss": 0.6439,
      "step": 617
    },
    {
      "epoch": 1.4414598540145986,
      "grad_norm": 2.377464771270752,
      "learning_rate": 0.00011300675675675676,
      "loss": 0.1803,
      "step": 618
    },
    {
      "epoch": 1.4437956204379563,
      "grad_norm": 5.770151615142822,
      "learning_rate": 0.00011283783783783784,
      "loss": 0.6368,
      "step": 619
    },
    {
      "epoch": 1.446131386861314,
      "grad_norm": 4.658301830291748,
      "learning_rate": 0.00011266891891891893,
      "loss": 0.6294,
      "step": 620
    },
    {
      "epoch": 1.4484671532846716,
      "grad_norm": 4.241156578063965,
      "learning_rate": 0.00011250000000000001,
      "loss": 0.3579,
      "step": 621
    },
    {
      "epoch": 1.4508029197080292,
      "grad_norm": 4.3998918533325195,
      "learning_rate": 0.00011233108108108108,
      "loss": 0.4769,
      "step": 622
    },
    {
      "epoch": 1.4531386861313869,
      "grad_norm": 4.28031063079834,
      "learning_rate": 0.00011216216216216217,
      "loss": 0.4642,
      "step": 623
    },
    {
      "epoch": 1.4554744525547445,
      "grad_norm": 3.184324026107788,
      "learning_rate": 0.00011199324324324325,
      "loss": 0.2038,
      "step": 624
    },
    {
      "epoch": 1.4578102189781021,
      "grad_norm": 11.943649291992188,
      "learning_rate": 0.00011182432432432432,
      "loss": 0.5848,
      "step": 625
    },
    {
      "epoch": 1.4601459854014598,
      "grad_norm": 4.33174991607666,
      "learning_rate": 0.0001116554054054054,
      "loss": 0.1936,
      "step": 626
    },
    {
      "epoch": 1.4624817518248174,
      "grad_norm": 8.97397232055664,
      "learning_rate": 0.0001114864864864865,
      "loss": 0.4393,
      "step": 627
    },
    {
      "epoch": 1.464817518248175,
      "grad_norm": 7.216663360595703,
      "learning_rate": 0.00011131756756756757,
      "loss": 0.8794,
      "step": 628
    },
    {
      "epoch": 1.4671532846715327,
      "grad_norm": 8.665989875793457,
      "learning_rate": 0.00011114864864864864,
      "loss": 0.3754,
      "step": 629
    },
    {
      "epoch": 1.4694890510948906,
      "grad_norm": 6.125514507293701,
      "learning_rate": 0.00011097972972972974,
      "loss": 0.4537,
      "step": 630
    },
    {
      "epoch": 1.4718248175182482,
      "grad_norm": 6.840622901916504,
      "learning_rate": 0.00011081081081081082,
      "loss": 0.5684,
      "step": 631
    },
    {
      "epoch": 1.4741605839416059,
      "grad_norm": 12.753043174743652,
      "learning_rate": 0.0001106418918918919,
      "loss": 0.6739,
      "step": 632
    },
    {
      "epoch": 1.4764963503649635,
      "grad_norm": 5.607357978820801,
      "learning_rate": 0.00011047297297297299,
      "loss": 0.3256,
      "step": 633
    },
    {
      "epoch": 1.4788321167883212,
      "grad_norm": 12.515810012817383,
      "learning_rate": 0.00011030405405405406,
      "loss": 0.3547,
      "step": 634
    },
    {
      "epoch": 1.4811678832116788,
      "grad_norm": 11.210979461669922,
      "learning_rate": 0.00011013513513513514,
      "loss": 0.6606,
      "step": 635
    },
    {
      "epoch": 1.4835036496350364,
      "grad_norm": 8.214737892150879,
      "learning_rate": 0.0001099662162162162,
      "loss": 0.7807,
      "step": 636
    },
    {
      "epoch": 1.485839416058394,
      "grad_norm": 23.013816833496094,
      "learning_rate": 0.00010979729729729731,
      "loss": 0.7968,
      "step": 637
    },
    {
      "epoch": 1.488175182481752,
      "grad_norm": 5.710624694824219,
      "learning_rate": 0.00010962837837837838,
      "loss": 0.5079,
      "step": 638
    },
    {
      "epoch": 1.4905109489051096,
      "grad_norm": 5.935061931610107,
      "learning_rate": 0.00010945945945945946,
      "loss": 0.6296,
      "step": 639
    },
    {
      "epoch": 1.4928467153284672,
      "grad_norm": 15.333270072937012,
      "learning_rate": 0.00010929054054054056,
      "loss": 1.0782,
      "step": 640
    },
    {
      "epoch": 1.4951824817518249,
      "grad_norm": 9.351359367370605,
      "learning_rate": 0.00010912162162162162,
      "loss": 1.7579,
      "step": 641
    },
    {
      "epoch": 1.4975182481751825,
      "grad_norm": 12.637046813964844,
      "learning_rate": 0.0001089527027027027,
      "loss": 0.8317,
      "step": 642
    },
    {
      "epoch": 1.4998540145985402,
      "grad_norm": 5.319942474365234,
      "learning_rate": 0.0001087837837837838,
      "loss": 0.619,
      "step": 643
    },
    {
      "epoch": 1.5021897810218978,
      "grad_norm": 11.27040958404541,
      "learning_rate": 0.00010861486486486488,
      "loss": 0.7153,
      "step": 644
    },
    {
      "epoch": 1.5045255474452555,
      "grad_norm": 15.185528755187988,
      "learning_rate": 0.00010844594594594595,
      "loss": 0.5436,
      "step": 645
    },
    {
      "epoch": 1.506861313868613,
      "grad_norm": 15.126139640808105,
      "learning_rate": 0.00010827702702702703,
      "loss": 1.3092,
      "step": 646
    },
    {
      "epoch": 1.5091970802919707,
      "grad_norm": 4.00553035736084,
      "learning_rate": 0.00010810810810810812,
      "loss": 0.4327,
      "step": 647
    },
    {
      "epoch": 1.5115328467153284,
      "grad_norm": 5.265575885772705,
      "learning_rate": 0.00010793918918918919,
      "loss": 0.6592,
      "step": 648
    },
    {
      "epoch": 1.513868613138686,
      "grad_norm": 3.9530837535858154,
      "learning_rate": 0.00010777027027027027,
      "loss": 0.5662,
      "step": 649
    },
    {
      "epoch": 1.5162043795620437,
      "grad_norm": 10.275737762451172,
      "learning_rate": 0.00010760135135135136,
      "loss": 0.7392,
      "step": 650
    },
    {
      "epoch": 1.5185401459854013,
      "grad_norm": 10.316825866699219,
      "learning_rate": 0.00010743243243243244,
      "loss": 0.9639,
      "step": 651
    },
    {
      "epoch": 1.520875912408759,
      "grad_norm": 4.931670188903809,
      "learning_rate": 0.00010726351351351351,
      "loss": 0.5289,
      "step": 652
    },
    {
      "epoch": 1.5232116788321168,
      "grad_norm": 6.19324254989624,
      "learning_rate": 0.0001070945945945946,
      "loss": 0.5221,
      "step": 653
    },
    {
      "epoch": 1.5255474452554745,
      "grad_norm": 3.2759077548980713,
      "learning_rate": 0.00010692567567567569,
      "loss": 0.3996,
      "step": 654
    },
    {
      "epoch": 1.5278832116788321,
      "grad_norm": 5.38596248626709,
      "learning_rate": 0.00010675675675675677,
      "loss": 0.594,
      "step": 655
    },
    {
      "epoch": 1.5302189781021898,
      "grad_norm": 3.9944286346435547,
      "learning_rate": 0.00010658783783783783,
      "loss": 0.7201,
      "step": 656
    },
    {
      "epoch": 1.5325547445255474,
      "grad_norm": 4.925441265106201,
      "learning_rate": 0.00010641891891891893,
      "loss": 0.5351,
      "step": 657
    },
    {
      "epoch": 1.5348905109489053,
      "grad_norm": 3.2369234561920166,
      "learning_rate": 0.00010625000000000001,
      "loss": 0.3384,
      "step": 658
    },
    {
      "epoch": 1.537226277372263,
      "grad_norm": 4.175079822540283,
      "learning_rate": 0.00010608108108108107,
      "loss": 0.4487,
      "step": 659
    },
    {
      "epoch": 1.5395620437956206,
      "grad_norm": 5.7236151695251465,
      "learning_rate": 0.00010591216216216218,
      "loss": 0.9614,
      "step": 660
    },
    {
      "epoch": 1.5418978102189782,
      "grad_norm": 3.381983518600464,
      "learning_rate": 0.00010574324324324325,
      "loss": 0.4564,
      "step": 661
    },
    {
      "epoch": 1.5442335766423358,
      "grad_norm": 6.818088531494141,
      "learning_rate": 0.00010557432432432433,
      "loss": 0.3936,
      "step": 662
    },
    {
      "epoch": 1.5465693430656935,
      "grad_norm": 7.308190822601318,
      "learning_rate": 0.0001054054054054054,
      "loss": 0.2699,
      "step": 663
    },
    {
      "epoch": 1.5489051094890511,
      "grad_norm": 3.3792812824249268,
      "learning_rate": 0.00010523648648648649,
      "loss": 0.3429,
      "step": 664
    },
    {
      "epoch": 1.5512408759124088,
      "grad_norm": 4.2996978759765625,
      "learning_rate": 0.00010506756756756757,
      "loss": 0.6353,
      "step": 665
    },
    {
      "epoch": 1.5535766423357664,
      "grad_norm": 2.174722909927368,
      "learning_rate": 0.00010489864864864864,
      "loss": 0.4526,
      "step": 666
    },
    {
      "epoch": 1.555912408759124,
      "grad_norm": 6.984275817871094,
      "learning_rate": 0.00010472972972972975,
      "loss": 0.3693,
      "step": 667
    },
    {
      "epoch": 1.5582481751824817,
      "grad_norm": 6.572193622589111,
      "learning_rate": 0.00010456081081081081,
      "loss": 0.6364,
      "step": 668
    },
    {
      "epoch": 1.5605839416058394,
      "grad_norm": 3.966214656829834,
      "learning_rate": 0.0001043918918918919,
      "loss": 0.5351,
      "step": 669
    },
    {
      "epoch": 1.562919708029197,
      "grad_norm": 10.080382347106934,
      "learning_rate": 0.00010422297297297299,
      "loss": 0.7708,
      "step": 670
    },
    {
      "epoch": 1.5652554744525546,
      "grad_norm": 8.148269653320312,
      "learning_rate": 0.00010405405405405406,
      "loss": 0.5632,
      "step": 671
    },
    {
      "epoch": 1.5675912408759123,
      "grad_norm": 7.866185188293457,
      "learning_rate": 0.00010388513513513514,
      "loss": 0.5129,
      "step": 672
    },
    {
      "epoch": 1.56992700729927,
      "grad_norm": 15.55344295501709,
      "learning_rate": 0.00010371621621621622,
      "loss": 0.5961,
      "step": 673
    },
    {
      "epoch": 1.5722627737226276,
      "grad_norm": 8.532434463500977,
      "learning_rate": 0.00010354729729729731,
      "loss": 0.6066,
      "step": 674
    },
    {
      "epoch": 1.5745985401459854,
      "grad_norm": 9.259129524230957,
      "learning_rate": 0.00010337837837837838,
      "loss": 0.9552,
      "step": 675
    },
    {
      "epoch": 1.576934306569343,
      "grad_norm": 11.412428855895996,
      "learning_rate": 0.00010320945945945946,
      "loss": 0.6092,
      "step": 676
    },
    {
      "epoch": 1.5792700729927007,
      "grad_norm": 8.254620552062988,
      "learning_rate": 0.00010304054054054055,
      "loss": 0.4259,
      "step": 677
    },
    {
      "epoch": 1.5816058394160584,
      "grad_norm": 6.695162773132324,
      "learning_rate": 0.00010287162162162163,
      "loss": 0.6414,
      "step": 678
    },
    {
      "epoch": 1.583941605839416,
      "grad_norm": 4.476767063140869,
      "learning_rate": 0.0001027027027027027,
      "loss": 0.6573,
      "step": 679
    },
    {
      "epoch": 1.5862773722627739,
      "grad_norm": 11.431736946105957,
      "learning_rate": 0.0001025337837837838,
      "loss": 0.3196,
      "step": 680
    },
    {
      "epoch": 1.5886131386861315,
      "grad_norm": 5.516087055206299,
      "learning_rate": 0.00010236486486486488,
      "loss": 0.3636,
      "step": 681
    },
    {
      "epoch": 1.5909489051094892,
      "grad_norm": 5.934694290161133,
      "learning_rate": 0.00010219594594594594,
      "loss": 0.5307,
      "step": 682
    },
    {
      "epoch": 1.5932846715328468,
      "grad_norm": 6.353368759155273,
      "learning_rate": 0.00010202702702702702,
      "loss": 0.5079,
      "step": 683
    },
    {
      "epoch": 1.5956204379562045,
      "grad_norm": 5.840286731719971,
      "learning_rate": 0.00010185810810810812,
      "loss": 0.44,
      "step": 684
    },
    {
      "epoch": 1.597956204379562,
      "grad_norm": 4.594315528869629,
      "learning_rate": 0.0001016891891891892,
      "loss": 0.3712,
      "step": 685
    },
    {
      "epoch": 1.6002919708029197,
      "grad_norm": 5.00961971282959,
      "learning_rate": 0.00010152027027027027,
      "loss": 0.2485,
      "step": 686
    },
    {
      "epoch": 1.6026277372262774,
      "grad_norm": 6.938563823699951,
      "learning_rate": 0.00010135135135135136,
      "loss": 0.5145,
      "step": 687
    },
    {
      "epoch": 1.604963503649635,
      "grad_norm": 7.908092975616455,
      "learning_rate": 0.00010118243243243244,
      "loss": 0.6283,
      "step": 688
    },
    {
      "epoch": 1.6072992700729927,
      "grad_norm": 7.741745948791504,
      "learning_rate": 0.00010101351351351351,
      "loss": 0.7127,
      "step": 689
    },
    {
      "epoch": 1.6096350364963503,
      "grad_norm": 7.115564346313477,
      "learning_rate": 0.00010084459459459462,
      "loss": 0.6542,
      "step": 690
    },
    {
      "epoch": 1.611970802919708,
      "grad_norm": 8.60051441192627,
      "learning_rate": 0.00010067567567567568,
      "loss": 0.6274,
      "step": 691
    },
    {
      "epoch": 1.6143065693430656,
      "grad_norm": 7.327205181121826,
      "learning_rate": 0.00010050675675675676,
      "loss": 0.8438,
      "step": 692
    },
    {
      "epoch": 1.6166423357664232,
      "grad_norm": 10.39903450012207,
      "learning_rate": 0.00010033783783783783,
      "loss": 0.5581,
      "step": 693
    },
    {
      "epoch": 1.6189781021897809,
      "grad_norm": 3.0146172046661377,
      "learning_rate": 0.00010016891891891892,
      "loss": 0.0936,
      "step": 694
    },
    {
      "epoch": 1.6213138686131385,
      "grad_norm": 6.335277557373047,
      "learning_rate": 0.0001,
      "loss": 0.9107,
      "step": 695
    },
    {
      "epoch": 1.6236496350364964,
      "grad_norm": 7.396388530731201,
      "learning_rate": 9.983108108108109e-05,
      "loss": 0.9076,
      "step": 696
    },
    {
      "epoch": 1.625985401459854,
      "grad_norm": 5.695559978485107,
      "learning_rate": 9.966216216216217e-05,
      "loss": 0.2362,
      "step": 697
    },
    {
      "epoch": 1.6283211678832117,
      "grad_norm": 4.015809535980225,
      "learning_rate": 9.949324324324325e-05,
      "loss": 0.5169,
      "step": 698
    },
    {
      "epoch": 1.6306569343065693,
      "grad_norm": 17.317306518554688,
      "learning_rate": 9.932432432432433e-05,
      "loss": 1.0382,
      "step": 699
    },
    {
      "epoch": 1.632992700729927,
      "grad_norm": 3.0362491607666016,
      "learning_rate": 9.915540540540541e-05,
      "loss": 0.2081,
      "step": 700
    },
    {
      "epoch": 1.6353284671532848,
      "grad_norm": 4.432880878448486,
      "learning_rate": 9.89864864864865e-05,
      "loss": 0.6079,
      "step": 701
    },
    {
      "epoch": 1.6376642335766425,
      "grad_norm": 3.610684394836426,
      "learning_rate": 9.881756756756757e-05,
      "loss": 0.1609,
      "step": 702
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 7.466769218444824,
      "learning_rate": 9.864864864864865e-05,
      "loss": 0.6723,
      "step": 703
    },
    {
      "epoch": 1.6423357664233578,
      "grad_norm": 8.789939880371094,
      "learning_rate": 9.847972972972973e-05,
      "loss": 0.7148,
      "step": 704
    },
    {
      "epoch": 1.6446715328467154,
      "grad_norm": 7.652363300323486,
      "learning_rate": 9.831081081081081e-05,
      "loss": 0.8174,
      "step": 705
    },
    {
      "epoch": 1.647007299270073,
      "grad_norm": 4.464966773986816,
      "learning_rate": 9.81418918918919e-05,
      "loss": 0.5266,
      "step": 706
    },
    {
      "epoch": 1.6493430656934307,
      "grad_norm": 4.471757411956787,
      "learning_rate": 9.797297297297297e-05,
      "loss": 0.4773,
      "step": 707
    },
    {
      "epoch": 1.6516788321167883,
      "grad_norm": 5.342580795288086,
      "learning_rate": 9.780405405405407e-05,
      "loss": 0.4524,
      "step": 708
    },
    {
      "epoch": 1.654014598540146,
      "grad_norm": 2.4927785396575928,
      "learning_rate": 9.763513513513513e-05,
      "loss": 0.3224,
      "step": 709
    },
    {
      "epoch": 1.6563503649635036,
      "grad_norm": 9.321112632751465,
      "learning_rate": 9.746621621621623e-05,
      "loss": 0.7735,
      "step": 710
    },
    {
      "epoch": 1.6586861313868613,
      "grad_norm": 3.9159045219421387,
      "learning_rate": 9.729729729729731e-05,
      "loss": 0.6557,
      "step": 711
    },
    {
      "epoch": 1.661021897810219,
      "grad_norm": 11.882433891296387,
      "learning_rate": 9.712837837837838e-05,
      "loss": 0.8147,
      "step": 712
    },
    {
      "epoch": 1.6633576642335766,
      "grad_norm": 8.087427139282227,
      "learning_rate": 9.695945945945947e-05,
      "loss": 0.3455,
      "step": 713
    },
    {
      "epoch": 1.6656934306569342,
      "grad_norm": 5.530930042266846,
      "learning_rate": 9.679054054054054e-05,
      "loss": 0.5379,
      "step": 714
    },
    {
      "epoch": 1.6680291970802918,
      "grad_norm": 6.789733409881592,
      "learning_rate": 9.662162162162163e-05,
      "loss": 0.7093,
      "step": 715
    },
    {
      "epoch": 1.6703649635036495,
      "grad_norm": 8.884444236755371,
      "learning_rate": 9.64527027027027e-05,
      "loss": 0.8445,
      "step": 716
    },
    {
      "epoch": 1.6727007299270071,
      "grad_norm": 6.867832183837891,
      "learning_rate": 9.628378378378379e-05,
      "loss": 0.6742,
      "step": 717
    },
    {
      "epoch": 1.675036496350365,
      "grad_norm": 6.5125346183776855,
      "learning_rate": 9.611486486486487e-05,
      "loss": 1.2502,
      "step": 718
    },
    {
      "epoch": 1.6773722627737226,
      "grad_norm": 6.055851459503174,
      "learning_rate": 9.594594594594595e-05,
      "loss": 0.6021,
      "step": 719
    },
    {
      "epoch": 1.6797080291970803,
      "grad_norm": 9.317755699157715,
      "learning_rate": 9.577702702702703e-05,
      "loss": 0.7874,
      "step": 720
    },
    {
      "epoch": 1.682043795620438,
      "grad_norm": 2.699270009994507,
      "learning_rate": 9.56081081081081e-05,
      "loss": 0.3199,
      "step": 721
    },
    {
      "epoch": 1.6843795620437956,
      "grad_norm": 5.52823543548584,
      "learning_rate": 9.54391891891892e-05,
      "loss": 0.7441,
      "step": 722
    },
    {
      "epoch": 1.6867153284671534,
      "grad_norm": 5.798912525177002,
      "learning_rate": 9.527027027027028e-05,
      "loss": 0.4566,
      "step": 723
    },
    {
      "epoch": 1.689051094890511,
      "grad_norm": 4.842703819274902,
      "learning_rate": 9.510135135135136e-05,
      "loss": 0.6358,
      "step": 724
    },
    {
      "epoch": 1.6913868613138687,
      "grad_norm": 7.762813091278076,
      "learning_rate": 9.493243243243244e-05,
      "loss": 0.6095,
      "step": 725
    },
    {
      "epoch": 1.6937226277372264,
      "grad_norm": 9.04981517791748,
      "learning_rate": 9.476351351351352e-05,
      "loss": 0.6397,
      "step": 726
    },
    {
      "epoch": 1.696058394160584,
      "grad_norm": 4.081748962402344,
      "learning_rate": 9.45945945945946e-05,
      "loss": 0.4712,
      "step": 727
    },
    {
      "epoch": 1.6983941605839417,
      "grad_norm": 3.002992868423462,
      "learning_rate": 9.442567567567568e-05,
      "loss": 0.3303,
      "step": 728
    },
    {
      "epoch": 1.7007299270072993,
      "grad_norm": 8.112920761108398,
      "learning_rate": 9.425675675675676e-05,
      "loss": 0.6398,
      "step": 729
    },
    {
      "epoch": 1.703065693430657,
      "grad_norm": 2.937147378921509,
      "learning_rate": 9.408783783783784e-05,
      "loss": 0.5074,
      "step": 730
    },
    {
      "epoch": 1.7054014598540146,
      "grad_norm": 7.854313850402832,
      "learning_rate": 9.391891891891892e-05,
      "loss": 0.7879,
      "step": 731
    },
    {
      "epoch": 1.7077372262773722,
      "grad_norm": 5.467408657073975,
      "learning_rate": 9.375e-05,
      "loss": 0.4024,
      "step": 732
    },
    {
      "epoch": 1.7100729927007299,
      "grad_norm": 6.8627214431762695,
      "learning_rate": 9.35810810810811e-05,
      "loss": 0.9988,
      "step": 733
    },
    {
      "epoch": 1.7124087591240875,
      "grad_norm": 8.864520072937012,
      "learning_rate": 9.341216216216216e-05,
      "loss": 0.4539,
      "step": 734
    },
    {
      "epoch": 1.7147445255474452,
      "grad_norm": 7.680822849273682,
      "learning_rate": 9.324324324324324e-05,
      "loss": 0.3856,
      "step": 735
    },
    {
      "epoch": 1.7170802919708028,
      "grad_norm": 6.372358322143555,
      "learning_rate": 9.307432432432432e-05,
      "loss": 0.58,
      "step": 736
    },
    {
      "epoch": 1.7194160583941605,
      "grad_norm": 7.12156343460083,
      "learning_rate": 9.29054054054054e-05,
      "loss": 0.4265,
      "step": 737
    },
    {
      "epoch": 1.721751824817518,
      "grad_norm": 8.210593223571777,
      "learning_rate": 9.27364864864865e-05,
      "loss": 0.541,
      "step": 738
    },
    {
      "epoch": 1.724087591240876,
      "grad_norm": 4.329899787902832,
      "learning_rate": 9.256756756756757e-05,
      "loss": 0.2409,
      "step": 739
    },
    {
      "epoch": 1.7264233576642336,
      "grad_norm": 10.728020668029785,
      "learning_rate": 9.239864864864866e-05,
      "loss": 1.164,
      "step": 740
    },
    {
      "epoch": 1.7287591240875912,
      "grad_norm": 11.430648803710938,
      "learning_rate": 9.222972972972973e-05,
      "loss": 1.1504,
      "step": 741
    },
    {
      "epoch": 1.731094890510949,
      "grad_norm": 5.366810321807861,
      "learning_rate": 9.206081081081082e-05,
      "loss": 0.2267,
      "step": 742
    },
    {
      "epoch": 1.7334306569343065,
      "grad_norm": 9.164381980895996,
      "learning_rate": 9.18918918918919e-05,
      "loss": 0.5357,
      "step": 743
    },
    {
      "epoch": 1.7357664233576642,
      "grad_norm": 7.169971466064453,
      "learning_rate": 9.172297297297297e-05,
      "loss": 0.2801,
      "step": 744
    },
    {
      "epoch": 1.738102189781022,
      "grad_norm": 9.154067039489746,
      "learning_rate": 9.155405405405406e-05,
      "loss": 0.5593,
      "step": 745
    },
    {
      "epoch": 1.7404379562043797,
      "grad_norm": 7.944876194000244,
      "learning_rate": 9.138513513513513e-05,
      "loss": 0.8494,
      "step": 746
    },
    {
      "epoch": 1.7427737226277373,
      "grad_norm": 5.0841240882873535,
      "learning_rate": 9.121621621621623e-05,
      "loss": 0.4974,
      "step": 747
    },
    {
      "epoch": 1.745109489051095,
      "grad_norm": 7.702600002288818,
      "learning_rate": 9.10472972972973e-05,
      "loss": 0.6676,
      "step": 748
    },
    {
      "epoch": 1.7474452554744526,
      "grad_norm": 2.800055503845215,
      "learning_rate": 9.087837837837839e-05,
      "loss": 0.2103,
      "step": 749
    },
    {
      "epoch": 1.7497810218978103,
      "grad_norm": 4.414070129394531,
      "learning_rate": 9.070945945945947e-05,
      "loss": 0.4781,
      "step": 750
    },
    {
      "epoch": 1.752116788321168,
      "grad_norm": 7.716489791870117,
      "learning_rate": 9.054054054054055e-05,
      "loss": 0.7875,
      "step": 751
    },
    {
      "epoch": 1.7544525547445255,
      "grad_norm": 5.402260780334473,
      "learning_rate": 9.037162162162163e-05,
      "loss": 0.7658,
      "step": 752
    },
    {
      "epoch": 1.7567883211678832,
      "grad_norm": 8.164752960205078,
      "learning_rate": 9.02027027027027e-05,
      "loss": 0.6759,
      "step": 753
    },
    {
      "epoch": 1.7591240875912408,
      "grad_norm": 6.177701473236084,
      "learning_rate": 9.003378378378379e-05,
      "loss": 0.4236,
      "step": 754
    },
    {
      "epoch": 1.7614598540145985,
      "grad_norm": 13.205263137817383,
      "learning_rate": 8.986486486486487e-05,
      "loss": 0.8465,
      "step": 755
    },
    {
      "epoch": 1.7637956204379561,
      "grad_norm": 3.1871302127838135,
      "learning_rate": 8.969594594594595e-05,
      "loss": 0.5658,
      "step": 756
    },
    {
      "epoch": 1.7661313868613138,
      "grad_norm": 8.627514839172363,
      "learning_rate": 8.952702702702703e-05,
      "loss": 0.5112,
      "step": 757
    },
    {
      "epoch": 1.7684671532846714,
      "grad_norm": 11.200648307800293,
      "learning_rate": 8.935810810810811e-05,
      "loss": 0.6882,
      "step": 758
    },
    {
      "epoch": 1.770802919708029,
      "grad_norm": 5.658144950866699,
      "learning_rate": 8.918918918918919e-05,
      "loss": 0.4958,
      "step": 759
    },
    {
      "epoch": 1.7731386861313867,
      "grad_norm": 7.864781379699707,
      "learning_rate": 8.902027027027027e-05,
      "loss": 0.4482,
      "step": 760
    },
    {
      "epoch": 1.7754744525547446,
      "grad_norm": 11.996665954589844,
      "learning_rate": 8.885135135135135e-05,
      "loss": 0.7339,
      "step": 761
    },
    {
      "epoch": 1.7778102189781022,
      "grad_norm": 5.4629387855529785,
      "learning_rate": 8.868243243243243e-05,
      "loss": 0.3153,
      "step": 762
    },
    {
      "epoch": 1.7801459854014599,
      "grad_norm": 11.335296630859375,
      "learning_rate": 8.851351351351352e-05,
      "loss": 0.6374,
      "step": 763
    },
    {
      "epoch": 1.7824817518248175,
      "grad_norm": 10.425701141357422,
      "learning_rate": 8.83445945945946e-05,
      "loss": 0.4683,
      "step": 764
    },
    {
      "epoch": 1.7848175182481751,
      "grad_norm": 4.171191692352295,
      "learning_rate": 8.817567567567569e-05,
      "loss": 0.3546,
      "step": 765
    },
    {
      "epoch": 1.787153284671533,
      "grad_norm": 5.474599361419678,
      "learning_rate": 8.800675675675676e-05,
      "loss": 0.2327,
      "step": 766
    },
    {
      "epoch": 1.7894890510948906,
      "grad_norm": 6.994254112243652,
      "learning_rate": 8.783783783783784e-05,
      "loss": 0.6673,
      "step": 767
    },
    {
      "epoch": 1.7918248175182483,
      "grad_norm": 2.5388388633728027,
      "learning_rate": 8.766891891891892e-05,
      "loss": 0.2683,
      "step": 768
    },
    {
      "epoch": 1.794160583941606,
      "grad_norm": 2.1569769382476807,
      "learning_rate": 8.75e-05,
      "loss": 0.2165,
      "step": 769
    },
    {
      "epoch": 1.7964963503649636,
      "grad_norm": 14.788117408752441,
      "learning_rate": 8.73310810810811e-05,
      "loss": 0.8292,
      "step": 770
    },
    {
      "epoch": 1.7988321167883212,
      "grad_norm": 5.1682915687561035,
      "learning_rate": 8.716216216216216e-05,
      "loss": 0.3669,
      "step": 771
    },
    {
      "epoch": 1.8011678832116789,
      "grad_norm": 11.07063102722168,
      "learning_rate": 8.699324324324325e-05,
      "loss": 0.5261,
      "step": 772
    },
    {
      "epoch": 1.8035036496350365,
      "grad_norm": 9.72867202758789,
      "learning_rate": 8.682432432432432e-05,
      "loss": 1.0374,
      "step": 773
    },
    {
      "epoch": 1.8058394160583942,
      "grad_norm": 10.23404312133789,
      "learning_rate": 8.665540540540542e-05,
      "loss": 0.665,
      "step": 774
    },
    {
      "epoch": 1.8081751824817518,
      "grad_norm": 3.8607265949249268,
      "learning_rate": 8.64864864864865e-05,
      "loss": 0.7691,
      "step": 775
    },
    {
      "epoch": 1.8105109489051094,
      "grad_norm": 10.45194149017334,
      "learning_rate": 8.631756756756756e-05,
      "loss": 0.6352,
      "step": 776
    },
    {
      "epoch": 1.812846715328467,
      "grad_norm": 10.06537914276123,
      "learning_rate": 8.614864864864866e-05,
      "loss": 0.3309,
      "step": 777
    },
    {
      "epoch": 1.8151824817518247,
      "grad_norm": 6.594700813293457,
      "learning_rate": 8.597972972972972e-05,
      "loss": 0.3833,
      "step": 778
    },
    {
      "epoch": 1.8175182481751824,
      "grad_norm": 15.106181144714355,
      "learning_rate": 8.581081081081082e-05,
      "loss": 0.4646,
      "step": 779
    },
    {
      "epoch": 1.81985401459854,
      "grad_norm": 6.2566819190979,
      "learning_rate": 8.56418918918919e-05,
      "loss": 0.7308,
      "step": 780
    },
    {
      "epoch": 1.8221897810218977,
      "grad_norm": 13.65975284576416,
      "learning_rate": 8.547297297297298e-05,
      "loss": 0.8332,
      "step": 781
    },
    {
      "epoch": 1.8245255474452555,
      "grad_norm": 7.9029130935668945,
      "learning_rate": 8.530405405405406e-05,
      "loss": 0.8561,
      "step": 782
    },
    {
      "epoch": 1.8268613138686132,
      "grad_norm": 9.664346694946289,
      "learning_rate": 8.513513513513514e-05,
      "loss": 0.3977,
      "step": 783
    },
    {
      "epoch": 1.8291970802919708,
      "grad_norm": 8.059286117553711,
      "learning_rate": 8.496621621621622e-05,
      "loss": 0.6512,
      "step": 784
    },
    {
      "epoch": 1.8315328467153285,
      "grad_norm": 4.926674842834473,
      "learning_rate": 8.47972972972973e-05,
      "loss": 0.4484,
      "step": 785
    },
    {
      "epoch": 1.833868613138686,
      "grad_norm": 6.594981670379639,
      "learning_rate": 8.462837837837838e-05,
      "loss": 0.8009,
      "step": 786
    },
    {
      "epoch": 1.8362043795620437,
      "grad_norm": 5.48205041885376,
      "learning_rate": 8.445945945945946e-05,
      "loss": 0.669,
      "step": 787
    },
    {
      "epoch": 1.8385401459854016,
      "grad_norm": 9.313400268554688,
      "learning_rate": 8.429054054054054e-05,
      "loss": 0.9709,
      "step": 788
    },
    {
      "epoch": 1.8408759124087593,
      "grad_norm": 6.300424575805664,
      "learning_rate": 8.412162162162163e-05,
      "loss": 0.3302,
      "step": 789
    },
    {
      "epoch": 1.843211678832117,
      "grad_norm": 5.219817638397217,
      "learning_rate": 8.39527027027027e-05,
      "loss": 0.51,
      "step": 790
    },
    {
      "epoch": 1.8455474452554745,
      "grad_norm": 11.238627433776855,
      "learning_rate": 8.378378378378379e-05,
      "loss": 1.2093,
      "step": 791
    },
    {
      "epoch": 1.8478832116788322,
      "grad_norm": 3.201404571533203,
      "learning_rate": 8.361486486486487e-05,
      "loss": 0.3347,
      "step": 792
    },
    {
      "epoch": 1.8502189781021898,
      "grad_norm": 9.264090538024902,
      "learning_rate": 8.344594594594595e-05,
      "loss": 0.7534,
      "step": 793
    },
    {
      "epoch": 1.8525547445255475,
      "grad_norm": 5.413304328918457,
      "learning_rate": 8.327702702702703e-05,
      "loss": 0.2688,
      "step": 794
    },
    {
      "epoch": 1.8548905109489051,
      "grad_norm": 6.442502498626709,
      "learning_rate": 8.310810810810811e-05,
      "loss": 0.6241,
      "step": 795
    },
    {
      "epoch": 1.8572262773722628,
      "grad_norm": 4.822715759277344,
      "learning_rate": 8.293918918918919e-05,
      "loss": 0.4534,
      "step": 796
    },
    {
      "epoch": 1.8595620437956204,
      "grad_norm": 5.87774658203125,
      "learning_rate": 8.277027027027028e-05,
      "loss": 0.7609,
      "step": 797
    },
    {
      "epoch": 1.861897810218978,
      "grad_norm": 4.373615264892578,
      "learning_rate": 8.260135135135135e-05,
      "loss": 0.369,
      "step": 798
    },
    {
      "epoch": 1.8642335766423357,
      "grad_norm": 5.604488372802734,
      "learning_rate": 8.243243243243243e-05,
      "loss": 0.7319,
      "step": 799
    },
    {
      "epoch": 1.8665693430656933,
      "grad_norm": 7.652642726898193,
      "learning_rate": 8.226351351351351e-05,
      "loss": 0.3433,
      "step": 800
    },
    {
      "epoch": 1.868905109489051,
      "grad_norm": 7.868985176086426,
      "learning_rate": 8.209459459459459e-05,
      "loss": 0.653,
      "step": 801
    },
    {
      "epoch": 1.8712408759124086,
      "grad_norm": 6.0097174644470215,
      "learning_rate": 8.192567567567569e-05,
      "loss": 0.6854,
      "step": 802
    },
    {
      "epoch": 1.8735766423357663,
      "grad_norm": 5.696136474609375,
      "learning_rate": 8.175675675675675e-05,
      "loss": 0.4864,
      "step": 803
    },
    {
      "epoch": 1.8759124087591241,
      "grad_norm": 7.393330097198486,
      "learning_rate": 8.158783783783785e-05,
      "loss": 0.5796,
      "step": 804
    },
    {
      "epoch": 1.8782481751824818,
      "grad_norm": 6.777770042419434,
      "learning_rate": 8.141891891891892e-05,
      "loss": 0.3667,
      "step": 805
    },
    {
      "epoch": 1.8805839416058394,
      "grad_norm": 6.430943965911865,
      "learning_rate": 8.125000000000001e-05,
      "loss": 1.109,
      "step": 806
    },
    {
      "epoch": 1.882919708029197,
      "grad_norm": 7.180905818939209,
      "learning_rate": 8.108108108108109e-05,
      "loss": 0.9059,
      "step": 807
    },
    {
      "epoch": 1.8852554744525547,
      "grad_norm": 8.015392303466797,
      "learning_rate": 8.091216216216216e-05,
      "loss": 0.5721,
      "step": 808
    },
    {
      "epoch": 1.8875912408759126,
      "grad_norm": 3.9615654945373535,
      "learning_rate": 8.074324324324325e-05,
      "loss": 0.8498,
      "step": 809
    },
    {
      "epoch": 1.8899270072992702,
      "grad_norm": 2.0966739654541016,
      "learning_rate": 8.057432432432432e-05,
      "loss": 0.2857,
      "step": 810
    },
    {
      "epoch": 1.8922627737226279,
      "grad_norm": 3.7222416400909424,
      "learning_rate": 8.040540540540541e-05,
      "loss": 0.4261,
      "step": 811
    },
    {
      "epoch": 1.8945985401459855,
      "grad_norm": 6.91390323638916,
      "learning_rate": 8.02364864864865e-05,
      "loss": 0.6432,
      "step": 812
    },
    {
      "epoch": 1.8969343065693431,
      "grad_norm": 4.275774955749512,
      "learning_rate": 8.006756756756757e-05,
      "loss": 0.2677,
      "step": 813
    },
    {
      "epoch": 1.8992700729927008,
      "grad_norm": 11.872330665588379,
      "learning_rate": 7.989864864864865e-05,
      "loss": 0.7095,
      "step": 814
    },
    {
      "epoch": 1.9016058394160584,
      "grad_norm": 7.776261806488037,
      "learning_rate": 7.972972972972974e-05,
      "loss": 0.7095,
      "step": 815
    },
    {
      "epoch": 1.903941605839416,
      "grad_norm": 3.48480486869812,
      "learning_rate": 7.956081081081082e-05,
      "loss": 0.4235,
      "step": 816
    },
    {
      "epoch": 1.9062773722627737,
      "grad_norm": 4.007656574249268,
      "learning_rate": 7.93918918918919e-05,
      "loss": 0.5435,
      "step": 817
    },
    {
      "epoch": 1.9086131386861314,
      "grad_norm": 5.890603065490723,
      "learning_rate": 7.922297297297298e-05,
      "loss": 0.4225,
      "step": 818
    },
    {
      "epoch": 1.910948905109489,
      "grad_norm": 6.344011306762695,
      "learning_rate": 7.905405405405406e-05,
      "loss": 0.6283,
      "step": 819
    },
    {
      "epoch": 1.9132846715328466,
      "grad_norm": 12.553902626037598,
      "learning_rate": 7.888513513513514e-05,
      "loss": 0.7455,
      "step": 820
    },
    {
      "epoch": 1.9156204379562043,
      "grad_norm": 7.240469455718994,
      "learning_rate": 7.871621621621622e-05,
      "loss": 1.0912,
      "step": 821
    },
    {
      "epoch": 1.917956204379562,
      "grad_norm": 6.861145973205566,
      "learning_rate": 7.85472972972973e-05,
      "loss": 0.3608,
      "step": 822
    },
    {
      "epoch": 1.9202919708029196,
      "grad_norm": 5.755197048187256,
      "learning_rate": 7.837837837837838e-05,
      "loss": 0.302,
      "step": 823
    },
    {
      "epoch": 1.9226277372262772,
      "grad_norm": 6.767340660095215,
      "learning_rate": 7.820945945945946e-05,
      "loss": 0.7743,
      "step": 824
    },
    {
      "epoch": 1.924963503649635,
      "grad_norm": 3.5960168838500977,
      "learning_rate": 7.804054054054054e-05,
      "loss": 0.3406,
      "step": 825
    },
    {
      "epoch": 1.9272992700729927,
      "grad_norm": 16.97742462158203,
      "learning_rate": 7.787162162162162e-05,
      "loss": 0.9843,
      "step": 826
    },
    {
      "epoch": 1.9296350364963504,
      "grad_norm": 12.622058868408203,
      "learning_rate": 7.77027027027027e-05,
      "loss": 0.5092,
      "step": 827
    },
    {
      "epoch": 1.931970802919708,
      "grad_norm": 5.946456432342529,
      "learning_rate": 7.753378378378378e-05,
      "loss": 1.0189,
      "step": 828
    },
    {
      "epoch": 1.9343065693430657,
      "grad_norm": 5.274685382843018,
      "learning_rate": 7.736486486486488e-05,
      "loss": 0.2466,
      "step": 829
    },
    {
      "epoch": 1.9366423357664233,
      "grad_norm": 5.924295902252197,
      "learning_rate": 7.719594594594595e-05,
      "loss": 0.3196,
      "step": 830
    },
    {
      "epoch": 1.9389781021897812,
      "grad_norm": 6.2528076171875,
      "learning_rate": 7.702702702702703e-05,
      "loss": 0.9302,
      "step": 831
    },
    {
      "epoch": 1.9413138686131388,
      "grad_norm": 12.239986419677734,
      "learning_rate": 7.68581081081081e-05,
      "loss": 0.6695,
      "step": 832
    },
    {
      "epoch": 1.9436496350364965,
      "grad_norm": 4.528422832489014,
      "learning_rate": 7.668918918918919e-05,
      "loss": 0.1938,
      "step": 833
    },
    {
      "epoch": 1.945985401459854,
      "grad_norm": 6.725959777832031,
      "learning_rate": 7.652027027027028e-05,
      "loss": 0.6057,
      "step": 834
    },
    {
      "epoch": 1.9483211678832117,
      "grad_norm": 8.146342277526855,
      "learning_rate": 7.635135135135135e-05,
      "loss": 0.4361,
      "step": 835
    },
    {
      "epoch": 1.9506569343065694,
      "grad_norm": 4.520862579345703,
      "learning_rate": 7.618243243243244e-05,
      "loss": 0.3609,
      "step": 836
    },
    {
      "epoch": 1.952992700729927,
      "grad_norm": 5.147967338562012,
      "learning_rate": 7.601351351351351e-05,
      "loss": 0.2663,
      "step": 837
    },
    {
      "epoch": 1.9553284671532847,
      "grad_norm": 8.673822402954102,
      "learning_rate": 7.58445945945946e-05,
      "loss": 0.7339,
      "step": 838
    },
    {
      "epoch": 1.9576642335766423,
      "grad_norm": 11.509164810180664,
      "learning_rate": 7.567567567567568e-05,
      "loss": 0.4647,
      "step": 839
    },
    {
      "epoch": 1.96,
      "grad_norm": 6.331346035003662,
      "learning_rate": 7.550675675675675e-05,
      "loss": 0.9282,
      "step": 840
    },
    {
      "epoch": 1.9623357664233576,
      "grad_norm": 7.355047702789307,
      "learning_rate": 7.533783783783785e-05,
      "loss": 0.6907,
      "step": 841
    },
    {
      "epoch": 1.9646715328467153,
      "grad_norm": 10.260043144226074,
      "learning_rate": 7.516891891891891e-05,
      "loss": 1.1402,
      "step": 842
    },
    {
      "epoch": 1.967007299270073,
      "grad_norm": 7.546574592590332,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.4252,
      "step": 843
    },
    {
      "epoch": 1.9693430656934305,
      "grad_norm": 11.642980575561523,
      "learning_rate": 7.483108108108109e-05,
      "loss": 0.8872,
      "step": 844
    },
    {
      "epoch": 1.9716788321167882,
      "grad_norm": 6.593610763549805,
      "learning_rate": 7.466216216216217e-05,
      "loss": 0.7121,
      "step": 845
    },
    {
      "epoch": 1.9740145985401458,
      "grad_norm": 7.847708225250244,
      "learning_rate": 7.449324324324325e-05,
      "loss": 0.8081,
      "step": 846
    },
    {
      "epoch": 1.9763503649635037,
      "grad_norm": 2.751678705215454,
      "learning_rate": 7.432432432432433e-05,
      "loss": 0.3585,
      "step": 847
    },
    {
      "epoch": 1.9786861313868613,
      "grad_norm": 5.436757564544678,
      "learning_rate": 7.415540540540541e-05,
      "loss": 0.7153,
      "step": 848
    },
    {
      "epoch": 1.981021897810219,
      "grad_norm": 5.703161239624023,
      "learning_rate": 7.398648648648649e-05,
      "loss": 0.5551,
      "step": 849
    },
    {
      "epoch": 1.9833576642335766,
      "grad_norm": 8.755200386047363,
      "learning_rate": 7.381756756756757e-05,
      "loss": 0.3066,
      "step": 850
    },
    {
      "epoch": 1.9856934306569343,
      "grad_norm": 6.361809730529785,
      "learning_rate": 7.364864864864865e-05,
      "loss": 0.7629,
      "step": 851
    },
    {
      "epoch": 1.9880291970802921,
      "grad_norm": 4.203160285949707,
      "learning_rate": 7.347972972972973e-05,
      "loss": 0.3255,
      "step": 852
    },
    {
      "epoch": 1.9903649635036498,
      "grad_norm": 8.680867195129395,
      "learning_rate": 7.331081081081081e-05,
      "loss": 0.9802,
      "step": 853
    },
    {
      "epoch": 1.9927007299270074,
      "grad_norm": 8.313638687133789,
      "learning_rate": 7.31418918918919e-05,
      "loss": 0.5528,
      "step": 854
    },
    {
      "epoch": 1.995036496350365,
      "grad_norm": 4.292128086090088,
      "learning_rate": 7.297297297297297e-05,
      "loss": 0.5522,
      "step": 855
    },
    {
      "epoch": 1.9973722627737227,
      "grad_norm": 5.686972141265869,
      "learning_rate": 7.280405405405406e-05,
      "loss": 0.6308,
      "step": 856
    },
    {
      "epoch": 1.9997080291970804,
      "grad_norm": 3.4434292316436768,
      "learning_rate": 7.263513513513514e-05,
      "loss": 0.4767,
      "step": 857
    },
    {
      "epoch": 2.0,
      "grad_norm": 5.710915565490723,
      "learning_rate": 7.246621621621622e-05,
      "loss": 0.1631,
      "step": 858
    },
    {
      "epoch": 2.0023357664233576,
      "grad_norm": 3.5193257331848145,
      "learning_rate": 7.229729729729731e-05,
      "loss": 0.2169,
      "step": 859
    },
    {
      "epoch": 2.0046715328467153,
      "grad_norm": 3.4521801471710205,
      "learning_rate": 7.212837837837838e-05,
      "loss": 0.4965,
      "step": 860
    },
    {
      "epoch": 2.007007299270073,
      "grad_norm": 4.7394633293151855,
      "learning_rate": 7.195945945945947e-05,
      "loss": 0.6993,
      "step": 861
    },
    {
      "epoch": 2.0093430656934306,
      "grad_norm": 5.198781490325928,
      "learning_rate": 7.179054054054054e-05,
      "loss": 0.227,
      "step": 862
    },
    {
      "epoch": 2.011678832116788,
      "grad_norm": 2.3757126331329346,
      "learning_rate": 7.162162162162162e-05,
      "loss": 0.4303,
      "step": 863
    },
    {
      "epoch": 2.014014598540146,
      "grad_norm": 1.9989441633224487,
      "learning_rate": 7.14527027027027e-05,
      "loss": 0.2517,
      "step": 864
    },
    {
      "epoch": 2.0163503649635035,
      "grad_norm": 3.1931886672973633,
      "learning_rate": 7.128378378378378e-05,
      "loss": 0.3331,
      "step": 865
    },
    {
      "epoch": 2.018686131386861,
      "grad_norm": 15.270614624023438,
      "learning_rate": 7.111486486486488e-05,
      "loss": 0.5443,
      "step": 866
    },
    {
      "epoch": 2.021021897810219,
      "grad_norm": 15.663867950439453,
      "learning_rate": 7.094594594594594e-05,
      "loss": 0.2877,
      "step": 867
    },
    {
      "epoch": 2.0233576642335764,
      "grad_norm": 5.34295129776001,
      "learning_rate": 7.077702702702704e-05,
      "loss": 0.2963,
      "step": 868
    },
    {
      "epoch": 2.0256934306569345,
      "grad_norm": 15.347269058227539,
      "learning_rate": 7.06081081081081e-05,
      "loss": 0.4132,
      "step": 869
    },
    {
      "epoch": 2.028029197080292,
      "grad_norm": 0.7316969633102417,
      "learning_rate": 7.04391891891892e-05,
      "loss": 0.0311,
      "step": 870
    },
    {
      "epoch": 2.03036496350365,
      "grad_norm": 5.3987274169921875,
      "learning_rate": 7.027027027027028e-05,
      "loss": 0.4819,
      "step": 871
    },
    {
      "epoch": 2.0327007299270075,
      "grad_norm": 2.6656322479248047,
      "learning_rate": 7.010135135135135e-05,
      "loss": 0.1336,
      "step": 872
    },
    {
      "epoch": 2.035036496350365,
      "grad_norm": 9.894675254821777,
      "learning_rate": 6.993243243243244e-05,
      "loss": 0.3775,
      "step": 873
    },
    {
      "epoch": 2.0373722627737227,
      "grad_norm": 3.136289358139038,
      "learning_rate": 6.97635135135135e-05,
      "loss": 0.3309,
      "step": 874
    },
    {
      "epoch": 2.0397080291970804,
      "grad_norm": 2.6406772136688232,
      "learning_rate": 6.95945945945946e-05,
      "loss": 0.2046,
      "step": 875
    },
    {
      "epoch": 2.042043795620438,
      "grad_norm": 4.557493209838867,
      "learning_rate": 6.942567567567568e-05,
      "loss": 0.2511,
      "step": 876
    },
    {
      "epoch": 2.0443795620437957,
      "grad_norm": 5.871634006500244,
      "learning_rate": 6.925675675675676e-05,
      "loss": 0.2855,
      "step": 877
    },
    {
      "epoch": 2.0467153284671533,
      "grad_norm": 5.45845365524292,
      "learning_rate": 6.908783783783784e-05,
      "loss": 0.4166,
      "step": 878
    },
    {
      "epoch": 2.049051094890511,
      "grad_norm": 2.441868305206299,
      "learning_rate": 6.891891891891892e-05,
      "loss": 0.1038,
      "step": 879
    },
    {
      "epoch": 2.0513868613138686,
      "grad_norm": 8.736930847167969,
      "learning_rate": 6.875e-05,
      "loss": 0.275,
      "step": 880
    },
    {
      "epoch": 2.0537226277372262,
      "grad_norm": 5.6278228759765625,
      "learning_rate": 6.858108108108108e-05,
      "loss": 0.1015,
      "step": 881
    },
    {
      "epoch": 2.056058394160584,
      "grad_norm": 4.1336517333984375,
      "learning_rate": 6.841216216216217e-05,
      "loss": 0.5404,
      "step": 882
    },
    {
      "epoch": 2.0583941605839415,
      "grad_norm": 8.742549896240234,
      "learning_rate": 6.824324324324325e-05,
      "loss": 0.5303,
      "step": 883
    },
    {
      "epoch": 2.060729927007299,
      "grad_norm": 0.6207708120346069,
      "learning_rate": 6.807432432432433e-05,
      "loss": 0.0109,
      "step": 884
    },
    {
      "epoch": 2.063065693430657,
      "grad_norm": 2.903871536254883,
      "learning_rate": 6.790540540540541e-05,
      "loss": 0.1119,
      "step": 885
    },
    {
      "epoch": 2.0654014598540145,
      "grad_norm": 3.4572160243988037,
      "learning_rate": 6.773648648648649e-05,
      "loss": 0.1305,
      "step": 886
    },
    {
      "epoch": 2.067737226277372,
      "grad_norm": 7.5499653816223145,
      "learning_rate": 6.756756756756757e-05,
      "loss": 0.5174,
      "step": 887
    },
    {
      "epoch": 2.0700729927007298,
      "grad_norm": 21.599998474121094,
      "learning_rate": 6.739864864864865e-05,
      "loss": 0.2114,
      "step": 888
    },
    {
      "epoch": 2.0724087591240874,
      "grad_norm": 18.4743709564209,
      "learning_rate": 6.722972972972973e-05,
      "loss": 0.3486,
      "step": 889
    },
    {
      "epoch": 2.0747445255474455,
      "grad_norm": 4.870938777923584,
      "learning_rate": 6.706081081081081e-05,
      "loss": 0.3216,
      "step": 890
    },
    {
      "epoch": 2.077080291970803,
      "grad_norm": 6.600725173950195,
      "learning_rate": 6.68918918918919e-05,
      "loss": 0.2738,
      "step": 891
    },
    {
      "epoch": 2.0794160583941608,
      "grad_norm": 3.7362029552459717,
      "learning_rate": 6.672297297297297e-05,
      "loss": 0.0261,
      "step": 892
    },
    {
      "epoch": 2.0817518248175184,
      "grad_norm": 0.5769867897033691,
      "learning_rate": 6.655405405405407e-05,
      "loss": 0.0092,
      "step": 893
    },
    {
      "epoch": 2.084087591240876,
      "grad_norm": 8.954545974731445,
      "learning_rate": 6.638513513513513e-05,
      "loss": 0.3018,
      "step": 894
    },
    {
      "epoch": 2.0864233576642337,
      "grad_norm": 4.052148818969727,
      "learning_rate": 6.621621621621621e-05,
      "loss": 0.1783,
      "step": 895
    },
    {
      "epoch": 2.0887591240875913,
      "grad_norm": 12.204293251037598,
      "learning_rate": 6.604729729729731e-05,
      "loss": 0.6667,
      "step": 896
    },
    {
      "epoch": 2.091094890510949,
      "grad_norm": 9.673587799072266,
      "learning_rate": 6.587837837837837e-05,
      "loss": 0.3899,
      "step": 897
    },
    {
      "epoch": 2.0934306569343066,
      "grad_norm": 7.666903495788574,
      "learning_rate": 6.570945945945947e-05,
      "loss": 0.3085,
      "step": 898
    },
    {
      "epoch": 2.0957664233576643,
      "grad_norm": 6.683174133300781,
      "learning_rate": 6.554054054054054e-05,
      "loss": 0.431,
      "step": 899
    },
    {
      "epoch": 2.098102189781022,
      "grad_norm": 11.365082740783691,
      "learning_rate": 6.537162162162163e-05,
      "loss": 0.6539,
      "step": 900
    },
    {
      "epoch": 2.1004379562043796,
      "grad_norm": 14.250883102416992,
      "learning_rate": 6.52027027027027e-05,
      "loss": 0.5682,
      "step": 901
    },
    {
      "epoch": 2.102773722627737,
      "grad_norm": 11.06118106842041,
      "learning_rate": 6.503378378378379e-05,
      "loss": 0.3988,
      "step": 902
    },
    {
      "epoch": 2.105109489051095,
      "grad_norm": 10.322637557983398,
      "learning_rate": 6.486486486486487e-05,
      "loss": 0.4646,
      "step": 903
    },
    {
      "epoch": 2.1074452554744525,
      "grad_norm": 16.539932250976562,
      "learning_rate": 6.469594594594594e-05,
      "loss": 1.0152,
      "step": 904
    },
    {
      "epoch": 2.10978102189781,
      "grad_norm": 2.704709768295288,
      "learning_rate": 6.452702702702703e-05,
      "loss": 0.1629,
      "step": 905
    },
    {
      "epoch": 2.112116788321168,
      "grad_norm": 5.541931629180908,
      "learning_rate": 6.43581081081081e-05,
      "loss": 0.3369,
      "step": 906
    },
    {
      "epoch": 2.1144525547445254,
      "grad_norm": 3.5514438152313232,
      "learning_rate": 6.41891891891892e-05,
      "loss": 0.3446,
      "step": 907
    },
    {
      "epoch": 2.116788321167883,
      "grad_norm": 4.965665340423584,
      "learning_rate": 6.402027027027028e-05,
      "loss": 0.1839,
      "step": 908
    },
    {
      "epoch": 2.1191240875912407,
      "grad_norm": 4.940207481384277,
      "learning_rate": 6.385135135135136e-05,
      "loss": 0.2371,
      "step": 909
    },
    {
      "epoch": 2.1214598540145984,
      "grad_norm": 11.219749450683594,
      "learning_rate": 6.368243243243244e-05,
      "loss": 0.321,
      "step": 910
    },
    {
      "epoch": 2.123795620437956,
      "grad_norm": 6.928119659423828,
      "learning_rate": 6.351351351351352e-05,
      "loss": 0.0772,
      "step": 911
    },
    {
      "epoch": 2.1261313868613136,
      "grad_norm": 3.665438175201416,
      "learning_rate": 6.33445945945946e-05,
      "loss": 0.1343,
      "step": 912
    },
    {
      "epoch": 2.1284671532846717,
      "grad_norm": 3.9423961639404297,
      "learning_rate": 6.317567567567568e-05,
      "loss": 0.5438,
      "step": 913
    },
    {
      "epoch": 2.1308029197080294,
      "grad_norm": 2.9990127086639404,
      "learning_rate": 6.300675675675676e-05,
      "loss": 0.1043,
      "step": 914
    },
    {
      "epoch": 2.133138686131387,
      "grad_norm": 8.793244361877441,
      "learning_rate": 6.283783783783784e-05,
      "loss": 0.2803,
      "step": 915
    },
    {
      "epoch": 2.1354744525547447,
      "grad_norm": 5.563775062561035,
      "learning_rate": 6.266891891891892e-05,
      "loss": 0.5115,
      "step": 916
    },
    {
      "epoch": 2.1378102189781023,
      "grad_norm": 5.204919815063477,
      "learning_rate": 6.25e-05,
      "loss": 0.5002,
      "step": 917
    },
    {
      "epoch": 2.14014598540146,
      "grad_norm": 5.4704389572143555,
      "learning_rate": 6.233108108108108e-05,
      "loss": 0.4686,
      "step": 918
    },
    {
      "epoch": 2.1424817518248176,
      "grad_norm": 12.740675926208496,
      "learning_rate": 6.216216216216216e-05,
      "loss": 0.558,
      "step": 919
    },
    {
      "epoch": 2.1448175182481752,
      "grad_norm": 17.242788314819336,
      "learning_rate": 6.199324324324324e-05,
      "loss": 0.1946,
      "step": 920
    },
    {
      "epoch": 2.147153284671533,
      "grad_norm": 5.576873779296875,
      "learning_rate": 6.182432432432432e-05,
      "loss": 0.3643,
      "step": 921
    },
    {
      "epoch": 2.1494890510948905,
      "grad_norm": 8.494433403015137,
      "learning_rate": 6.16554054054054e-05,
      "loss": 0.2822,
      "step": 922
    },
    {
      "epoch": 2.151824817518248,
      "grad_norm": 10.136574745178223,
      "learning_rate": 6.14864864864865e-05,
      "loss": 0.3038,
      "step": 923
    },
    {
      "epoch": 2.154160583941606,
      "grad_norm": 4.512696266174316,
      "learning_rate": 6.131756756756757e-05,
      "loss": 0.2568,
      "step": 924
    },
    {
      "epoch": 2.1564963503649635,
      "grad_norm": 1.4485148191452026,
      "learning_rate": 6.114864864864866e-05,
      "loss": 0.0399,
      "step": 925
    },
    {
      "epoch": 2.158832116788321,
      "grad_norm": 4.001060962677002,
      "learning_rate": 6.097972972972973e-05,
      "loss": 0.1721,
      "step": 926
    },
    {
      "epoch": 2.1611678832116787,
      "grad_norm": 13.653802871704102,
      "learning_rate": 6.0810810810810814e-05,
      "loss": 0.3396,
      "step": 927
    },
    {
      "epoch": 2.1635036496350364,
      "grad_norm": 5.104618549346924,
      "learning_rate": 6.06418918918919e-05,
      "loss": 0.1777,
      "step": 928
    },
    {
      "epoch": 2.165839416058394,
      "grad_norm": 3.7685353755950928,
      "learning_rate": 6.0472972972972976e-05,
      "loss": 0.1288,
      "step": 929
    },
    {
      "epoch": 2.1681751824817517,
      "grad_norm": 7.519352912902832,
      "learning_rate": 6.030405405405406e-05,
      "loss": 0.2146,
      "step": 930
    },
    {
      "epoch": 2.1705109489051093,
      "grad_norm": 8.890201568603516,
      "learning_rate": 6.013513513513514e-05,
      "loss": 0.5174,
      "step": 931
    },
    {
      "epoch": 2.172846715328467,
      "grad_norm": 7.360520839691162,
      "learning_rate": 5.996621621621622e-05,
      "loss": 0.4361,
      "step": 932
    },
    {
      "epoch": 2.1751824817518246,
      "grad_norm": 5.629226207733154,
      "learning_rate": 5.9797297297297305e-05,
      "loss": 0.2376,
      "step": 933
    },
    {
      "epoch": 2.1775182481751827,
      "grad_norm": 9.71076774597168,
      "learning_rate": 5.962837837837838e-05,
      "loss": 0.2862,
      "step": 934
    },
    {
      "epoch": 2.1798540145985403,
      "grad_norm": 6.677799701690674,
      "learning_rate": 5.9459459459459466e-05,
      "loss": 0.4591,
      "step": 935
    },
    {
      "epoch": 2.182189781021898,
      "grad_norm": 9.98587703704834,
      "learning_rate": 5.929054054054054e-05,
      "loss": 0.2164,
      "step": 936
    },
    {
      "epoch": 2.1845255474452556,
      "grad_norm": 5.9787397384643555,
      "learning_rate": 5.912162162162163e-05,
      "loss": 0.2977,
      "step": 937
    },
    {
      "epoch": 2.1868613138686133,
      "grad_norm": 20.538846969604492,
      "learning_rate": 5.89527027027027e-05,
      "loss": 0.3006,
      "step": 938
    },
    {
      "epoch": 2.189197080291971,
      "grad_norm": 5.4462761878967285,
      "learning_rate": 5.878378378378379e-05,
      "loss": 0.2898,
      "step": 939
    },
    {
      "epoch": 2.1915328467153286,
      "grad_norm": 6.33899450302124,
      "learning_rate": 5.861486486486487e-05,
      "loss": 0.0399,
      "step": 940
    },
    {
      "epoch": 2.193868613138686,
      "grad_norm": 5.031274795532227,
      "learning_rate": 5.8445945945945943e-05,
      "loss": 0.1016,
      "step": 941
    },
    {
      "epoch": 2.196204379562044,
      "grad_norm": 6.107280731201172,
      "learning_rate": 5.827702702702703e-05,
      "loss": 0.3254,
      "step": 942
    },
    {
      "epoch": 2.1985401459854015,
      "grad_norm": 5.847208023071289,
      "learning_rate": 5.8108108108108105e-05,
      "loss": 0.1962,
      "step": 943
    },
    {
      "epoch": 2.200875912408759,
      "grad_norm": 5.28696870803833,
      "learning_rate": 5.793918918918919e-05,
      "loss": 0.4725,
      "step": 944
    },
    {
      "epoch": 2.2032116788321168,
      "grad_norm": 6.163674354553223,
      "learning_rate": 5.777027027027028e-05,
      "loss": 0.4869,
      "step": 945
    },
    {
      "epoch": 2.2055474452554744,
      "grad_norm": 7.307027816772461,
      "learning_rate": 5.760135135135135e-05,
      "loss": 0.4735,
      "step": 946
    },
    {
      "epoch": 2.207883211678832,
      "grad_norm": 7.912344932556152,
      "learning_rate": 5.7432432432432434e-05,
      "loss": 0.4709,
      "step": 947
    },
    {
      "epoch": 2.2102189781021897,
      "grad_norm": 2.0604419708251953,
      "learning_rate": 5.7263513513513515e-05,
      "loss": 0.234,
      "step": 948
    },
    {
      "epoch": 2.2125547445255473,
      "grad_norm": 4.151031494140625,
      "learning_rate": 5.7094594594594595e-05,
      "loss": 0.1088,
      "step": 949
    },
    {
      "epoch": 2.214890510948905,
      "grad_norm": 5.520337104797363,
      "learning_rate": 5.692567567567568e-05,
      "loss": 0.4036,
      "step": 950
    },
    {
      "epoch": 2.2172262773722626,
      "grad_norm": 11.598443031311035,
      "learning_rate": 5.6756756756756757e-05,
      "loss": 0.1728,
      "step": 951
    },
    {
      "epoch": 2.2195620437956203,
      "grad_norm": 5.263885498046875,
      "learning_rate": 5.6587837837837844e-05,
      "loss": 0.3258,
      "step": 952
    },
    {
      "epoch": 2.221897810218978,
      "grad_norm": 9.751086235046387,
      "learning_rate": 5.641891891891892e-05,
      "loss": 0.6168,
      "step": 953
    },
    {
      "epoch": 2.2242335766423356,
      "grad_norm": 4.615471363067627,
      "learning_rate": 5.6250000000000005e-05,
      "loss": 0.1761,
      "step": 954
    },
    {
      "epoch": 2.2265693430656937,
      "grad_norm": 3.3673477172851562,
      "learning_rate": 5.6081081081081086e-05,
      "loss": 0.3126,
      "step": 955
    },
    {
      "epoch": 2.2289051094890513,
      "grad_norm": 18.696788787841797,
      "learning_rate": 5.591216216216216e-05,
      "loss": 0.5653,
      "step": 956
    },
    {
      "epoch": 2.231240875912409,
      "grad_norm": 8.950966835021973,
      "learning_rate": 5.574324324324325e-05,
      "loss": 0.6135,
      "step": 957
    },
    {
      "epoch": 2.2335766423357666,
      "grad_norm": 2.295591354370117,
      "learning_rate": 5.557432432432432e-05,
      "loss": 0.0704,
      "step": 958
    },
    {
      "epoch": 2.2359124087591242,
      "grad_norm": 3.5784175395965576,
      "learning_rate": 5.540540540540541e-05,
      "loss": 0.2089,
      "step": 959
    },
    {
      "epoch": 2.238248175182482,
      "grad_norm": 4.266890525817871,
      "learning_rate": 5.5236486486486496e-05,
      "loss": 0.0442,
      "step": 960
    },
    {
      "epoch": 2.2405839416058395,
      "grad_norm": 6.149914741516113,
      "learning_rate": 5.506756756756757e-05,
      "loss": 0.3079,
      "step": 961
    },
    {
      "epoch": 2.242919708029197,
      "grad_norm": 4.920650959014893,
      "learning_rate": 5.489864864864866e-05,
      "loss": 0.2662,
      "step": 962
    },
    {
      "epoch": 2.245255474452555,
      "grad_norm": 7.316057205200195,
      "learning_rate": 5.472972972972973e-05,
      "loss": 0.2032,
      "step": 963
    },
    {
      "epoch": 2.2475912408759124,
      "grad_norm": 3.5024735927581787,
      "learning_rate": 5.456081081081081e-05,
      "loss": 0.2129,
      "step": 964
    },
    {
      "epoch": 2.24992700729927,
      "grad_norm": 4.966104507446289,
      "learning_rate": 5.43918918918919e-05,
      "loss": 0.2866,
      "step": 965
    },
    {
      "epoch": 2.2522627737226277,
      "grad_norm": 4.5798492431640625,
      "learning_rate": 5.422297297297297e-05,
      "loss": 0.1899,
      "step": 966
    },
    {
      "epoch": 2.2545985401459854,
      "grad_norm": 2.9142568111419678,
      "learning_rate": 5.405405405405406e-05,
      "loss": 0.3663,
      "step": 967
    },
    {
      "epoch": 2.256934306569343,
      "grad_norm": 5.859002590179443,
      "learning_rate": 5.3885135135135134e-05,
      "loss": 0.2209,
      "step": 968
    },
    {
      "epoch": 2.2592700729927007,
      "grad_norm": 2.200768232345581,
      "learning_rate": 5.371621621621622e-05,
      "loss": 0.0688,
      "step": 969
    },
    {
      "epoch": 2.2616058394160583,
      "grad_norm": 10.733940124511719,
      "learning_rate": 5.35472972972973e-05,
      "loss": 0.1902,
      "step": 970
    },
    {
      "epoch": 2.263941605839416,
      "grad_norm": 0.8198657035827637,
      "learning_rate": 5.337837837837838e-05,
      "loss": 0.0194,
      "step": 971
    },
    {
      "epoch": 2.2662773722627736,
      "grad_norm": 4.248816013336182,
      "learning_rate": 5.3209459459459463e-05,
      "loss": 0.2973,
      "step": 972
    },
    {
      "epoch": 2.2686131386861312,
      "grad_norm": 6.957678318023682,
      "learning_rate": 5.304054054054054e-05,
      "loss": 0.1564,
      "step": 973
    },
    {
      "epoch": 2.270948905109489,
      "grad_norm": 8.134377479553223,
      "learning_rate": 5.2871621621621625e-05,
      "loss": 0.6224,
      "step": 974
    },
    {
      "epoch": 2.2732846715328465,
      "grad_norm": 3.4277255535125732,
      "learning_rate": 5.27027027027027e-05,
      "loss": 0.2629,
      "step": 975
    },
    {
      "epoch": 2.2756204379562046,
      "grad_norm": 4.971540927886963,
      "learning_rate": 5.2533783783783786e-05,
      "loss": 0.0818,
      "step": 976
    },
    {
      "epoch": 2.277956204379562,
      "grad_norm": 8.655292510986328,
      "learning_rate": 5.2364864864864873e-05,
      "loss": 0.7079,
      "step": 977
    },
    {
      "epoch": 2.28029197080292,
      "grad_norm": 4.031152248382568,
      "learning_rate": 5.219594594594595e-05,
      "loss": 0.305,
      "step": 978
    },
    {
      "epoch": 2.2826277372262775,
      "grad_norm": 1.7274906635284424,
      "learning_rate": 5.202702702702703e-05,
      "loss": 0.0575,
      "step": 979
    },
    {
      "epoch": 2.284963503649635,
      "grad_norm": 4.045938968658447,
      "learning_rate": 5.185810810810811e-05,
      "loss": 0.3172,
      "step": 980
    },
    {
      "epoch": 2.287299270072993,
      "grad_norm": 8.902253150939941,
      "learning_rate": 5.168918918918919e-05,
      "loss": 0.3744,
      "step": 981
    },
    {
      "epoch": 2.2896350364963505,
      "grad_norm": 8.148984909057617,
      "learning_rate": 5.152027027027028e-05,
      "loss": 0.3007,
      "step": 982
    },
    {
      "epoch": 2.291970802919708,
      "grad_norm": 2.8056979179382324,
      "learning_rate": 5.135135135135135e-05,
      "loss": 0.1456,
      "step": 983
    },
    {
      "epoch": 2.2943065693430658,
      "grad_norm": 13.468132972717285,
      "learning_rate": 5.118243243243244e-05,
      "loss": 0.4157,
      "step": 984
    },
    {
      "epoch": 2.2966423357664234,
      "grad_norm": 2.9505951404571533,
      "learning_rate": 5.101351351351351e-05,
      "loss": 0.1049,
      "step": 985
    },
    {
      "epoch": 2.298978102189781,
      "grad_norm": 4.530311107635498,
      "learning_rate": 5.08445945945946e-05,
      "loss": 0.1989,
      "step": 986
    },
    {
      "epoch": 2.3013138686131387,
      "grad_norm": 6.364823341369629,
      "learning_rate": 5.067567567567568e-05,
      "loss": 0.1166,
      "step": 987
    },
    {
      "epoch": 2.3036496350364963,
      "grad_norm": 6.975858688354492,
      "learning_rate": 5.0506756756756754e-05,
      "loss": 0.1292,
      "step": 988
    },
    {
      "epoch": 2.305985401459854,
      "grad_norm": 7.167938709259033,
      "learning_rate": 5.033783783783784e-05,
      "loss": 0.7525,
      "step": 989
    },
    {
      "epoch": 2.3083211678832116,
      "grad_norm": 8.04529094696045,
      "learning_rate": 5.0168918918918915e-05,
      "loss": 0.3888,
      "step": 990
    },
    {
      "epoch": 2.3106569343065693,
      "grad_norm": 7.6360392570495605,
      "learning_rate": 5e-05,
      "loss": 0.5807,
      "step": 991
    },
    {
      "epoch": 2.312992700729927,
      "grad_norm": 6.393797874450684,
      "learning_rate": 4.983108108108108e-05,
      "loss": 0.1078,
      "step": 992
    },
    {
      "epoch": 2.3153284671532846,
      "grad_norm": 8.737953186035156,
      "learning_rate": 4.9662162162162164e-05,
      "loss": 0.1665,
      "step": 993
    },
    {
      "epoch": 2.317664233576642,
      "grad_norm": 24.268341064453125,
      "learning_rate": 4.949324324324325e-05,
      "loss": 0.4851,
      "step": 994
    },
    {
      "epoch": 2.32,
      "grad_norm": 11.714314460754395,
      "learning_rate": 4.9324324324324325e-05,
      "loss": 0.5899,
      "step": 995
    },
    {
      "epoch": 2.3223357664233575,
      "grad_norm": 7.104164123535156,
      "learning_rate": 4.9155405405405406e-05,
      "loss": 0.3701,
      "step": 996
    },
    {
      "epoch": 2.3246715328467156,
      "grad_norm": 8.714505195617676,
      "learning_rate": 4.8986486486486486e-05,
      "loss": 0.2722,
      "step": 997
    },
    {
      "epoch": 2.3270072992700728,
      "grad_norm": 4.1314239501953125,
      "learning_rate": 4.881756756756757e-05,
      "loss": 0.3256,
      "step": 998
    },
    {
      "epoch": 2.329343065693431,
      "grad_norm": 11.892415046691895,
      "learning_rate": 4.8648648648648654e-05,
      "loss": 0.3508,
      "step": 999
    },
    {
      "epoch": 2.3316788321167885,
      "grad_norm": 4.673643112182617,
      "learning_rate": 4.8479729729729735e-05,
      "loss": 0.3089,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1284,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5555734575983206e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
