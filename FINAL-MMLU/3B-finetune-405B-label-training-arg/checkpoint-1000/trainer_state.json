{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.3316788321167885,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0023357664233576644,
      "grad_norm": 9.646712303161621,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.3263,
      "step": 1
    },
    {
      "epoch": 0.004671532846715329,
      "grad_norm": 3.3205323219299316,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.4875,
      "step": 2
    },
    {
      "epoch": 0.0070072992700729924,
      "grad_norm": 1.9079598188400269,
      "learning_rate": 6e-06,
      "loss": 1.0041,
      "step": 3
    },
    {
      "epoch": 0.009343065693430658,
      "grad_norm": 1.8188966512680054,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.6704,
      "step": 4
    },
    {
      "epoch": 0.01167883211678832,
      "grad_norm": 2.8920953273773193,
      "learning_rate": 1e-05,
      "loss": 1.2237,
      "step": 5
    },
    {
      "epoch": 0.014014598540145985,
      "grad_norm": 4.758312702178955,
      "learning_rate": 1.2e-05,
      "loss": 2.0003,
      "step": 6
    },
    {
      "epoch": 0.01635036496350365,
      "grad_norm": 3.2056708335876465,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.7679,
      "step": 7
    },
    {
      "epoch": 0.018686131386861315,
      "grad_norm": 5.192562580108643,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.2613,
      "step": 8
    },
    {
      "epoch": 0.021021897810218976,
      "grad_norm": 4.261957168579102,
      "learning_rate": 1.8e-05,
      "loss": 1.9933,
      "step": 9
    },
    {
      "epoch": 0.02335766423357664,
      "grad_norm": 1.6321076154708862,
      "learning_rate": 2e-05,
      "loss": 0.9605,
      "step": 10
    },
    {
      "epoch": 0.025693430656934305,
      "grad_norm": 7.087320327758789,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.8818,
      "step": 11
    },
    {
      "epoch": 0.02802919708029197,
      "grad_norm": 8.284543991088867,
      "learning_rate": 2.4e-05,
      "loss": 1.9022,
      "step": 12
    },
    {
      "epoch": 0.030364963503649634,
      "grad_norm": 3.1102190017700195,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.8716,
      "step": 13
    },
    {
      "epoch": 0.0327007299270073,
      "grad_norm": 3.828200578689575,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.7729,
      "step": 14
    },
    {
      "epoch": 0.035036496350364967,
      "grad_norm": 6.7098870277404785,
      "learning_rate": 3e-05,
      "loss": 1.5776,
      "step": 15
    },
    {
      "epoch": 0.03737226277372263,
      "grad_norm": 2.4126808643341064,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.2101,
      "step": 16
    },
    {
      "epoch": 0.039708029197080295,
      "grad_norm": 2.7332799434661865,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.3022,
      "step": 17
    },
    {
      "epoch": 0.04204379562043795,
      "grad_norm": 4.498257637023926,
      "learning_rate": 3.6e-05,
      "loss": 1.6632,
      "step": 18
    },
    {
      "epoch": 0.04437956204379562,
      "grad_norm": 2.053090810775757,
      "learning_rate": 3.8e-05,
      "loss": 1.9096,
      "step": 19
    },
    {
      "epoch": 0.04671532846715328,
      "grad_norm": 1.75882089138031,
      "learning_rate": 4e-05,
      "loss": 0.6311,
      "step": 20
    },
    {
      "epoch": 0.049051094890510946,
      "grad_norm": 3.4434094429016113,
      "learning_rate": 4.2e-05,
      "loss": 1.4673,
      "step": 21
    },
    {
      "epoch": 0.05138686131386861,
      "grad_norm": 1.6319960355758667,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.3918,
      "step": 22
    },
    {
      "epoch": 0.053722627737226275,
      "grad_norm": 3.798489809036255,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.2851,
      "step": 23
    },
    {
      "epoch": 0.05605839416058394,
      "grad_norm": 3.884711265563965,
      "learning_rate": 4.8e-05,
      "loss": 2.076,
      "step": 24
    },
    {
      "epoch": 0.058394160583941604,
      "grad_norm": 2.033061981201172,
      "learning_rate": 5e-05,
      "loss": 1.196,
      "step": 25
    },
    {
      "epoch": 0.06072992700729927,
      "grad_norm": 3.2974166870117188,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.8049,
      "step": 26
    },
    {
      "epoch": 0.06306569343065693,
      "grad_norm": 5.183714389801025,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 1.0151,
      "step": 27
    },
    {
      "epoch": 0.0654014598540146,
      "grad_norm": 1.782165765762329,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 1.0448,
      "step": 28
    },
    {
      "epoch": 0.06773722627737226,
      "grad_norm": 3.089111328125,
      "learning_rate": 5.8e-05,
      "loss": 2.172,
      "step": 29
    },
    {
      "epoch": 0.07007299270072993,
      "grad_norm": 2.176145076751709,
      "learning_rate": 6e-05,
      "loss": 1.0168,
      "step": 30
    },
    {
      "epoch": 0.07240875912408759,
      "grad_norm": 3.054516315460205,
      "learning_rate": 6.2e-05,
      "loss": 1.1175,
      "step": 31
    },
    {
      "epoch": 0.07474452554744526,
      "grad_norm": 4.146263599395752,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.8582,
      "step": 32
    },
    {
      "epoch": 0.07708029197080292,
      "grad_norm": 4.224486351013184,
      "learning_rate": 6.6e-05,
      "loss": 1.3875,
      "step": 33
    },
    {
      "epoch": 0.07941605839416059,
      "grad_norm": 3.0773675441741943,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.7038,
      "step": 34
    },
    {
      "epoch": 0.08175182481751825,
      "grad_norm": 4.232114791870117,
      "learning_rate": 7e-05,
      "loss": 0.8837,
      "step": 35
    },
    {
      "epoch": 0.0840875912408759,
      "grad_norm": 3.222210645675659,
      "learning_rate": 7.2e-05,
      "loss": 1.2624,
      "step": 36
    },
    {
      "epoch": 0.08642335766423358,
      "grad_norm": 2.7065463066101074,
      "learning_rate": 7.4e-05,
      "loss": 0.6411,
      "step": 37
    },
    {
      "epoch": 0.08875912408759123,
      "grad_norm": 4.730200290679932,
      "learning_rate": 7.6e-05,
      "loss": 1.3452,
      "step": 38
    },
    {
      "epoch": 0.0910948905109489,
      "grad_norm": NaN,
      "learning_rate": 7.6e-05,
      "loss": 0.879,
      "step": 39
    },
    {
      "epoch": 0.09343065693430656,
      "grad_norm": 2.1229093074798584,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.8042,
      "step": 40
    },
    {
      "epoch": 0.09576642335766423,
      "grad_norm": 3.7009143829345703,
      "learning_rate": 8e-05,
      "loss": 0.9811,
      "step": 41
    },
    {
      "epoch": 0.09810218978102189,
      "grad_norm": 1.4300702810287476,
      "learning_rate": 8.2e-05,
      "loss": 1.0926,
      "step": 42
    },
    {
      "epoch": 0.10043795620437956,
      "grad_norm": 4.517112731933594,
      "learning_rate": 8.4e-05,
      "loss": 1.1061,
      "step": 43
    },
    {
      "epoch": 0.10277372262773722,
      "grad_norm": 4.657340049743652,
      "learning_rate": 8.6e-05,
      "loss": 1.268,
      "step": 44
    },
    {
      "epoch": 0.10510948905109489,
      "grad_norm": 3.1248056888580322,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.8928,
      "step": 45
    },
    {
      "epoch": 0.10744525547445255,
      "grad_norm": 3.3580052852630615,
      "learning_rate": 9e-05,
      "loss": 1.3477,
      "step": 46
    },
    {
      "epoch": 0.10978102189781022,
      "grad_norm": 7.336539268493652,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.2007,
      "step": 47
    },
    {
      "epoch": 0.11211678832116788,
      "grad_norm": 2.8637797832489014,
      "learning_rate": 9.4e-05,
      "loss": 1.3874,
      "step": 48
    },
    {
      "epoch": 0.11445255474452555,
      "grad_norm": 1.9749127626419067,
      "learning_rate": 9.6e-05,
      "loss": 0.9476,
      "step": 49
    },
    {
      "epoch": 0.11678832116788321,
      "grad_norm": 2.8057267665863037,
      "learning_rate": 9.8e-05,
      "loss": 0.6075,
      "step": 50
    },
    {
      "epoch": 0.11912408759124088,
      "grad_norm": 6.22161340713501,
      "learning_rate": 0.0001,
      "loss": 0.746,
      "step": 51
    },
    {
      "epoch": 0.12145985401459854,
      "grad_norm": 8.288653373718262,
      "learning_rate": 0.00010200000000000001,
      "loss": 0.8826,
      "step": 52
    },
    {
      "epoch": 0.12379562043795621,
      "grad_norm": 6.350207328796387,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.7846,
      "step": 53
    },
    {
      "epoch": 0.12613138686131387,
      "grad_norm": 5.674867153167725,
      "learning_rate": 0.00010600000000000002,
      "loss": 0.8584,
      "step": 54
    },
    {
      "epoch": 0.12846715328467154,
      "grad_norm": 5.566493511199951,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.9449,
      "step": 55
    },
    {
      "epoch": 0.1308029197080292,
      "grad_norm": 5.459056377410889,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.3381,
      "step": 56
    },
    {
      "epoch": 0.13313868613138685,
      "grad_norm": 1.8937464952468872,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.9149,
      "step": 57
    },
    {
      "epoch": 0.13547445255474452,
      "grad_norm": 1.8422143459320068,
      "learning_rate": 0.00011399999999999999,
      "loss": 2.2569,
      "step": 58
    },
    {
      "epoch": 0.1378102189781022,
      "grad_norm": 6.235785007476807,
      "learning_rate": 0.000116,
      "loss": 0.6543,
      "step": 59
    },
    {
      "epoch": 0.14014598540145987,
      "grad_norm": 5.300136566162109,
      "learning_rate": 0.000118,
      "loss": 0.7906,
      "step": 60
    },
    {
      "epoch": 0.1424817518248175,
      "grad_norm": 4.158382892608643,
      "learning_rate": 0.00012,
      "loss": 0.5961,
      "step": 61
    },
    {
      "epoch": 0.14481751824817518,
      "grad_norm": 3.2228128910064697,
      "learning_rate": 0.000122,
      "loss": 0.831,
      "step": 62
    },
    {
      "epoch": 0.14715328467153285,
      "grad_norm": 8.936296463012695,
      "learning_rate": 0.000124,
      "loss": 0.9846,
      "step": 63
    },
    {
      "epoch": 0.14948905109489052,
      "grad_norm": 1.4789657592773438,
      "learning_rate": 0.000126,
      "loss": 0.9214,
      "step": 64
    },
    {
      "epoch": 0.15182481751824817,
      "grad_norm": 2.7812132835388184,
      "learning_rate": 0.00012800000000000002,
      "loss": 1.4278,
      "step": 65
    },
    {
      "epoch": 0.15416058394160584,
      "grad_norm": 3.0558204650878906,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.6442,
      "step": 66
    },
    {
      "epoch": 0.1564963503649635,
      "grad_norm": 3.9644575119018555,
      "learning_rate": 0.000132,
      "loss": 0.9519,
      "step": 67
    },
    {
      "epoch": 0.15883211678832118,
      "grad_norm": 3.429780960083008,
      "learning_rate": 0.000134,
      "loss": 0.7918,
      "step": 68
    },
    {
      "epoch": 0.16116788321167883,
      "grad_norm": 12.62124252319336,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.9994,
      "step": 69
    },
    {
      "epoch": 0.1635036496350365,
      "grad_norm": 9.589418411254883,
      "learning_rate": 0.000138,
      "loss": 0.8139,
      "step": 70
    },
    {
      "epoch": 0.16583941605839417,
      "grad_norm": 8.263140678405762,
      "learning_rate": 0.00014,
      "loss": 1.0465,
      "step": 71
    },
    {
      "epoch": 0.1681751824817518,
      "grad_norm": 6.109710216522217,
      "learning_rate": 0.000142,
      "loss": 0.632,
      "step": 72
    },
    {
      "epoch": 0.17051094890510948,
      "grad_norm": 3.7016868591308594,
      "learning_rate": 0.000144,
      "loss": 1.4703,
      "step": 73
    },
    {
      "epoch": 0.17284671532846715,
      "grad_norm": 12.745828628540039,
      "learning_rate": 0.000146,
      "loss": 1.2512,
      "step": 74
    },
    {
      "epoch": 0.17518248175182483,
      "grad_norm": 4.832345962524414,
      "learning_rate": 0.000148,
      "loss": 0.7822,
      "step": 75
    },
    {
      "epoch": 0.17751824817518247,
      "grad_norm": 6.094310283660889,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.9264,
      "step": 76
    },
    {
      "epoch": 0.17985401459854014,
      "grad_norm": 1.7915579080581665,
      "learning_rate": 0.000152,
      "loss": 2.5225,
      "step": 77
    },
    {
      "epoch": 0.1821897810218978,
      "grad_norm": 6.176816463470459,
      "learning_rate": 0.000154,
      "loss": 1.0799,
      "step": 78
    },
    {
      "epoch": 0.18452554744525548,
      "grad_norm": 4.586825370788574,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.6814,
      "step": 79
    },
    {
      "epoch": 0.18686131386861313,
      "grad_norm": 5.457256317138672,
      "learning_rate": 0.00015800000000000002,
      "loss": 1.2452,
      "step": 80
    },
    {
      "epoch": 0.1891970802919708,
      "grad_norm": 11.090995788574219,
      "learning_rate": 0.00016,
      "loss": 0.7229,
      "step": 81
    },
    {
      "epoch": 0.19153284671532847,
      "grad_norm": 2.7039639949798584,
      "learning_rate": 0.000162,
      "loss": 1.2886,
      "step": 82
    },
    {
      "epoch": 0.19386861313868614,
      "grad_norm": 2.5039045810699463,
      "learning_rate": 0.000164,
      "loss": 1.6096,
      "step": 83
    },
    {
      "epoch": 0.19620437956204378,
      "grad_norm": 3.9964723587036133,
      "learning_rate": 0.000166,
      "loss": 1.571,
      "step": 84
    },
    {
      "epoch": 0.19854014598540146,
      "grad_norm": 2.3026864528656006,
      "learning_rate": 0.000168,
      "loss": 1.3805,
      "step": 85
    },
    {
      "epoch": 0.20087591240875913,
      "grad_norm": 4.709723949432373,
      "learning_rate": 0.00017,
      "loss": 0.9795,
      "step": 86
    },
    {
      "epoch": 0.2032116788321168,
      "grad_norm": 5.5897955894470215,
      "learning_rate": 0.000172,
      "loss": 0.7584,
      "step": 87
    },
    {
      "epoch": 0.20554744525547444,
      "grad_norm": 2.530242443084717,
      "learning_rate": 0.000174,
      "loss": 1.0532,
      "step": 88
    },
    {
      "epoch": 0.2078832116788321,
      "grad_norm": 2.2790944576263428,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.3921,
      "step": 89
    },
    {
      "epoch": 0.21021897810218979,
      "grad_norm": 1.7932714223861694,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.5456,
      "step": 90
    },
    {
      "epoch": 0.21255474452554746,
      "grad_norm": 2.400456666946411,
      "learning_rate": 0.00018,
      "loss": 0.8194,
      "step": 91
    },
    {
      "epoch": 0.2148905109489051,
      "grad_norm": 3.069441080093384,
      "learning_rate": 0.000182,
      "loss": 0.5116,
      "step": 92
    },
    {
      "epoch": 0.21722627737226277,
      "grad_norm": 6.33929443359375,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.4176,
      "step": 93
    },
    {
      "epoch": 0.21956204379562044,
      "grad_norm": 3.2925233840942383,
      "learning_rate": 0.00018600000000000002,
      "loss": 0.696,
      "step": 94
    },
    {
      "epoch": 0.22189781021897811,
      "grad_norm": 1.1584466695785522,
      "learning_rate": 0.000188,
      "loss": 0.5074,
      "step": 95
    },
    {
      "epoch": 0.22423357664233576,
      "grad_norm": 1.8245811462402344,
      "learning_rate": 0.00019,
      "loss": 2.402,
      "step": 96
    },
    {
      "epoch": 0.22656934306569343,
      "grad_norm": 4.843524932861328,
      "learning_rate": 0.000192,
      "loss": 1.1224,
      "step": 97
    },
    {
      "epoch": 0.2289051094890511,
      "grad_norm": 2.5633246898651123,
      "learning_rate": 0.000194,
      "loss": 1.3858,
      "step": 98
    },
    {
      "epoch": 0.23124087591240877,
      "grad_norm": 3.289133310317993,
      "learning_rate": 0.000196,
      "loss": 1.2974,
      "step": 99
    },
    {
      "epoch": 0.23357664233576642,
      "grad_norm": 2.673614501953125,
      "learning_rate": 0.00019800000000000002,
      "loss": 1.8721,
      "step": 100
    },
    {
      "epoch": 0.2359124087591241,
      "grad_norm": 6.7372541427612305,
      "learning_rate": 0.0002,
      "loss": 0.6603,
      "step": 101
    },
    {
      "epoch": 0.23824817518248176,
      "grad_norm": 6.448697090148926,
      "learning_rate": 0.0001998310810810811,
      "loss": 0.6618,
      "step": 102
    },
    {
      "epoch": 0.24058394160583943,
      "grad_norm": 6.816660404205322,
      "learning_rate": 0.00019966216216216217,
      "loss": 0.9092,
      "step": 103
    },
    {
      "epoch": 0.24291970802919707,
      "grad_norm": 4.490960121154785,
      "learning_rate": 0.00019949324324324325,
      "loss": 1.095,
      "step": 104
    },
    {
      "epoch": 0.24525547445255474,
      "grad_norm": 4.718636512756348,
      "learning_rate": 0.00019932432432432433,
      "loss": 1.037,
      "step": 105
    },
    {
      "epoch": 0.24759124087591242,
      "grad_norm": 4.164815902709961,
      "learning_rate": 0.0001991554054054054,
      "loss": 0.5081,
      "step": 106
    },
    {
      "epoch": 0.24992700729927006,
      "grad_norm": 7.014705657958984,
      "learning_rate": 0.0001989864864864865,
      "loss": 0.7693,
      "step": 107
    },
    {
      "epoch": 0.25226277372262773,
      "grad_norm": 3.371401786804199,
      "learning_rate": 0.00019881756756756757,
      "loss": 1.2052,
      "step": 108
    },
    {
      "epoch": 0.2545985401459854,
      "grad_norm": 3.2324812412261963,
      "learning_rate": 0.00019864864864864865,
      "loss": 0.6789,
      "step": 109
    },
    {
      "epoch": 0.2569343065693431,
      "grad_norm": 3.3694939613342285,
      "learning_rate": 0.00019847972972972974,
      "loss": 0.3494,
      "step": 110
    },
    {
      "epoch": 0.2592700729927007,
      "grad_norm": 4.961350917816162,
      "learning_rate": 0.00019831081081081082,
      "loss": 1.2359,
      "step": 111
    },
    {
      "epoch": 0.2616058394160584,
      "grad_norm": 1.8468223810195923,
      "learning_rate": 0.0001981418918918919,
      "loss": 0.8174,
      "step": 112
    },
    {
      "epoch": 0.26394160583941606,
      "grad_norm": 2.055049419403076,
      "learning_rate": 0.000197972972972973,
      "loss": 0.7379,
      "step": 113
    },
    {
      "epoch": 0.2662773722627737,
      "grad_norm": 3.0730557441711426,
      "learning_rate": 0.00019780405405405406,
      "loss": 0.9551,
      "step": 114
    },
    {
      "epoch": 0.2686131386861314,
      "grad_norm": 3.2306220531463623,
      "learning_rate": 0.00019763513513513514,
      "loss": 1.4238,
      "step": 115
    },
    {
      "epoch": 0.27094890510948905,
      "grad_norm": 2.8618807792663574,
      "learning_rate": 0.00019746621621621622,
      "loss": 0.5734,
      "step": 116
    },
    {
      "epoch": 0.2732846715328467,
      "grad_norm": 4.845156669616699,
      "learning_rate": 0.0001972972972972973,
      "loss": 0.5135,
      "step": 117
    },
    {
      "epoch": 0.2756204379562044,
      "grad_norm": 3.5995192527770996,
      "learning_rate": 0.00019712837837837838,
      "loss": 1.6713,
      "step": 118
    },
    {
      "epoch": 0.27795620437956203,
      "grad_norm": 2.82346510887146,
      "learning_rate": 0.00019695945945945946,
      "loss": 1.0662,
      "step": 119
    },
    {
      "epoch": 0.28029197080291973,
      "grad_norm": 3.5851430892944336,
      "learning_rate": 0.00019679054054054057,
      "loss": 0.7344,
      "step": 120
    },
    {
      "epoch": 0.2826277372262774,
      "grad_norm": 7.349481582641602,
      "learning_rate": 0.00019662162162162162,
      "loss": 0.7918,
      "step": 121
    },
    {
      "epoch": 0.284963503649635,
      "grad_norm": 6.554172992706299,
      "learning_rate": 0.0001964527027027027,
      "loss": 0.8491,
      "step": 122
    },
    {
      "epoch": 0.2872992700729927,
      "grad_norm": 13.40595531463623,
      "learning_rate": 0.0001962837837837838,
      "loss": 1.074,
      "step": 123
    },
    {
      "epoch": 0.28963503649635036,
      "grad_norm": 3.787529230117798,
      "learning_rate": 0.0001961148648648649,
      "loss": 0.506,
      "step": 124
    },
    {
      "epoch": 0.291970802919708,
      "grad_norm": 3.3496577739715576,
      "learning_rate": 0.00019594594594594594,
      "loss": 1.0336,
      "step": 125
    },
    {
      "epoch": 0.2943065693430657,
      "grad_norm": 4.72794771194458,
      "learning_rate": 0.00019577702702702703,
      "loss": 0.6604,
      "step": 126
    },
    {
      "epoch": 0.29664233576642335,
      "grad_norm": 2.9885456562042236,
      "learning_rate": 0.00019560810810810813,
      "loss": 0.8823,
      "step": 127
    },
    {
      "epoch": 0.29897810218978105,
      "grad_norm": 4.504444599151611,
      "learning_rate": 0.0001954391891891892,
      "loss": 1.8077,
      "step": 128
    },
    {
      "epoch": 0.3013138686131387,
      "grad_norm": 1.841825008392334,
      "learning_rate": 0.00019527027027027027,
      "loss": 1.1831,
      "step": 129
    },
    {
      "epoch": 0.30364963503649633,
      "grad_norm": 2.107212543487549,
      "learning_rate": 0.00019510135135135138,
      "loss": 1.2168,
      "step": 130
    },
    {
      "epoch": 0.30598540145985403,
      "grad_norm": 1.8849961757659912,
      "learning_rate": 0.00019493243243243246,
      "loss": 0.5006,
      "step": 131
    },
    {
      "epoch": 0.3083211678832117,
      "grad_norm": 6.285797119140625,
      "learning_rate": 0.0001947635135135135,
      "loss": 0.9268,
      "step": 132
    },
    {
      "epoch": 0.3106569343065693,
      "grad_norm": 5.135541915893555,
      "learning_rate": 0.00019459459459459462,
      "loss": 0.7412,
      "step": 133
    },
    {
      "epoch": 0.312992700729927,
      "grad_norm": 2.551398754119873,
      "learning_rate": 0.0001944256756756757,
      "loss": 1.3438,
      "step": 134
    },
    {
      "epoch": 0.31532846715328466,
      "grad_norm": 2.7641711235046387,
      "learning_rate": 0.00019425675675675675,
      "loss": 0.3712,
      "step": 135
    },
    {
      "epoch": 0.31766423357664236,
      "grad_norm": 2.630913496017456,
      "learning_rate": 0.00019408783783783783,
      "loss": 1.177,
      "step": 136
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.1470396518707275,
      "learning_rate": 0.00019391891891891894,
      "loss": 0.8556,
      "step": 137
    },
    {
      "epoch": 0.32233576642335765,
      "grad_norm": 2.9842991828918457,
      "learning_rate": 0.00019375000000000002,
      "loss": 2.238,
      "step": 138
    },
    {
      "epoch": 0.32467153284671535,
      "grad_norm": 2.41078519821167,
      "learning_rate": 0.00019358108108108107,
      "loss": 1.0468,
      "step": 139
    },
    {
      "epoch": 0.327007299270073,
      "grad_norm": 2.7255334854125977,
      "learning_rate": 0.00019341216216216218,
      "loss": 1.819,
      "step": 140
    },
    {
      "epoch": 0.32934306569343064,
      "grad_norm": 4.379427909851074,
      "learning_rate": 0.00019324324324324326,
      "loss": 0.7795,
      "step": 141
    },
    {
      "epoch": 0.33167883211678834,
      "grad_norm": 4.996096134185791,
      "learning_rate": 0.00019307432432432434,
      "loss": 0.7951,
      "step": 142
    },
    {
      "epoch": 0.334014598540146,
      "grad_norm": 6.509439945220947,
      "learning_rate": 0.0001929054054054054,
      "loss": 0.7018,
      "step": 143
    },
    {
      "epoch": 0.3363503649635036,
      "grad_norm": 3.0013315677642822,
      "learning_rate": 0.0001927364864864865,
      "loss": 0.6609,
      "step": 144
    },
    {
      "epoch": 0.3386861313868613,
      "grad_norm": 8.832230567932129,
      "learning_rate": 0.00019256756756756758,
      "loss": 0.6875,
      "step": 145
    },
    {
      "epoch": 0.34102189781021897,
      "grad_norm": 2.0254430770874023,
      "learning_rate": 0.00019239864864864864,
      "loss": 1.3551,
      "step": 146
    },
    {
      "epoch": 0.34335766423357666,
      "grad_norm": 2.569816827774048,
      "learning_rate": 0.00019222972972972975,
      "loss": 1.1404,
      "step": 147
    },
    {
      "epoch": 0.3456934306569343,
      "grad_norm": 3.2073159217834473,
      "learning_rate": 0.00019206081081081083,
      "loss": 1.297,
      "step": 148
    },
    {
      "epoch": 0.34802919708029195,
      "grad_norm": 2.434915781021118,
      "learning_rate": 0.0001918918918918919,
      "loss": 0.8498,
      "step": 149
    },
    {
      "epoch": 0.35036496350364965,
      "grad_norm": 3.858553647994995,
      "learning_rate": 0.000191722972972973,
      "loss": 0.8008,
      "step": 150
    },
    {
      "epoch": 0.3527007299270073,
      "grad_norm": 3.953310012817383,
      "learning_rate": 0.00019155405405405407,
      "loss": 0.6452,
      "step": 151
    },
    {
      "epoch": 0.35503649635036494,
      "grad_norm": 2.1383581161499023,
      "learning_rate": 0.00019138513513513515,
      "loss": 0.7597,
      "step": 152
    },
    {
      "epoch": 0.35737226277372264,
      "grad_norm": 5.327272891998291,
      "learning_rate": 0.0001912162162162162,
      "loss": 0.6602,
      "step": 153
    },
    {
      "epoch": 0.3597080291970803,
      "grad_norm": 2.3751261234283447,
      "learning_rate": 0.0001910472972972973,
      "loss": 0.9266,
      "step": 154
    },
    {
      "epoch": 0.362043795620438,
      "grad_norm": 2.453806161880493,
      "learning_rate": 0.0001908783783783784,
      "loss": 0.8277,
      "step": 155
    },
    {
      "epoch": 0.3643795620437956,
      "grad_norm": 6.937707424163818,
      "learning_rate": 0.00019070945945945947,
      "loss": 1.5405,
      "step": 156
    },
    {
      "epoch": 0.36671532846715327,
      "grad_norm": 10.204195976257324,
      "learning_rate": 0.00019054054054054055,
      "loss": 1.5534,
      "step": 157
    },
    {
      "epoch": 0.36905109489051097,
      "grad_norm": 5.199671745300293,
      "learning_rate": 0.00019037162162162163,
      "loss": 0.9425,
      "step": 158
    },
    {
      "epoch": 0.3713868613138686,
      "grad_norm": 1.8277074098587036,
      "learning_rate": 0.00019020270270270271,
      "loss": 1.6331,
      "step": 159
    },
    {
      "epoch": 0.37372262773722625,
      "grad_norm": 2.2670531272888184,
      "learning_rate": 0.0001900337837837838,
      "loss": 0.6795,
      "step": 160
    },
    {
      "epoch": 0.37605839416058395,
      "grad_norm": 3.0607852935791016,
      "learning_rate": 0.00018986486486486487,
      "loss": 0.6951,
      "step": 161
    },
    {
      "epoch": 0.3783941605839416,
      "grad_norm": 5.7294535636901855,
      "learning_rate": 0.00018969594594594596,
      "loss": 1.2735,
      "step": 162
    },
    {
      "epoch": 0.3807299270072993,
      "grad_norm": 5.3356733322143555,
      "learning_rate": 0.00018952702702702704,
      "loss": 0.9845,
      "step": 163
    },
    {
      "epoch": 0.38306569343065694,
      "grad_norm": 9.104644775390625,
      "learning_rate": 0.00018935810810810812,
      "loss": 0.6306,
      "step": 164
    },
    {
      "epoch": 0.3854014598540146,
      "grad_norm": 9.114243507385254,
      "learning_rate": 0.0001891891891891892,
      "loss": 0.7296,
      "step": 165
    },
    {
      "epoch": 0.3877372262773723,
      "grad_norm": 4.399960994720459,
      "learning_rate": 0.00018902027027027028,
      "loss": 0.6901,
      "step": 166
    },
    {
      "epoch": 0.3900729927007299,
      "grad_norm": 5.799288749694824,
      "learning_rate": 0.00018885135135135136,
      "loss": 0.9027,
      "step": 167
    },
    {
      "epoch": 0.39240875912408757,
      "grad_norm": 2.801816940307617,
      "learning_rate": 0.00018868243243243244,
      "loss": 1.1837,
      "step": 168
    },
    {
      "epoch": 0.39474452554744527,
      "grad_norm": 3.2686314582824707,
      "learning_rate": 0.00018851351351351352,
      "loss": 1.9249,
      "step": 169
    },
    {
      "epoch": 0.3970802919708029,
      "grad_norm": 4.647471904754639,
      "learning_rate": 0.00018834459459459463,
      "loss": 0.3442,
      "step": 170
    },
    {
      "epoch": 0.3994160583941606,
      "grad_norm": 4.223328590393066,
      "learning_rate": 0.00018817567567567568,
      "loss": 2.0209,
      "step": 171
    },
    {
      "epoch": 0.40175182481751825,
      "grad_norm": 3.7961370944976807,
      "learning_rate": 0.00018800675675675676,
      "loss": 0.9398,
      "step": 172
    },
    {
      "epoch": 0.4040875912408759,
      "grad_norm": 6.198826313018799,
      "learning_rate": 0.00018783783783783784,
      "loss": 0.7627,
      "step": 173
    },
    {
      "epoch": 0.4064233576642336,
      "grad_norm": 3.764906644821167,
      "learning_rate": 0.00018766891891891892,
      "loss": 0.5007,
      "step": 174
    },
    {
      "epoch": 0.40875912408759124,
      "grad_norm": 3.7581379413604736,
      "learning_rate": 0.0001875,
      "loss": 0.4493,
      "step": 175
    },
    {
      "epoch": 0.4110948905109489,
      "grad_norm": 7.820786952972412,
      "learning_rate": 0.00018733108108108108,
      "loss": 0.9131,
      "step": 176
    },
    {
      "epoch": 0.4134306569343066,
      "grad_norm": 4.98733377456665,
      "learning_rate": 0.0001871621621621622,
      "loss": 0.5711,
      "step": 177
    },
    {
      "epoch": 0.4157664233576642,
      "grad_norm": 10.452738761901855,
      "learning_rate": 0.00018699324324324325,
      "loss": 0.921,
      "step": 178
    },
    {
      "epoch": 0.41810218978102187,
      "grad_norm": 6.678703784942627,
      "learning_rate": 0.00018682432432432433,
      "loss": 1.1073,
      "step": 179
    },
    {
      "epoch": 0.42043795620437957,
      "grad_norm": 1.999263048171997,
      "learning_rate": 0.0001866554054054054,
      "loss": 0.5242,
      "step": 180
    },
    {
      "epoch": 0.4227737226277372,
      "grad_norm": 7.690557479858398,
      "learning_rate": 0.0001864864864864865,
      "loss": 0.865,
      "step": 181
    },
    {
      "epoch": 0.4251094890510949,
      "grad_norm": 1.811572790145874,
      "learning_rate": 0.00018631756756756757,
      "loss": 0.647,
      "step": 182
    },
    {
      "epoch": 0.42744525547445256,
      "grad_norm": 7.162997245788574,
      "learning_rate": 0.00018614864864864865,
      "loss": 0.7682,
      "step": 183
    },
    {
      "epoch": 0.4297810218978102,
      "grad_norm": 2.463698625564575,
      "learning_rate": 0.00018597972972972976,
      "loss": 0.4075,
      "step": 184
    },
    {
      "epoch": 0.4321167883211679,
      "grad_norm": 3.1825597286224365,
      "learning_rate": 0.0001858108108108108,
      "loss": 0.9917,
      "step": 185
    },
    {
      "epoch": 0.43445255474452554,
      "grad_norm": 4.353607177734375,
      "learning_rate": 0.0001856418918918919,
      "loss": 1.068,
      "step": 186
    },
    {
      "epoch": 0.4367883211678832,
      "grad_norm": 4.00612211227417,
      "learning_rate": 0.000185472972972973,
      "loss": 1.448,
      "step": 187
    },
    {
      "epoch": 0.4391240875912409,
      "grad_norm": 1.2521460056304932,
      "learning_rate": 0.00018530405405405408,
      "loss": 2.5552,
      "step": 188
    },
    {
      "epoch": 0.44145985401459853,
      "grad_norm": 3.6407196521759033,
      "learning_rate": 0.00018513513513513513,
      "loss": 1.7554,
      "step": 189
    },
    {
      "epoch": 0.44379562043795623,
      "grad_norm": 5.640860080718994,
      "learning_rate": 0.0001849662162162162,
      "loss": 0.7637,
      "step": 190
    },
    {
      "epoch": 0.44613138686131387,
      "grad_norm": 5.73008918762207,
      "learning_rate": 0.00018479729729729732,
      "loss": 1.1355,
      "step": 191
    },
    {
      "epoch": 0.4484671532846715,
      "grad_norm": 1.944442868232727,
      "learning_rate": 0.00018462837837837837,
      "loss": 0.5688,
      "step": 192
    },
    {
      "epoch": 0.4508029197080292,
      "grad_norm": 2.7227437496185303,
      "learning_rate": 0.00018445945945945946,
      "loss": 0.6323,
      "step": 193
    },
    {
      "epoch": 0.45313868613138686,
      "grad_norm": 2.8060991764068604,
      "learning_rate": 0.00018429054054054056,
      "loss": 0.8376,
      "step": 194
    },
    {
      "epoch": 0.4554744525547445,
      "grad_norm": 3.335940361022949,
      "learning_rate": 0.00018412162162162164,
      "loss": 0.698,
      "step": 195
    },
    {
      "epoch": 0.4578102189781022,
      "grad_norm": 6.155426502227783,
      "learning_rate": 0.0001839527027027027,
      "loss": 0.8791,
      "step": 196
    },
    {
      "epoch": 0.46014598540145984,
      "grad_norm": 3.7658114433288574,
      "learning_rate": 0.0001837837837837838,
      "loss": 0.9442,
      "step": 197
    },
    {
      "epoch": 0.46248175182481754,
      "grad_norm": 3.3522450923919678,
      "learning_rate": 0.00018361486486486489,
      "loss": 1.0083,
      "step": 198
    },
    {
      "epoch": 0.4648175182481752,
      "grad_norm": 2.8631513118743896,
      "learning_rate": 0.00018344594594594594,
      "loss": 0.4809,
      "step": 199
    },
    {
      "epoch": 0.46715328467153283,
      "grad_norm": 2.25435471534729,
      "learning_rate": 0.00018327702702702702,
      "loss": 1.0156,
      "step": 200
    },
    {
      "epoch": 0.46948905109489053,
      "grad_norm": 1.9246671199798584,
      "learning_rate": 0.00018310810810810813,
      "loss": 1.3381,
      "step": 201
    },
    {
      "epoch": 0.4718248175182482,
      "grad_norm": 3.110198974609375,
      "learning_rate": 0.0001829391891891892,
      "loss": 1.1552,
      "step": 202
    },
    {
      "epoch": 0.4741605839416058,
      "grad_norm": 2.503044605255127,
      "learning_rate": 0.00018277027027027026,
      "loss": 2.2655,
      "step": 203
    },
    {
      "epoch": 0.4764963503649635,
      "grad_norm": 1.8450448513031006,
      "learning_rate": 0.00018260135135135137,
      "loss": 0.6278,
      "step": 204
    },
    {
      "epoch": 0.47883211678832116,
      "grad_norm": 1.6864385604858398,
      "learning_rate": 0.00018243243243243245,
      "loss": 1.2003,
      "step": 205
    },
    {
      "epoch": 0.48116788321167886,
      "grad_norm": 5.188325881958008,
      "learning_rate": 0.00018226351351351353,
      "loss": 0.9013,
      "step": 206
    },
    {
      "epoch": 0.4835036496350365,
      "grad_norm": 3.4707822799682617,
      "learning_rate": 0.0001820945945945946,
      "loss": 0.543,
      "step": 207
    },
    {
      "epoch": 0.48583941605839415,
      "grad_norm": 2.843325138092041,
      "learning_rate": 0.0001819256756756757,
      "loss": 1.0545,
      "step": 208
    },
    {
      "epoch": 0.48817518248175185,
      "grad_norm": 3.6500461101531982,
      "learning_rate": 0.00018175675675675677,
      "loss": 1.2508,
      "step": 209
    },
    {
      "epoch": 0.4905109489051095,
      "grad_norm": 3.4692318439483643,
      "learning_rate": 0.00018158783783783783,
      "loss": 1.0221,
      "step": 210
    },
    {
      "epoch": 0.49284671532846713,
      "grad_norm": 3.8502328395843506,
      "learning_rate": 0.00018141891891891893,
      "loss": 1.4199,
      "step": 211
    },
    {
      "epoch": 0.49518248175182483,
      "grad_norm": 2.7180144786834717,
      "learning_rate": 0.00018125000000000001,
      "loss": 0.8236,
      "step": 212
    },
    {
      "epoch": 0.4975182481751825,
      "grad_norm": 2.541574239730835,
      "learning_rate": 0.0001810810810810811,
      "loss": 1.1596,
      "step": 213
    },
    {
      "epoch": 0.4998540145985401,
      "grad_norm": 6.666364669799805,
      "learning_rate": 0.00018091216216216218,
      "loss": 0.6143,
      "step": 214
    },
    {
      "epoch": 0.5021897810218978,
      "grad_norm": 1.401397466659546,
      "learning_rate": 0.00018074324324324326,
      "loss": 0.4404,
      "step": 215
    },
    {
      "epoch": 0.5045255474452555,
      "grad_norm": 4.416714191436768,
      "learning_rate": 0.00018057432432432434,
      "loss": 1.301,
      "step": 216
    },
    {
      "epoch": 0.5068613138686131,
      "grad_norm": 1.7608212232589722,
      "learning_rate": 0.0001804054054054054,
      "loss": 0.6554,
      "step": 217
    },
    {
      "epoch": 0.5091970802919707,
      "grad_norm": 3.2877306938171387,
      "learning_rate": 0.0001802364864864865,
      "loss": 0.797,
      "step": 218
    },
    {
      "epoch": 0.5115328467153285,
      "grad_norm": 2.2018120288848877,
      "learning_rate": 0.00018006756756756758,
      "loss": 0.4726,
      "step": 219
    },
    {
      "epoch": 0.5138686131386861,
      "grad_norm": 3.223947763442993,
      "learning_rate": 0.00017989864864864866,
      "loss": 0.7042,
      "step": 220
    },
    {
      "epoch": 0.5162043795620438,
      "grad_norm": 4.954391956329346,
      "learning_rate": 0.00017972972972972974,
      "loss": 0.7653,
      "step": 221
    },
    {
      "epoch": 0.5185401459854014,
      "grad_norm": 1.2630620002746582,
      "learning_rate": 0.00017956081081081082,
      "loss": 0.3717,
      "step": 222
    },
    {
      "epoch": 0.5208759124087591,
      "grad_norm": 2.3903441429138184,
      "learning_rate": 0.0001793918918918919,
      "loss": 1.1681,
      "step": 223
    },
    {
      "epoch": 0.5232116788321168,
      "grad_norm": 3.2285449504852295,
      "learning_rate": 0.00017922297297297298,
      "loss": 0.5467,
      "step": 224
    },
    {
      "epoch": 0.5255474452554745,
      "grad_norm": 4.086148262023926,
      "learning_rate": 0.00017905405405405406,
      "loss": 0.9469,
      "step": 225
    },
    {
      "epoch": 0.5278832116788321,
      "grad_norm": 2.425776958465576,
      "learning_rate": 0.00017888513513513514,
      "loss": 0.6075,
      "step": 226
    },
    {
      "epoch": 0.5302189781021898,
      "grad_norm": 9.902058601379395,
      "learning_rate": 0.00017871621621621622,
      "loss": 1.0295,
      "step": 227
    },
    {
      "epoch": 0.5325547445255474,
      "grad_norm": 6.67811393737793,
      "learning_rate": 0.0001785472972972973,
      "loss": 0.8288,
      "step": 228
    },
    {
      "epoch": 0.5348905109489052,
      "grad_norm": 3.784651041030884,
      "learning_rate": 0.00017837837837837839,
      "loss": 0.9246,
      "step": 229
    },
    {
      "epoch": 0.5372262773722628,
      "grad_norm": 2.31585431098938,
      "learning_rate": 0.00017820945945945947,
      "loss": 0.8299,
      "step": 230
    },
    {
      "epoch": 0.5395620437956204,
      "grad_norm": 7.760765075683594,
      "learning_rate": 0.00017804054054054055,
      "loss": 0.7052,
      "step": 231
    },
    {
      "epoch": 0.5418978102189781,
      "grad_norm": 7.895640850067139,
      "learning_rate": 0.00017787162162162163,
      "loss": 0.9297,
      "step": 232
    },
    {
      "epoch": 0.5442335766423357,
      "grad_norm": 9.609262466430664,
      "learning_rate": 0.0001777027027027027,
      "loss": 0.7022,
      "step": 233
    },
    {
      "epoch": 0.5465693430656934,
      "grad_norm": 2.8617677688598633,
      "learning_rate": 0.00017753378378378382,
      "loss": 0.8914,
      "step": 234
    },
    {
      "epoch": 0.5489051094890511,
      "grad_norm": 4.731967926025391,
      "learning_rate": 0.00017736486486486487,
      "loss": 0.8823,
      "step": 235
    },
    {
      "epoch": 0.5512408759124088,
      "grad_norm": 3.072221279144287,
      "learning_rate": 0.00017719594594594595,
      "loss": 0.6338,
      "step": 236
    },
    {
      "epoch": 0.5535766423357664,
      "grad_norm": 1.8313015699386597,
      "learning_rate": 0.00017702702702702703,
      "loss": 1.053,
      "step": 237
    },
    {
      "epoch": 0.5559124087591241,
      "grad_norm": 4.1119704246521,
      "learning_rate": 0.0001768581081081081,
      "loss": 1.4707,
      "step": 238
    },
    {
      "epoch": 0.5582481751824817,
      "grad_norm": 2.7245967388153076,
      "learning_rate": 0.0001766891891891892,
      "loss": 0.9795,
      "step": 239
    },
    {
      "epoch": 0.5605839416058395,
      "grad_norm": 8.945819854736328,
      "learning_rate": 0.00017652027027027027,
      "loss": 0.735,
      "step": 240
    },
    {
      "epoch": 0.5629197080291971,
      "grad_norm": 2.044471025466919,
      "learning_rate": 0.00017635135135135138,
      "loss": 0.8076,
      "step": 241
    },
    {
      "epoch": 0.5652554744525548,
      "grad_norm": 3.2664029598236084,
      "learning_rate": 0.00017618243243243243,
      "loss": 0.4587,
      "step": 242
    },
    {
      "epoch": 0.5675912408759124,
      "grad_norm": 1.938437581062317,
      "learning_rate": 0.00017601351351351351,
      "loss": 1.5969,
      "step": 243
    },
    {
      "epoch": 0.56992700729927,
      "grad_norm": 2.2933285236358643,
      "learning_rate": 0.00017584459459459462,
      "loss": 1.1118,
      "step": 244
    },
    {
      "epoch": 0.5722627737226277,
      "grad_norm": 1.5469526052474976,
      "learning_rate": 0.00017567567567567568,
      "loss": 0.9707,
      "step": 245
    },
    {
      "epoch": 0.5745985401459854,
      "grad_norm": 5.938149929046631,
      "learning_rate": 0.00017550675675675676,
      "loss": 0.9209,
      "step": 246
    },
    {
      "epoch": 0.5769343065693431,
      "grad_norm": 4.532657623291016,
      "learning_rate": 0.00017533783783783784,
      "loss": 0.7824,
      "step": 247
    },
    {
      "epoch": 0.5792700729927007,
      "grad_norm": 3.2225654125213623,
      "learning_rate": 0.00017516891891891894,
      "loss": 0.9999,
      "step": 248
    },
    {
      "epoch": 0.5816058394160584,
      "grad_norm": 3.537065267562866,
      "learning_rate": 0.000175,
      "loss": 0.9898,
      "step": 249
    },
    {
      "epoch": 0.583941605839416,
      "grad_norm": 1.4845114946365356,
      "learning_rate": 0.00017483108108108108,
      "loss": 0.8972,
      "step": 250
    },
    {
      "epoch": 0.5862773722627738,
      "grad_norm": 3.526463508605957,
      "learning_rate": 0.0001746621621621622,
      "loss": 0.8341,
      "step": 251
    },
    {
      "epoch": 0.5886131386861314,
      "grad_norm": 1.9015963077545166,
      "learning_rate": 0.00017449324324324327,
      "loss": 0.426,
      "step": 252
    },
    {
      "epoch": 0.590948905109489,
      "grad_norm": 3.654292583465576,
      "learning_rate": 0.00017432432432432432,
      "loss": 0.7703,
      "step": 253
    },
    {
      "epoch": 0.5932846715328467,
      "grad_norm": 5.358891010284424,
      "learning_rate": 0.0001741554054054054,
      "loss": 0.6846,
      "step": 254
    },
    {
      "epoch": 0.5956204379562043,
      "grad_norm": 6.123629093170166,
      "learning_rate": 0.0001739864864864865,
      "loss": 0.868,
      "step": 255
    },
    {
      "epoch": 0.5979562043795621,
      "grad_norm": 1.9580988883972168,
      "learning_rate": 0.00017381756756756756,
      "loss": 0.5064,
      "step": 256
    },
    {
      "epoch": 0.6002919708029197,
      "grad_norm": 2.4821245670318604,
      "learning_rate": 0.00017364864864864864,
      "loss": 1.1768,
      "step": 257
    },
    {
      "epoch": 0.6026277372262774,
      "grad_norm": 1.998465657234192,
      "learning_rate": 0.00017347972972972975,
      "loss": 1.2251,
      "step": 258
    },
    {
      "epoch": 0.604963503649635,
      "grad_norm": 5.191498756408691,
      "learning_rate": 0.00017331081081081083,
      "loss": 0.6278,
      "step": 259
    },
    {
      "epoch": 0.6072992700729927,
      "grad_norm": 7.989247798919678,
      "learning_rate": 0.00017314189189189189,
      "loss": 0.7441,
      "step": 260
    },
    {
      "epoch": 0.6096350364963503,
      "grad_norm": 5.824645519256592,
      "learning_rate": 0.000172972972972973,
      "loss": 0.9839,
      "step": 261
    },
    {
      "epoch": 0.6119708029197081,
      "grad_norm": 4.20768404006958,
      "learning_rate": 0.00017280405405405407,
      "loss": 0.6605,
      "step": 262
    },
    {
      "epoch": 0.6143065693430657,
      "grad_norm": 2.1478023529052734,
      "learning_rate": 0.00017263513513513513,
      "loss": 0.8917,
      "step": 263
    },
    {
      "epoch": 0.6166423357664234,
      "grad_norm": 6.322198867797852,
      "learning_rate": 0.0001724662162162162,
      "loss": 0.4609,
      "step": 264
    },
    {
      "epoch": 0.618978102189781,
      "grad_norm": 1.9217528104782104,
      "learning_rate": 0.00017229729729729732,
      "loss": 0.4678,
      "step": 265
    },
    {
      "epoch": 0.6213138686131386,
      "grad_norm": 3.9349427223205566,
      "learning_rate": 0.0001721283783783784,
      "loss": 0.9399,
      "step": 266
    },
    {
      "epoch": 0.6236496350364964,
      "grad_norm": 3.911863088607788,
      "learning_rate": 0.00017195945945945945,
      "loss": 1.1188,
      "step": 267
    },
    {
      "epoch": 0.625985401459854,
      "grad_norm": 4.022555828094482,
      "learning_rate": 0.00017179054054054056,
      "loss": 1.0559,
      "step": 268
    },
    {
      "epoch": 0.6283211678832117,
      "grad_norm": 2.1869313716888428,
      "learning_rate": 0.00017162162162162164,
      "loss": 1.3344,
      "step": 269
    },
    {
      "epoch": 0.6306569343065693,
      "grad_norm": 2.5787506103515625,
      "learning_rate": 0.00017145270270270272,
      "loss": 1.4437,
      "step": 270
    },
    {
      "epoch": 0.632992700729927,
      "grad_norm": 3.2177252769470215,
      "learning_rate": 0.0001712837837837838,
      "loss": 0.7836,
      "step": 271
    },
    {
      "epoch": 0.6353284671532847,
      "grad_norm": 1.9432443380355835,
      "learning_rate": 0.00017111486486486488,
      "loss": 0.6803,
      "step": 272
    },
    {
      "epoch": 0.6376642335766424,
      "grad_norm": 2.0556795597076416,
      "learning_rate": 0.00017094594594594596,
      "loss": 1.0469,
      "step": 273
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.7991763353347778,
      "learning_rate": 0.00017077702702702701,
      "loss": 0.6517,
      "step": 274
    },
    {
      "epoch": 0.6423357664233577,
      "grad_norm": 2.1025683879852295,
      "learning_rate": 0.00017060810810810812,
      "loss": 0.6207,
      "step": 275
    },
    {
      "epoch": 0.6446715328467153,
      "grad_norm": 2.27294659614563,
      "learning_rate": 0.0001704391891891892,
      "loss": 0.5834,
      "step": 276
    },
    {
      "epoch": 0.6470072992700729,
      "grad_norm": 1.1743026971817017,
      "learning_rate": 0.00017027027027027028,
      "loss": 0.7875,
      "step": 277
    },
    {
      "epoch": 0.6493430656934307,
      "grad_norm": 2.3890464305877686,
      "learning_rate": 0.00017010135135135136,
      "loss": 0.9477,
      "step": 278
    },
    {
      "epoch": 0.6516788321167883,
      "grad_norm": 2.0864815711975098,
      "learning_rate": 0.00016993243243243244,
      "loss": 0.9688,
      "step": 279
    },
    {
      "epoch": 0.654014598540146,
      "grad_norm": 2.793003797531128,
      "learning_rate": 0.00016976351351351353,
      "loss": 1.6551,
      "step": 280
    },
    {
      "epoch": 0.6563503649635036,
      "grad_norm": 2.7168452739715576,
      "learning_rate": 0.0001695945945945946,
      "loss": 0.5236,
      "step": 281
    },
    {
      "epoch": 0.6586861313868613,
      "grad_norm": 3.3829567432403564,
      "learning_rate": 0.00016942567567567569,
      "loss": 0.6305,
      "step": 282
    },
    {
      "epoch": 0.661021897810219,
      "grad_norm": 2.9282820224761963,
      "learning_rate": 0.00016925675675675677,
      "loss": 1.0663,
      "step": 283
    },
    {
      "epoch": 0.6633576642335767,
      "grad_norm": 2.4982731342315674,
      "learning_rate": 0.00016908783783783785,
      "loss": 1.4056,
      "step": 284
    },
    {
      "epoch": 0.6656934306569343,
      "grad_norm": 4.1182074546813965,
      "learning_rate": 0.00016891891891891893,
      "loss": 0.3817,
      "step": 285
    },
    {
      "epoch": 0.668029197080292,
      "grad_norm": 1.6579341888427734,
      "learning_rate": 0.00016875,
      "loss": 1.614,
      "step": 286
    },
    {
      "epoch": 0.6703649635036496,
      "grad_norm": 3.9945240020751953,
      "learning_rate": 0.0001685810810810811,
      "loss": 0.5839,
      "step": 287
    },
    {
      "epoch": 0.6727007299270072,
      "grad_norm": 2.2712409496307373,
      "learning_rate": 0.00016841216216216217,
      "loss": 0.9833,
      "step": 288
    },
    {
      "epoch": 0.675036496350365,
      "grad_norm": 4.3409576416015625,
      "learning_rate": 0.00016824324324324325,
      "loss": 0.9326,
      "step": 289
    },
    {
      "epoch": 0.6773722627737226,
      "grad_norm": 4.233548641204834,
      "learning_rate": 0.00016807432432432433,
      "loss": 1.2809,
      "step": 290
    },
    {
      "epoch": 0.6797080291970803,
      "grad_norm": 3.92018985748291,
      "learning_rate": 0.0001679054054054054,
      "loss": 0.9821,
      "step": 291
    },
    {
      "epoch": 0.6820437956204379,
      "grad_norm": 5.72330904006958,
      "learning_rate": 0.0001677364864864865,
      "loss": 1.2485,
      "step": 292
    },
    {
      "epoch": 0.6843795620437956,
      "grad_norm": 10.873211860656738,
      "learning_rate": 0.00016756756756756757,
      "loss": 0.9811,
      "step": 293
    },
    {
      "epoch": 0.6867153284671533,
      "grad_norm": 3.64264178276062,
      "learning_rate": 0.00016739864864864865,
      "loss": 1.093,
      "step": 294
    },
    {
      "epoch": 0.689051094890511,
      "grad_norm": 3.0265986919403076,
      "learning_rate": 0.00016722972972972973,
      "loss": 1.2747,
      "step": 295
    },
    {
      "epoch": 0.6913868613138686,
      "grad_norm": 6.359126567840576,
      "learning_rate": 0.00016706081081081082,
      "loss": 1.0517,
      "step": 296
    },
    {
      "epoch": 0.6937226277372263,
      "grad_norm": 2.490720748901367,
      "learning_rate": 0.0001668918918918919,
      "loss": 1.7478,
      "step": 297
    },
    {
      "epoch": 0.6960583941605839,
      "grad_norm": 2.5984482765197754,
      "learning_rate": 0.000166722972972973,
      "loss": 0.7707,
      "step": 298
    },
    {
      "epoch": 0.6983941605839417,
      "grad_norm": 4.215023994445801,
      "learning_rate": 0.00016655405405405406,
      "loss": 2.931,
      "step": 299
    },
    {
      "epoch": 0.7007299270072993,
      "grad_norm": 6.943389892578125,
      "learning_rate": 0.00016638513513513514,
      "loss": 0.7385,
      "step": 300
    },
    {
      "epoch": 0.703065693430657,
      "grad_norm": 3.961850166320801,
      "learning_rate": 0.00016621621621621622,
      "loss": 0.3184,
      "step": 301
    },
    {
      "epoch": 0.7054014598540146,
      "grad_norm": 2.5563337802886963,
      "learning_rate": 0.0001660472972972973,
      "loss": 0.9316,
      "step": 302
    },
    {
      "epoch": 0.7077372262773722,
      "grad_norm": 3.0961525440216064,
      "learning_rate": 0.00016587837837837838,
      "loss": 1.1406,
      "step": 303
    },
    {
      "epoch": 0.7100729927007299,
      "grad_norm": 12.185523986816406,
      "learning_rate": 0.00016570945945945946,
      "loss": 0.8294,
      "step": 304
    },
    {
      "epoch": 0.7124087591240876,
      "grad_norm": NaN,
      "learning_rate": 0.00016570945945945946,
      "loss": 0.8503,
      "step": 305
    },
    {
      "epoch": 0.7147445255474453,
      "grad_norm": 2.089846134185791,
      "learning_rate": 0.00016554054054054057,
      "loss": 0.8128,
      "step": 306
    },
    {
      "epoch": 0.7170802919708029,
      "grad_norm": 5.167967796325684,
      "learning_rate": 0.00016537162162162162,
      "loss": 0.4046,
      "step": 307
    },
    {
      "epoch": 0.7194160583941606,
      "grad_norm": 4.712947845458984,
      "learning_rate": 0.0001652027027027027,
      "loss": 0.7207,
      "step": 308
    },
    {
      "epoch": 0.7217518248175182,
      "grad_norm": 3.363433837890625,
      "learning_rate": 0.0001650337837837838,
      "loss": 0.7708,
      "step": 309
    },
    {
      "epoch": 0.724087591240876,
      "grad_norm": 3.8109636306762695,
      "learning_rate": 0.00016486486486486486,
      "loss": 1.4245,
      "step": 310
    },
    {
      "epoch": 0.7264233576642336,
      "grad_norm": 2.907350540161133,
      "learning_rate": 0.00016469594594594594,
      "loss": 0.7309,
      "step": 311
    },
    {
      "epoch": 0.7287591240875912,
      "grad_norm": 2.1876847743988037,
      "learning_rate": 0.00016452702702702702,
      "loss": 0.6708,
      "step": 312
    },
    {
      "epoch": 0.7310948905109489,
      "grad_norm": 2.936444044113159,
      "learning_rate": 0.00016435810810810813,
      "loss": 0.6147,
      "step": 313
    },
    {
      "epoch": 0.7334306569343065,
      "grad_norm": 2.402863025665283,
      "learning_rate": 0.00016418918918918919,
      "loss": 0.6549,
      "step": 314
    },
    {
      "epoch": 0.7357664233576642,
      "grad_norm": 2.768766164779663,
      "learning_rate": 0.00016402027027027027,
      "loss": 1.0779,
      "step": 315
    },
    {
      "epoch": 0.7381021897810219,
      "grad_norm": 1.8137203454971313,
      "learning_rate": 0.00016385135135135137,
      "loss": 1.2745,
      "step": 316
    },
    {
      "epoch": 0.7404379562043796,
      "grad_norm": 4.790500640869141,
      "learning_rate": 0.00016368243243243246,
      "loss": 0.7227,
      "step": 317
    },
    {
      "epoch": 0.7427737226277372,
      "grad_norm": 2.5448482036590576,
      "learning_rate": 0.0001635135135135135,
      "loss": 1.5351,
      "step": 318
    },
    {
      "epoch": 0.7451094890510949,
      "grad_norm": 2.249009609222412,
      "learning_rate": 0.00016334459459459462,
      "loss": 0.9273,
      "step": 319
    },
    {
      "epoch": 0.7474452554744525,
      "grad_norm": 5.881746768951416,
      "learning_rate": 0.0001631756756756757,
      "loss": 0.7885,
      "step": 320
    },
    {
      "epoch": 0.7497810218978103,
      "grad_norm": 4.1182098388671875,
      "learning_rate": 0.00016300675675675675,
      "loss": 0.6611,
      "step": 321
    },
    {
      "epoch": 0.7521167883211679,
      "grad_norm": 4.125105381011963,
      "learning_rate": 0.00016283783783783783,
      "loss": 0.7805,
      "step": 322
    },
    {
      "epoch": 0.7544525547445255,
      "grad_norm": 6.871982097625732,
      "learning_rate": 0.00016266891891891894,
      "loss": 0.6841,
      "step": 323
    },
    {
      "epoch": 0.7567883211678832,
      "grad_norm": 2.1913461685180664,
      "learning_rate": 0.00016250000000000002,
      "loss": 1.6673,
      "step": 324
    },
    {
      "epoch": 0.7591240875912408,
      "grad_norm": 1.7159411907196045,
      "learning_rate": 0.00016233108108108107,
      "loss": 0.7626,
      "step": 325
    },
    {
      "epoch": 0.7614598540145986,
      "grad_norm": 2.5882887840270996,
      "learning_rate": 0.00016216216216216218,
      "loss": 1.1786,
      "step": 326
    },
    {
      "epoch": 0.7637956204379562,
      "grad_norm": 3.8828423023223877,
      "learning_rate": 0.00016199324324324326,
      "loss": 0.5637,
      "step": 327
    },
    {
      "epoch": 0.7661313868613139,
      "grad_norm": 1.9610986709594727,
      "learning_rate": 0.00016182432432432432,
      "loss": 0.6983,
      "step": 328
    },
    {
      "epoch": 0.7684671532846715,
      "grad_norm": 2.7131645679473877,
      "learning_rate": 0.0001616554054054054,
      "loss": 0.4128,
      "step": 329
    },
    {
      "epoch": 0.7708029197080292,
      "grad_norm": 2.7551701068878174,
      "learning_rate": 0.0001614864864864865,
      "loss": 1.0298,
      "step": 330
    },
    {
      "epoch": 0.7731386861313868,
      "grad_norm": 4.264751434326172,
      "learning_rate": 0.00016131756756756758,
      "loss": 0.9903,
      "step": 331
    },
    {
      "epoch": 0.7754744525547446,
      "grad_norm": 4.602597713470459,
      "learning_rate": 0.00016114864864864864,
      "loss": 0.5458,
      "step": 332
    },
    {
      "epoch": 0.7778102189781022,
      "grad_norm": 1.5120424032211304,
      "learning_rate": 0.00016097972972972975,
      "loss": 2.5905,
      "step": 333
    },
    {
      "epoch": 0.7801459854014599,
      "grad_norm": 2.6494271755218506,
      "learning_rate": 0.00016081081081081083,
      "loss": 1.0212,
      "step": 334
    },
    {
      "epoch": 0.7824817518248175,
      "grad_norm": 6.638841152191162,
      "learning_rate": 0.0001606418918918919,
      "loss": 0.9082,
      "step": 335
    },
    {
      "epoch": 0.7848175182481751,
      "grad_norm": 2.6522650718688965,
      "learning_rate": 0.000160472972972973,
      "loss": 0.9434,
      "step": 336
    },
    {
      "epoch": 0.7871532846715329,
      "grad_norm": 2.214141368865967,
      "learning_rate": 0.00016030405405405407,
      "loss": 0.8359,
      "step": 337
    },
    {
      "epoch": 0.7894890510948905,
      "grad_norm": 4.496076583862305,
      "learning_rate": 0.00016013513513513515,
      "loss": 0.6255,
      "step": 338
    },
    {
      "epoch": 0.7918248175182482,
      "grad_norm": 1.229704737663269,
      "learning_rate": 0.0001599662162162162,
      "loss": 0.2057,
      "step": 339
    },
    {
      "epoch": 0.7941605839416058,
      "grad_norm": 3.092505931854248,
      "learning_rate": 0.0001597972972972973,
      "loss": 0.4029,
      "step": 340
    },
    {
      "epoch": 0.7964963503649635,
      "grad_norm": 2.5165462493896484,
      "learning_rate": 0.0001596283783783784,
      "loss": 1.6965,
      "step": 341
    },
    {
      "epoch": 0.7988321167883212,
      "grad_norm": 1.8728752136230469,
      "learning_rate": 0.00015945945945945947,
      "loss": 1.8965,
      "step": 342
    },
    {
      "epoch": 0.8011678832116789,
      "grad_norm": 1.953677773475647,
      "learning_rate": 0.00015929054054054055,
      "loss": 1.0527,
      "step": 343
    },
    {
      "epoch": 0.8035036496350365,
      "grad_norm": 9.640122413635254,
      "learning_rate": 0.00015912162162162163,
      "loss": 0.8802,
      "step": 344
    },
    {
      "epoch": 0.8058394160583942,
      "grad_norm": 4.136576175689697,
      "learning_rate": 0.0001589527027027027,
      "loss": 1.0966,
      "step": 345
    },
    {
      "epoch": 0.8081751824817518,
      "grad_norm": 8.289653778076172,
      "learning_rate": 0.0001587837837837838,
      "loss": 1.267,
      "step": 346
    },
    {
      "epoch": 0.8105109489051094,
      "grad_norm": 2.6098451614379883,
      "learning_rate": 0.00015861486486486487,
      "loss": 0.601,
      "step": 347
    },
    {
      "epoch": 0.8128467153284672,
      "grad_norm": 8.853809356689453,
      "learning_rate": 0.00015844594594594595,
      "loss": 0.6428,
      "step": 348
    },
    {
      "epoch": 0.8151824817518248,
      "grad_norm": 7.313607215881348,
      "learning_rate": 0.00015827702702702704,
      "loss": 1.4037,
      "step": 349
    },
    {
      "epoch": 0.8175182481751825,
      "grad_norm": 2.3068883419036865,
      "learning_rate": 0.00015810810810810812,
      "loss": 0.9704,
      "step": 350
    },
    {
      "epoch": 0.8198540145985401,
      "grad_norm": 1.0041519403457642,
      "learning_rate": 0.0001579391891891892,
      "loss": 0.9391,
      "step": 351
    },
    {
      "epoch": 0.8221897810218978,
      "grad_norm": 3.295273780822754,
      "learning_rate": 0.00015777027027027028,
      "loss": 0.4963,
      "step": 352
    },
    {
      "epoch": 0.8245255474452555,
      "grad_norm": 2.468909740447998,
      "learning_rate": 0.00015760135135135136,
      "loss": 1.1721,
      "step": 353
    },
    {
      "epoch": 0.8268613138686132,
      "grad_norm": 2.1851062774658203,
      "learning_rate": 0.00015743243243243244,
      "loss": 0.8735,
      "step": 354
    },
    {
      "epoch": 0.8291970802919708,
      "grad_norm": 4.88815450668335,
      "learning_rate": 0.00015726351351351352,
      "loss": 1.4543,
      "step": 355
    },
    {
      "epoch": 0.8315328467153285,
      "grad_norm": 7.139379024505615,
      "learning_rate": 0.0001570945945945946,
      "loss": 1.0163,
      "step": 356
    },
    {
      "epoch": 0.8338686131386861,
      "grad_norm": 3.892881155014038,
      "learning_rate": 0.00015692567567567568,
      "loss": 0.8275,
      "step": 357
    },
    {
      "epoch": 0.8362043795620437,
      "grad_norm": 4.3439040184021,
      "learning_rate": 0.00015675675675675676,
      "loss": 0.6678,
      "step": 358
    },
    {
      "epoch": 0.8385401459854015,
      "grad_norm": 2.9291558265686035,
      "learning_rate": 0.00015658783783783784,
      "loss": 0.9591,
      "step": 359
    },
    {
      "epoch": 0.8408759124087591,
      "grad_norm": 5.515872478485107,
      "learning_rate": 0.00015641891891891892,
      "loss": 0.7373,
      "step": 360
    },
    {
      "epoch": 0.8432116788321168,
      "grad_norm": 1.5914455652236938,
      "learning_rate": 0.00015625,
      "loss": 1.046,
      "step": 361
    },
    {
      "epoch": 0.8455474452554744,
      "grad_norm": 2.9707462787628174,
      "learning_rate": 0.00015608108108108108,
      "loss": 1.1528,
      "step": 362
    },
    {
      "epoch": 0.8478832116788321,
      "grad_norm": 2.7418034076690674,
      "learning_rate": 0.0001559121621621622,
      "loss": 0.8383,
      "step": 363
    },
    {
      "epoch": 0.8502189781021898,
      "grad_norm": 2.875624656677246,
      "learning_rate": 0.00015574324324324325,
      "loss": 0.7292,
      "step": 364
    },
    {
      "epoch": 0.8525547445255475,
      "grad_norm": 4.805342674255371,
      "learning_rate": 0.00015557432432432433,
      "loss": 0.7271,
      "step": 365
    },
    {
      "epoch": 0.8548905109489051,
      "grad_norm": 1.9354231357574463,
      "learning_rate": 0.0001554054054054054,
      "loss": 1.1982,
      "step": 366
    },
    {
      "epoch": 0.8572262773722628,
      "grad_norm": 4.458685398101807,
      "learning_rate": 0.0001552364864864865,
      "loss": 0.7594,
      "step": 367
    },
    {
      "epoch": 0.8595620437956204,
      "grad_norm": 8.416303634643555,
      "learning_rate": 0.00015506756756756757,
      "loss": 0.5069,
      "step": 368
    },
    {
      "epoch": 0.8618978102189782,
      "grad_norm": 1.4562419652938843,
      "learning_rate": 0.00015489864864864865,
      "loss": 0.355,
      "step": 369
    },
    {
      "epoch": 0.8642335766423358,
      "grad_norm": 3.0824434757232666,
      "learning_rate": 0.00015472972972972976,
      "loss": 0.9551,
      "step": 370
    },
    {
      "epoch": 0.8665693430656934,
      "grad_norm": 1.8801568746566772,
      "learning_rate": 0.0001545608108108108,
      "loss": 1.0313,
      "step": 371
    },
    {
      "epoch": 0.8689051094890511,
      "grad_norm": 2.2479984760284424,
      "learning_rate": 0.0001543918918918919,
      "loss": 1.1564,
      "step": 372
    },
    {
      "epoch": 0.8712408759124087,
      "grad_norm": 1.7715014219284058,
      "learning_rate": 0.000154222972972973,
      "loss": 1.4466,
      "step": 373
    },
    {
      "epoch": 0.8735766423357664,
      "grad_norm": 3.561250925064087,
      "learning_rate": 0.00015405405405405405,
      "loss": 0.5438,
      "step": 374
    },
    {
      "epoch": 0.8759124087591241,
      "grad_norm": 1.9813706874847412,
      "learning_rate": 0.00015388513513513513,
      "loss": 0.9061,
      "step": 375
    },
    {
      "epoch": 0.8782481751824818,
      "grad_norm": 5.188766956329346,
      "learning_rate": 0.0001537162162162162,
      "loss": 1.1247,
      "step": 376
    },
    {
      "epoch": 0.8805839416058394,
      "grad_norm": 5.391952037811279,
      "learning_rate": 0.00015354729729729732,
      "loss": 1.1592,
      "step": 377
    },
    {
      "epoch": 0.8829197080291971,
      "grad_norm": 1.6342374086380005,
      "learning_rate": 0.00015337837837837837,
      "loss": 0.2935,
      "step": 378
    },
    {
      "epoch": 0.8852554744525547,
      "grad_norm": 3.589534044265747,
      "learning_rate": 0.00015320945945945945,
      "loss": 1.051,
      "step": 379
    },
    {
      "epoch": 0.8875912408759125,
      "grad_norm": 5.808187961578369,
      "learning_rate": 0.00015304054054054056,
      "loss": 1.2949,
      "step": 380
    },
    {
      "epoch": 0.8899270072992701,
      "grad_norm": 1.5610824823379517,
      "learning_rate": 0.00015287162162162164,
      "loss": 1.0197,
      "step": 381
    },
    {
      "epoch": 0.8922627737226277,
      "grad_norm": 1.8191949129104614,
      "learning_rate": 0.0001527027027027027,
      "loss": 0.8033,
      "step": 382
    },
    {
      "epoch": 0.8945985401459854,
      "grad_norm": 2.9425387382507324,
      "learning_rate": 0.0001525337837837838,
      "loss": 0.4286,
      "step": 383
    },
    {
      "epoch": 0.896934306569343,
      "grad_norm": 5.299906253814697,
      "learning_rate": 0.00015236486486486489,
      "loss": 0.7099,
      "step": 384
    },
    {
      "epoch": 0.8992700729927007,
      "grad_norm": 2.0762338638305664,
      "learning_rate": 0.00015219594594594594,
      "loss": 0.5335,
      "step": 385
    },
    {
      "epoch": 0.9016058394160584,
      "grad_norm": 11.368979454040527,
      "learning_rate": 0.00015202702702702702,
      "loss": 1.5672,
      "step": 386
    },
    {
      "epoch": 0.9039416058394161,
      "grad_norm": 1.9610389471054077,
      "learning_rate": 0.00015185810810810813,
      "loss": 1.1118,
      "step": 387
    },
    {
      "epoch": 0.9062773722627737,
      "grad_norm": 1.9583417177200317,
      "learning_rate": 0.0001516891891891892,
      "loss": 0.3301,
      "step": 388
    },
    {
      "epoch": 0.9086131386861314,
      "grad_norm": 1.6399168968200684,
      "learning_rate": 0.00015152027027027026,
      "loss": 1.3719,
      "step": 389
    },
    {
      "epoch": 0.910948905109489,
      "grad_norm": 5.5207200050354,
      "learning_rate": 0.00015135135135135137,
      "loss": 1.2168,
      "step": 390
    },
    {
      "epoch": 0.9132846715328468,
      "grad_norm": 4.31194543838501,
      "learning_rate": 0.00015118243243243245,
      "loss": 0.3857,
      "step": 391
    },
    {
      "epoch": 0.9156204379562044,
      "grad_norm": 3.891728639602661,
      "learning_rate": 0.0001510135135135135,
      "loss": 0.5043,
      "step": 392
    },
    {
      "epoch": 0.917956204379562,
      "grad_norm": 4.581470966339111,
      "learning_rate": 0.0001508445945945946,
      "loss": 0.58,
      "step": 393
    },
    {
      "epoch": 0.9202919708029197,
      "grad_norm": 0.7590975761413574,
      "learning_rate": 0.0001506756756756757,
      "loss": 0.1627,
      "step": 394
    },
    {
      "epoch": 0.9226277372262773,
      "grad_norm": 2.1293394565582275,
      "learning_rate": 0.00015050675675675677,
      "loss": 0.8471,
      "step": 395
    },
    {
      "epoch": 0.9249635036496351,
      "grad_norm": 1.195570945739746,
      "learning_rate": 0.00015033783783783783,
      "loss": 0.5253,
      "step": 396
    },
    {
      "epoch": 0.9272992700729927,
      "grad_norm": 7.542329788208008,
      "learning_rate": 0.00015016891891891893,
      "loss": 0.8121,
      "step": 397
    },
    {
      "epoch": 0.9296350364963504,
      "grad_norm": 7.633064270019531,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.7355,
      "step": 398
    },
    {
      "epoch": 0.931970802919708,
      "grad_norm": 6.178310871124268,
      "learning_rate": 0.0001498310810810811,
      "loss": 0.3021,
      "step": 399
    },
    {
      "epoch": 0.9343065693430657,
      "grad_norm": 3.40964937210083,
      "learning_rate": 0.00014966216216216218,
      "loss": 0.5324,
      "step": 400
    },
    {
      "epoch": 0.9366423357664233,
      "grad_norm": 2.4244136810302734,
      "learning_rate": 0.00014949324324324326,
      "loss": 1.5202,
      "step": 401
    },
    {
      "epoch": 0.9389781021897811,
      "grad_norm": 1.9517186880111694,
      "learning_rate": 0.00014932432432432434,
      "loss": 1.254,
      "step": 402
    },
    {
      "epoch": 0.9413138686131387,
      "grad_norm": 4.7616071701049805,
      "learning_rate": 0.0001491554054054054,
      "loss": 1.0087,
      "step": 403
    },
    {
      "epoch": 0.9436496350364963,
      "grad_norm": 2.92547607421875,
      "learning_rate": 0.0001489864864864865,
      "loss": 0.6089,
      "step": 404
    },
    {
      "epoch": 0.945985401459854,
      "grad_norm": 2.200528383255005,
      "learning_rate": 0.00014881756756756758,
      "loss": 0.4942,
      "step": 405
    },
    {
      "epoch": 0.9483211678832116,
      "grad_norm": 4.480522155761719,
      "learning_rate": 0.00014864864864864866,
      "loss": 1.0949,
      "step": 406
    },
    {
      "epoch": 0.9506569343065694,
      "grad_norm": 3.0811851024627686,
      "learning_rate": 0.00014847972972972974,
      "loss": 1.2832,
      "step": 407
    },
    {
      "epoch": 0.952992700729927,
      "grad_norm": 2.8186399936676025,
      "learning_rate": 0.00014831081081081082,
      "loss": 0.8532,
      "step": 408
    },
    {
      "epoch": 0.9553284671532847,
      "grad_norm": 9.831841468811035,
      "learning_rate": 0.0001481418918918919,
      "loss": 0.7241,
      "step": 409
    },
    {
      "epoch": 0.9576642335766423,
      "grad_norm": 7.400583267211914,
      "learning_rate": 0.00014797297297297298,
      "loss": 0.5215,
      "step": 410
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.5731189250946045,
      "learning_rate": 0.00014780405405405406,
      "loss": 0.7878,
      "step": 411
    },
    {
      "epoch": 0.9623357664233577,
      "grad_norm": 2.8391876220703125,
      "learning_rate": 0.00014763513513513514,
      "loss": 1.2615,
      "step": 412
    },
    {
      "epoch": 0.9646715328467154,
      "grad_norm": 7.0435872077941895,
      "learning_rate": 0.00014746621621621622,
      "loss": 0.4852,
      "step": 413
    },
    {
      "epoch": 0.967007299270073,
      "grad_norm": 1.9420051574707031,
      "learning_rate": 0.0001472972972972973,
      "loss": 0.7783,
      "step": 414
    },
    {
      "epoch": 0.9693430656934306,
      "grad_norm": 3.671562910079956,
      "learning_rate": 0.00014712837837837838,
      "loss": 0.4397,
      "step": 415
    },
    {
      "epoch": 0.9716788321167883,
      "grad_norm": 2.814558267593384,
      "learning_rate": 0.00014695945945945947,
      "loss": 0.932,
      "step": 416
    },
    {
      "epoch": 0.9740145985401459,
      "grad_norm": 1.5518584251403809,
      "learning_rate": 0.00014679054054054055,
      "loss": 1.1473,
      "step": 417
    },
    {
      "epoch": 0.9763503649635037,
      "grad_norm": 2.018237829208374,
      "learning_rate": 0.00014662162162162163,
      "loss": 0.8581,
      "step": 418
    },
    {
      "epoch": 0.9786861313868613,
      "grad_norm": 2.3939900398254395,
      "learning_rate": 0.0001464527027027027,
      "loss": 1.0448,
      "step": 419
    },
    {
      "epoch": 0.981021897810219,
      "grad_norm": 5.15247917175293,
      "learning_rate": 0.0001462837837837838,
      "loss": 0.5708,
      "step": 420
    },
    {
      "epoch": 0.9833576642335766,
      "grad_norm": 2.790555238723755,
      "learning_rate": 0.00014611486486486487,
      "loss": 1.5882,
      "step": 421
    },
    {
      "epoch": 0.9856934306569343,
      "grad_norm": 1.3068681955337524,
      "learning_rate": 0.00014594594594594595,
      "loss": 0.4103,
      "step": 422
    },
    {
      "epoch": 0.988029197080292,
      "grad_norm": 1.901110291481018,
      "learning_rate": 0.00014577702702702703,
      "loss": 0.3691,
      "step": 423
    },
    {
      "epoch": 0.9903649635036497,
      "grad_norm": 1.7155890464782715,
      "learning_rate": 0.0001456081081081081,
      "loss": 1.2976,
      "step": 424
    },
    {
      "epoch": 0.9927007299270073,
      "grad_norm": 2.1223747730255127,
      "learning_rate": 0.0001454391891891892,
      "loss": 1.1577,
      "step": 425
    },
    {
      "epoch": 0.995036496350365,
      "grad_norm": 5.73456335067749,
      "learning_rate": 0.00014527027027027027,
      "loss": 0.7295,
      "step": 426
    },
    {
      "epoch": 0.9973722627737226,
      "grad_norm": 7.500953197479248,
      "learning_rate": 0.00014510135135135138,
      "loss": 0.8001,
      "step": 427
    },
    {
      "epoch": 0.9997080291970802,
      "grad_norm": 4.013876438140869,
      "learning_rate": 0.00014493243243243243,
      "loss": 0.5883,
      "step": 428
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.987792015075684,
      "learning_rate": 0.00014476351351351351,
      "loss": 1.8589,
      "step": 429
    },
    {
      "epoch": 1.0023357664233576,
      "grad_norm": 2.6663358211517334,
      "learning_rate": 0.00014459459459459462,
      "loss": 0.4224,
      "step": 430
    },
    {
      "epoch": 1.0046715328467153,
      "grad_norm": 1.4752650260925293,
      "learning_rate": 0.00014442567567567568,
      "loss": 1.3026,
      "step": 431
    },
    {
      "epoch": 1.007007299270073,
      "grad_norm": 1.4273279905319214,
      "learning_rate": 0.00014425675675675676,
      "loss": 0.9914,
      "step": 432
    },
    {
      "epoch": 1.0093430656934306,
      "grad_norm": 3.495724678039551,
      "learning_rate": 0.00014408783783783784,
      "loss": 1.1562,
      "step": 433
    },
    {
      "epoch": 1.0116788321167882,
      "grad_norm": 2.3646366596221924,
      "learning_rate": 0.00014391891891891894,
      "loss": 0.1491,
      "step": 434
    },
    {
      "epoch": 1.014014598540146,
      "grad_norm": 1.6226823329925537,
      "learning_rate": 0.00014375,
      "loss": 0.6917,
      "step": 435
    },
    {
      "epoch": 1.0163503649635037,
      "grad_norm": 3.7774510383605957,
      "learning_rate": 0.00014358108108108108,
      "loss": 0.7092,
      "step": 436
    },
    {
      "epoch": 1.0186861313868614,
      "grad_norm": 3.163459062576294,
      "learning_rate": 0.00014341216216216219,
      "loss": 0.4126,
      "step": 437
    },
    {
      "epoch": 1.021021897810219,
      "grad_norm": 3.5788369178771973,
      "learning_rate": 0.00014324324324324324,
      "loss": 0.9827,
      "step": 438
    },
    {
      "epoch": 1.0233576642335767,
      "grad_norm": 3.5603153705596924,
      "learning_rate": 0.00014307432432432432,
      "loss": 0.9384,
      "step": 439
    },
    {
      "epoch": 1.0256934306569343,
      "grad_norm": 2.47163987159729,
      "learning_rate": 0.0001429054054054054,
      "loss": 0.628,
      "step": 440
    },
    {
      "epoch": 1.028029197080292,
      "grad_norm": 9.257896423339844,
      "learning_rate": 0.0001427364864864865,
      "loss": 0.8678,
      "step": 441
    },
    {
      "epoch": 1.0303649635036496,
      "grad_norm": 4.375235557556152,
      "learning_rate": 0.00014256756756756756,
      "loss": 0.718,
      "step": 442
    },
    {
      "epoch": 1.0327007299270072,
      "grad_norm": 7.775930881500244,
      "learning_rate": 0.00014239864864864864,
      "loss": 0.4353,
      "step": 443
    },
    {
      "epoch": 1.0350364963503649,
      "grad_norm": 10.122757911682129,
      "learning_rate": 0.00014222972972972975,
      "loss": 1.3095,
      "step": 444
    },
    {
      "epoch": 1.0373722627737227,
      "grad_norm": 7.393694877624512,
      "learning_rate": 0.00014206081081081083,
      "loss": 0.7094,
      "step": 445
    },
    {
      "epoch": 1.0397080291970804,
      "grad_norm": 3.2135009765625,
      "learning_rate": 0.00014189189189189188,
      "loss": 0.9288,
      "step": 446
    },
    {
      "epoch": 1.042043795620438,
      "grad_norm": 5.30876350402832,
      "learning_rate": 0.000141722972972973,
      "loss": 0.5809,
      "step": 447
    },
    {
      "epoch": 1.0443795620437957,
      "grad_norm": 3.6422739028930664,
      "learning_rate": 0.00014155405405405407,
      "loss": 0.3461,
      "step": 448
    },
    {
      "epoch": 1.0467153284671533,
      "grad_norm": 3.07734751701355,
      "learning_rate": 0.00014138513513513513,
      "loss": 1.894,
      "step": 449
    },
    {
      "epoch": 1.049051094890511,
      "grad_norm": 2.529322385787964,
      "learning_rate": 0.0001412162162162162,
      "loss": 0.7668,
      "step": 450
    },
    {
      "epoch": 1.0513868613138686,
      "grad_norm": 4.1276726722717285,
      "learning_rate": 0.00014104729729729731,
      "loss": 1.6975,
      "step": 451
    },
    {
      "epoch": 1.0537226277372262,
      "grad_norm": 2.6643192768096924,
      "learning_rate": 0.0001408783783783784,
      "loss": 0.7128,
      "step": 452
    },
    {
      "epoch": 1.056058394160584,
      "grad_norm": 4.093564987182617,
      "learning_rate": 0.00014070945945945945,
      "loss": 0.7616,
      "step": 453
    },
    {
      "epoch": 1.0583941605839415,
      "grad_norm": 5.2682881355285645,
      "learning_rate": 0.00014054054054054056,
      "loss": 1.1163,
      "step": 454
    },
    {
      "epoch": 1.0607299270072992,
      "grad_norm": 1.4154895544052124,
      "learning_rate": 0.00014037162162162164,
      "loss": 0.6773,
      "step": 455
    },
    {
      "epoch": 1.0630656934306568,
      "grad_norm": 1.6841880083084106,
      "learning_rate": 0.0001402027027027027,
      "loss": 0.4854,
      "step": 456
    },
    {
      "epoch": 1.0654014598540147,
      "grad_norm": 4.4962873458862305,
      "learning_rate": 0.0001400337837837838,
      "loss": 0.2922,
      "step": 457
    },
    {
      "epoch": 1.0677372262773723,
      "grad_norm": 9.540863990783691,
      "learning_rate": 0.00013986486486486488,
      "loss": 0.2961,
      "step": 458
    },
    {
      "epoch": 1.07007299270073,
      "grad_norm": 4.336996078491211,
      "learning_rate": 0.00013969594594594596,
      "loss": 0.65,
      "step": 459
    },
    {
      "epoch": 1.0724087591240876,
      "grad_norm": 1.757400393486023,
      "learning_rate": 0.000139527027027027,
      "loss": 0.2517,
      "step": 460
    },
    {
      "epoch": 1.0747445255474453,
      "grad_norm": 2.2488203048706055,
      "learning_rate": 0.00013935810810810812,
      "loss": 0.356,
      "step": 461
    },
    {
      "epoch": 1.077080291970803,
      "grad_norm": 2.4168267250061035,
      "learning_rate": 0.0001391891891891892,
      "loss": 0.73,
      "step": 462
    },
    {
      "epoch": 1.0794160583941605,
      "grad_norm": 14.8611478805542,
      "learning_rate": 0.00013902027027027028,
      "loss": 0.9293,
      "step": 463
    },
    {
      "epoch": 1.0817518248175182,
      "grad_norm": 7.562475681304932,
      "learning_rate": 0.00013885135135135136,
      "loss": 0.4659,
      "step": 464
    },
    {
      "epoch": 1.0840875912408758,
      "grad_norm": 10.878897666931152,
      "learning_rate": 0.00013868243243243244,
      "loss": 0.5656,
      "step": 465
    },
    {
      "epoch": 1.0864233576642335,
      "grad_norm": 1.9428468942642212,
      "learning_rate": 0.00013851351351351352,
      "loss": 0.7216,
      "step": 466
    },
    {
      "epoch": 1.0887591240875913,
      "grad_norm": 4.490625381469727,
      "learning_rate": 0.0001383445945945946,
      "loss": 0.5168,
      "step": 467
    },
    {
      "epoch": 1.091094890510949,
      "grad_norm": 1.4554146528244019,
      "learning_rate": 0.00013817567567567569,
      "loss": 0.345,
      "step": 468
    },
    {
      "epoch": 1.0934306569343066,
      "grad_norm": 3.708404779434204,
      "learning_rate": 0.00013800675675675677,
      "loss": 0.8453,
      "step": 469
    },
    {
      "epoch": 1.0957664233576643,
      "grad_norm": 3.478311538696289,
      "learning_rate": 0.00013783783783783785,
      "loss": 0.546,
      "step": 470
    },
    {
      "epoch": 1.098102189781022,
      "grad_norm": 5.451096057891846,
      "learning_rate": 0.00013766891891891893,
      "loss": 0.5944,
      "step": 471
    },
    {
      "epoch": 1.1004379562043796,
      "grad_norm": 2.353628158569336,
      "learning_rate": 0.0001375,
      "loss": 1.2569,
      "step": 472
    },
    {
      "epoch": 1.1027737226277372,
      "grad_norm": 5.6700663566589355,
      "learning_rate": 0.0001373310810810811,
      "loss": 0.2717,
      "step": 473
    },
    {
      "epoch": 1.1051094890510949,
      "grad_norm": 3.992699146270752,
      "learning_rate": 0.00013716216216216217,
      "loss": 0.0819,
      "step": 474
    },
    {
      "epoch": 1.1074452554744525,
      "grad_norm": 4.970634937286377,
      "learning_rate": 0.00013699324324324325,
      "loss": 1.0442,
      "step": 475
    },
    {
      "epoch": 1.1097810218978101,
      "grad_norm": 5.993344783782959,
      "learning_rate": 0.00013682432432432433,
      "loss": 0.6957,
      "step": 476
    },
    {
      "epoch": 1.1121167883211678,
      "grad_norm": 3.7142879962921143,
      "learning_rate": 0.0001366554054054054,
      "loss": 0.2347,
      "step": 477
    },
    {
      "epoch": 1.1144525547445256,
      "grad_norm": 4.988788604736328,
      "learning_rate": 0.0001364864864864865,
      "loss": 0.7241,
      "step": 478
    },
    {
      "epoch": 1.1167883211678833,
      "grad_norm": 4.23198127746582,
      "learning_rate": 0.00013631756756756757,
      "loss": 0.6084,
      "step": 479
    },
    {
      "epoch": 1.119124087591241,
      "grad_norm": 1.568503737449646,
      "learning_rate": 0.00013614864864864865,
      "loss": 0.8374,
      "step": 480
    },
    {
      "epoch": 1.1214598540145986,
      "grad_norm": 6.3954668045043945,
      "learning_rate": 0.00013597972972972973,
      "loss": 0.3933,
      "step": 481
    },
    {
      "epoch": 1.1237956204379562,
      "grad_norm": 4.724154949188232,
      "learning_rate": 0.00013581081081081081,
      "loss": 0.9119,
      "step": 482
    },
    {
      "epoch": 1.1261313868613139,
      "grad_norm": 2.958904981613159,
      "learning_rate": 0.0001356418918918919,
      "loss": 0.592,
      "step": 483
    },
    {
      "epoch": 1.1284671532846715,
      "grad_norm": 3.609558343887329,
      "learning_rate": 0.00013547297297297298,
      "loss": 1.0113,
      "step": 484
    },
    {
      "epoch": 1.1308029197080292,
      "grad_norm": 1.9043591022491455,
      "learning_rate": 0.00013530405405405406,
      "loss": 0.3178,
      "step": 485
    },
    {
      "epoch": 1.1331386861313868,
      "grad_norm": 5.575804233551025,
      "learning_rate": 0.00013513513513513514,
      "loss": 0.7167,
      "step": 486
    },
    {
      "epoch": 1.1354744525547444,
      "grad_norm": 2.095766305923462,
      "learning_rate": 0.00013496621621621622,
      "loss": 1.0679,
      "step": 487
    },
    {
      "epoch": 1.1378102189781023,
      "grad_norm": 1.760097622871399,
      "learning_rate": 0.0001347972972972973,
      "loss": 0.1846,
      "step": 488
    },
    {
      "epoch": 1.14014598540146,
      "grad_norm": 3.480710983276367,
      "learning_rate": 0.00013462837837837838,
      "loss": 0.6395,
      "step": 489
    },
    {
      "epoch": 1.1424817518248176,
      "grad_norm": 2.260648250579834,
      "learning_rate": 0.00013445945945945946,
      "loss": 0.772,
      "step": 490
    },
    {
      "epoch": 1.1448175182481752,
      "grad_norm": 4.663466930389404,
      "learning_rate": 0.00013429054054054057,
      "loss": 0.359,
      "step": 491
    },
    {
      "epoch": 1.1471532846715329,
      "grad_norm": 6.141119956970215,
      "learning_rate": 0.00013412162162162162,
      "loss": 0.7838,
      "step": 492
    },
    {
      "epoch": 1.1494890510948905,
      "grad_norm": 1.9188268184661865,
      "learning_rate": 0.0001339527027027027,
      "loss": 0.9788,
      "step": 493
    },
    {
      "epoch": 1.1518248175182482,
      "grad_norm": 4.018616199493408,
      "learning_rate": 0.0001337837837837838,
      "loss": 0.4595,
      "step": 494
    },
    {
      "epoch": 1.1541605839416058,
      "grad_norm": 2.25522518157959,
      "learning_rate": 0.00013361486486486486,
      "loss": 0.1878,
      "step": 495
    },
    {
      "epoch": 1.1564963503649635,
      "grad_norm": 1.5120854377746582,
      "learning_rate": 0.00013344594594594594,
      "loss": 0.6008,
      "step": 496
    },
    {
      "epoch": 1.158832116788321,
      "grad_norm": 4.747971057891846,
      "learning_rate": 0.00013327702702702702,
      "loss": 0.4252,
      "step": 497
    },
    {
      "epoch": 1.1611678832116787,
      "grad_norm": 1.8794506788253784,
      "learning_rate": 0.00013310810810810813,
      "loss": 1.01,
      "step": 498
    },
    {
      "epoch": 1.1635036496350364,
      "grad_norm": 1.746358036994934,
      "learning_rate": 0.00013293918918918919,
      "loss": 2.5744,
      "step": 499
    },
    {
      "epoch": 1.1658394160583943,
      "grad_norm": 9.952081680297852,
      "learning_rate": 0.00013277027027027027,
      "loss": 0.5757,
      "step": 500
    },
    {
      "epoch": 1.168175182481752,
      "grad_norm": 2.418027400970459,
      "learning_rate": 0.00013260135135135137,
      "loss": 0.6715,
      "step": 501
    },
    {
      "epoch": 1.1705109489051095,
      "grad_norm": 2.9672658443450928,
      "learning_rate": 0.00013243243243243243,
      "loss": 0.5822,
      "step": 502
    },
    {
      "epoch": 1.1728467153284672,
      "grad_norm": 4.777379512786865,
      "learning_rate": 0.0001322635135135135,
      "loss": 0.7357,
      "step": 503
    },
    {
      "epoch": 1.1751824817518248,
      "grad_norm": 4.643790245056152,
      "learning_rate": 0.00013209459459459462,
      "loss": 0.3977,
      "step": 504
    },
    {
      "epoch": 1.1775182481751825,
      "grad_norm": 2.038804531097412,
      "learning_rate": 0.0001319256756756757,
      "loss": 1.1286,
      "step": 505
    },
    {
      "epoch": 1.1798540145985401,
      "grad_norm": 3.3418233394622803,
      "learning_rate": 0.00013175675675675675,
      "loss": 0.7113,
      "step": 506
    },
    {
      "epoch": 1.1821897810218978,
      "grad_norm": 4.087080478668213,
      "learning_rate": 0.00013158783783783783,
      "loss": 1.1878,
      "step": 507
    },
    {
      "epoch": 1.1845255474452554,
      "grad_norm": 2.3230299949645996,
      "learning_rate": 0.00013141891891891894,
      "loss": 1.9473,
      "step": 508
    },
    {
      "epoch": 1.186861313868613,
      "grad_norm": 2.6505794525146484,
      "learning_rate": 0.00013125000000000002,
      "loss": 0.2226,
      "step": 509
    },
    {
      "epoch": 1.189197080291971,
      "grad_norm": 3.90958571434021,
      "learning_rate": 0.00013108108108108107,
      "loss": 0.9778,
      "step": 510
    },
    {
      "epoch": 1.1915328467153286,
      "grad_norm": 2.0912463665008545,
      "learning_rate": 0.00013091216216216218,
      "loss": 0.4416,
      "step": 511
    },
    {
      "epoch": 1.1938686131386862,
      "grad_norm": 2.934500217437744,
      "learning_rate": 0.00013074324324324326,
      "loss": 0.4881,
      "step": 512
    },
    {
      "epoch": 1.1962043795620438,
      "grad_norm": 2.147897958755493,
      "learning_rate": 0.00013057432432432431,
      "loss": 0.2948,
      "step": 513
    },
    {
      "epoch": 1.1985401459854015,
      "grad_norm": 4.584478378295898,
      "learning_rate": 0.0001304054054054054,
      "loss": 0.5805,
      "step": 514
    },
    {
      "epoch": 1.2008759124087591,
      "grad_norm": 4.407607555389404,
      "learning_rate": 0.0001302364864864865,
      "loss": 0.6014,
      "step": 515
    },
    {
      "epoch": 1.2032116788321168,
      "grad_norm": 2.376967668533325,
      "learning_rate": 0.00013006756756756758,
      "loss": 0.5401,
      "step": 516
    },
    {
      "epoch": 1.2055474452554744,
      "grad_norm": 7.897099018096924,
      "learning_rate": 0.00012989864864864864,
      "loss": 0.6434,
      "step": 517
    },
    {
      "epoch": 1.207883211678832,
      "grad_norm": 9.265311241149902,
      "learning_rate": 0.00012972972972972974,
      "loss": 0.7708,
      "step": 518
    },
    {
      "epoch": 1.2102189781021897,
      "grad_norm": 2.112762928009033,
      "learning_rate": 0.00012956081081081083,
      "loss": 0.6977,
      "step": 519
    },
    {
      "epoch": 1.2125547445255473,
      "grad_norm": 2.773000717163086,
      "learning_rate": 0.00012939189189189188,
      "loss": 1.394,
      "step": 520
    },
    {
      "epoch": 1.214890510948905,
      "grad_norm": 3.4872798919677734,
      "learning_rate": 0.000129222972972973,
      "loss": 0.5779,
      "step": 521
    },
    {
      "epoch": 1.2172262773722629,
      "grad_norm": 2.233842372894287,
      "learning_rate": 0.00012905405405405407,
      "loss": 0.3789,
      "step": 522
    },
    {
      "epoch": 1.2195620437956205,
      "grad_norm": 2.169309139251709,
      "learning_rate": 0.00012888513513513515,
      "loss": 0.8546,
      "step": 523
    },
    {
      "epoch": 1.2218978102189781,
      "grad_norm": 2.422292709350586,
      "learning_rate": 0.0001287162162162162,
      "loss": 0.3203,
      "step": 524
    },
    {
      "epoch": 1.2242335766423358,
      "grad_norm": 1.5928748846054077,
      "learning_rate": 0.0001285472972972973,
      "loss": 1.9447,
      "step": 525
    },
    {
      "epoch": 1.2265693430656934,
      "grad_norm": 4.372344970703125,
      "learning_rate": 0.0001283783783783784,
      "loss": 0.2811,
      "step": 526
    },
    {
      "epoch": 1.228905109489051,
      "grad_norm": 2.894336223602295,
      "learning_rate": 0.00012820945945945947,
      "loss": 0.5803,
      "step": 527
    },
    {
      "epoch": 1.2312408759124087,
      "grad_norm": 6.96621561050415,
      "learning_rate": 0.00012804054054054055,
      "loss": 0.4235,
      "step": 528
    },
    {
      "epoch": 1.2335766423357664,
      "grad_norm": 3.6265008449554443,
      "learning_rate": 0.00012787162162162163,
      "loss": 1.1738,
      "step": 529
    },
    {
      "epoch": 1.235912408759124,
      "grad_norm": 2.79616379737854,
      "learning_rate": 0.0001277027027027027,
      "loss": 0.4158,
      "step": 530
    },
    {
      "epoch": 1.2382481751824819,
      "grad_norm": 2.1088099479675293,
      "learning_rate": 0.0001275337837837838,
      "loss": 1.0351,
      "step": 531
    },
    {
      "epoch": 1.2405839416058395,
      "grad_norm": 4.855320453643799,
      "learning_rate": 0.00012736486486486487,
      "loss": 0.7606,
      "step": 532
    },
    {
      "epoch": 1.2429197080291972,
      "grad_norm": 2.527460813522339,
      "learning_rate": 0.00012719594594594595,
      "loss": 1.3047,
      "step": 533
    },
    {
      "epoch": 1.2452554744525548,
      "grad_norm": 3.3988590240478516,
      "learning_rate": 0.00012702702702702703,
      "loss": 0.559,
      "step": 534
    },
    {
      "epoch": 1.2475912408759124,
      "grad_norm": 3.532290458679199,
      "learning_rate": 0.00012685810810810812,
      "loss": 0.6811,
      "step": 535
    },
    {
      "epoch": 1.24992700729927,
      "grad_norm": 2.8737471103668213,
      "learning_rate": 0.0001266891891891892,
      "loss": 1.9213,
      "step": 536
    },
    {
      "epoch": 1.2522627737226277,
      "grad_norm": 2.341247320175171,
      "learning_rate": 0.00012652027027027028,
      "loss": 0.7994,
      "step": 537
    },
    {
      "epoch": 1.2545985401459854,
      "grad_norm": 2.820291757583618,
      "learning_rate": 0.00012635135135135136,
      "loss": 1.0104,
      "step": 538
    },
    {
      "epoch": 1.256934306569343,
      "grad_norm": 9.025318145751953,
      "learning_rate": 0.00012618243243243244,
      "loss": 0.4871,
      "step": 539
    },
    {
      "epoch": 1.2592700729927007,
      "grad_norm": 4.90973424911499,
      "learning_rate": 0.00012601351351351352,
      "loss": 0.9057,
      "step": 540
    },
    {
      "epoch": 1.2616058394160583,
      "grad_norm": 3.7741854190826416,
      "learning_rate": 0.0001258445945945946,
      "loss": 0.3941,
      "step": 541
    },
    {
      "epoch": 1.263941605839416,
      "grad_norm": 5.697646141052246,
      "learning_rate": 0.00012567567567567568,
      "loss": 0.2676,
      "step": 542
    },
    {
      "epoch": 1.2662773722627736,
      "grad_norm": 2.27360463142395,
      "learning_rate": 0.00012550675675675676,
      "loss": 1.1696,
      "step": 543
    },
    {
      "epoch": 1.2686131386861315,
      "grad_norm": 6.3615827560424805,
      "learning_rate": 0.00012533783783783784,
      "loss": 0.4573,
      "step": 544
    },
    {
      "epoch": 1.270948905109489,
      "grad_norm": 6.627460479736328,
      "learning_rate": 0.00012516891891891892,
      "loss": 0.8,
      "step": 545
    },
    {
      "epoch": 1.2732846715328467,
      "grad_norm": 1.5485548973083496,
      "learning_rate": 0.000125,
      "loss": 0.5721,
      "step": 546
    },
    {
      "epoch": 1.2756204379562044,
      "grad_norm": 7.7313103675842285,
      "learning_rate": 0.00012483108108108108,
      "loss": 0.6883,
      "step": 547
    },
    {
      "epoch": 1.277956204379562,
      "grad_norm": 4.763189792633057,
      "learning_rate": 0.00012466216216216216,
      "loss": 0.1768,
      "step": 548
    },
    {
      "epoch": 1.2802919708029197,
      "grad_norm": 2.382877826690674,
      "learning_rate": 0.00012449324324324324,
      "loss": 0.7408,
      "step": 549
    },
    {
      "epoch": 1.2826277372262773,
      "grad_norm": 2.7286219596862793,
      "learning_rate": 0.00012432432432432433,
      "loss": 0.6861,
      "step": 550
    },
    {
      "epoch": 1.284963503649635,
      "grad_norm": 3.980268716812134,
      "learning_rate": 0.0001241554054054054,
      "loss": 0.363,
      "step": 551
    },
    {
      "epoch": 1.2872992700729928,
      "grad_norm": 5.0962677001953125,
      "learning_rate": 0.00012398648648648649,
      "loss": 0.521,
      "step": 552
    },
    {
      "epoch": 1.2896350364963505,
      "grad_norm": 2.5538899898529053,
      "learning_rate": 0.00012381756756756757,
      "loss": 0.739,
      "step": 553
    },
    {
      "epoch": 1.2919708029197081,
      "grad_norm": 3.512890577316284,
      "learning_rate": 0.00012364864864864865,
      "loss": 0.5069,
      "step": 554
    },
    {
      "epoch": 1.2943065693430658,
      "grad_norm": 3.6836414337158203,
      "learning_rate": 0.00012347972972972976,
      "loss": 1.2659,
      "step": 555
    },
    {
      "epoch": 1.2966423357664234,
      "grad_norm": 2.6543407440185547,
      "learning_rate": 0.0001233108108108108,
      "loss": 0.5137,
      "step": 556
    },
    {
      "epoch": 1.298978102189781,
      "grad_norm": 3.8334836959838867,
      "learning_rate": 0.0001231418918918919,
      "loss": 0.883,
      "step": 557
    },
    {
      "epoch": 1.3013138686131387,
      "grad_norm": 3.3452484607696533,
      "learning_rate": 0.000122972972972973,
      "loss": 0.5719,
      "step": 558
    },
    {
      "epoch": 1.3036496350364963,
      "grad_norm": 2.119973659515381,
      "learning_rate": 0.00012280405405405405,
      "loss": 0.7441,
      "step": 559
    },
    {
      "epoch": 1.305985401459854,
      "grad_norm": 1.5954504013061523,
      "learning_rate": 0.00012263513513513513,
      "loss": 0.1623,
      "step": 560
    },
    {
      "epoch": 1.3083211678832116,
      "grad_norm": 3.5536465644836426,
      "learning_rate": 0.0001224662162162162,
      "loss": 1.1732,
      "step": 561
    },
    {
      "epoch": 1.3106569343065693,
      "grad_norm": 1.7904388904571533,
      "learning_rate": 0.00012229729729729732,
      "loss": 0.6606,
      "step": 562
    },
    {
      "epoch": 1.312992700729927,
      "grad_norm": 5.595737457275391,
      "learning_rate": 0.00012212837837837837,
      "loss": 0.6079,
      "step": 563
    },
    {
      "epoch": 1.3153284671532846,
      "grad_norm": 2.813387155532837,
      "learning_rate": 0.00012195945945945945,
      "loss": 0.9812,
      "step": 564
    },
    {
      "epoch": 1.3176642335766424,
      "grad_norm": 3.234769105911255,
      "learning_rate": 0.00012179054054054055,
      "loss": 0.4886,
      "step": 565
    },
    {
      "epoch": 1.32,
      "grad_norm": 6.8744330406188965,
      "learning_rate": 0.00012162162162162163,
      "loss": 0.4938,
      "step": 566
    },
    {
      "epoch": 1.3223357664233577,
      "grad_norm": 4.682141304016113,
      "learning_rate": 0.00012145270270270271,
      "loss": 0.8777,
      "step": 567
    },
    {
      "epoch": 1.3246715328467153,
      "grad_norm": 5.041783809661865,
      "learning_rate": 0.0001212837837837838,
      "loss": 0.4564,
      "step": 568
    },
    {
      "epoch": 1.327007299270073,
      "grad_norm": 6.01272439956665,
      "learning_rate": 0.00012111486486486487,
      "loss": 1.2756,
      "step": 569
    },
    {
      "epoch": 1.3293430656934306,
      "grad_norm": 2.6941866874694824,
      "learning_rate": 0.00012094594594594595,
      "loss": 1.0279,
      "step": 570
    },
    {
      "epoch": 1.3316788321167883,
      "grad_norm": 4.157473564147949,
      "learning_rate": 0.00012077702702702702,
      "loss": 0.5357,
      "step": 571
    },
    {
      "epoch": 1.334014598540146,
      "grad_norm": 3.2945284843444824,
      "learning_rate": 0.00012060810810810813,
      "loss": 0.9541,
      "step": 572
    },
    {
      "epoch": 1.3363503649635036,
      "grad_norm": 3.0104987621307373,
      "learning_rate": 0.0001204391891891892,
      "loss": 0.3829,
      "step": 573
    },
    {
      "epoch": 1.3386861313868614,
      "grad_norm": 2.742198944091797,
      "learning_rate": 0.00012027027027027027,
      "loss": 0.5757,
      "step": 574
    },
    {
      "epoch": 1.341021897810219,
      "grad_norm": 1.33230459690094,
      "learning_rate": 0.00012010135135135137,
      "loss": 0.3765,
      "step": 575
    },
    {
      "epoch": 1.3433576642335767,
      "grad_norm": 3.499751329421997,
      "learning_rate": 0.00011993243243243244,
      "loss": 0.6307,
      "step": 576
    },
    {
      "epoch": 1.3456934306569344,
      "grad_norm": 4.105998516082764,
      "learning_rate": 0.00011976351351351352,
      "loss": 0.3837,
      "step": 577
    },
    {
      "epoch": 1.348029197080292,
      "grad_norm": 4.162230968475342,
      "learning_rate": 0.00011959459459459461,
      "loss": 0.6844,
      "step": 578
    },
    {
      "epoch": 1.3503649635036497,
      "grad_norm": 5.860576629638672,
      "learning_rate": 0.00011942567567567569,
      "loss": 0.5234,
      "step": 579
    },
    {
      "epoch": 1.3527007299270073,
      "grad_norm": 6.394032001495361,
      "learning_rate": 0.00011925675675675676,
      "loss": 0.5114,
      "step": 580
    },
    {
      "epoch": 1.355036496350365,
      "grad_norm": 2.483757495880127,
      "learning_rate": 0.00011908783783783784,
      "loss": 0.8154,
      "step": 581
    },
    {
      "epoch": 1.3573722627737226,
      "grad_norm": 8.320639610290527,
      "learning_rate": 0.00011891891891891893,
      "loss": 0.6299,
      "step": 582
    },
    {
      "epoch": 1.3597080291970802,
      "grad_norm": 3.079892873764038,
      "learning_rate": 0.00011875,
      "loss": 0.502,
      "step": 583
    },
    {
      "epoch": 1.3620437956204379,
      "grad_norm": 3.378688097000122,
      "learning_rate": 0.00011858108108108108,
      "loss": 0.3447,
      "step": 584
    },
    {
      "epoch": 1.3643795620437955,
      "grad_norm": 7.331258296966553,
      "learning_rate": 0.00011841216216216217,
      "loss": 0.9933,
      "step": 585
    },
    {
      "epoch": 1.3667153284671532,
      "grad_norm": 5.125000953674316,
      "learning_rate": 0.00011824324324324326,
      "loss": 0.9942,
      "step": 586
    },
    {
      "epoch": 1.369051094890511,
      "grad_norm": 6.590306282043457,
      "learning_rate": 0.00011807432432432432,
      "loss": 0.4303,
      "step": 587
    },
    {
      "epoch": 1.3713868613138687,
      "grad_norm": 2.864807367324829,
      "learning_rate": 0.0001179054054054054,
      "loss": 1.0077,
      "step": 588
    },
    {
      "epoch": 1.3737226277372263,
      "grad_norm": 6.212836742401123,
      "learning_rate": 0.0001177364864864865,
      "loss": 0.7671,
      "step": 589
    },
    {
      "epoch": 1.376058394160584,
      "grad_norm": 7.0513739585876465,
      "learning_rate": 0.00011756756756756758,
      "loss": 0.6134,
      "step": 590
    },
    {
      "epoch": 1.3783941605839416,
      "grad_norm": 7.633090019226074,
      "learning_rate": 0.00011739864864864864,
      "loss": 0.4267,
      "step": 591
    },
    {
      "epoch": 1.3807299270072992,
      "grad_norm": 3.2789690494537354,
      "learning_rate": 0.00011722972972972974,
      "loss": 1.3884,
      "step": 592
    },
    {
      "epoch": 1.3830656934306569,
      "grad_norm": 1.6279276609420776,
      "learning_rate": 0.00011706081081081082,
      "loss": 1.6225,
      "step": 593
    },
    {
      "epoch": 1.3854014598540145,
      "grad_norm": 2.978745222091675,
      "learning_rate": 0.00011689189189189189,
      "loss": 1.3304,
      "step": 594
    },
    {
      "epoch": 1.3877372262773724,
      "grad_norm": 3.958935260772705,
      "learning_rate": 0.000116722972972973,
      "loss": 0.4981,
      "step": 595
    },
    {
      "epoch": 1.39007299270073,
      "grad_norm": 4.397089958190918,
      "learning_rate": 0.00011655405405405406,
      "loss": 0.5121,
      "step": 596
    },
    {
      "epoch": 1.3924087591240877,
      "grad_norm": 3.487309694290161,
      "learning_rate": 0.00011638513513513514,
      "loss": 0.4558,
      "step": 597
    },
    {
      "epoch": 1.3947445255474453,
      "grad_norm": 3.6038818359375,
      "learning_rate": 0.00011621621621621621,
      "loss": 0.5077,
      "step": 598
    },
    {
      "epoch": 1.397080291970803,
      "grad_norm": 8.617998123168945,
      "learning_rate": 0.0001160472972972973,
      "loss": 0.4353,
      "step": 599
    },
    {
      "epoch": 1.3994160583941606,
      "grad_norm": 3.721604108810425,
      "learning_rate": 0.00011587837837837838,
      "loss": 1.1753,
      "step": 600
    },
    {
      "epoch": 1.4017518248175183,
      "grad_norm": 3.981379747390747,
      "learning_rate": 0.00011570945945945945,
      "loss": 0.3247,
      "step": 601
    },
    {
      "epoch": 1.404087591240876,
      "grad_norm": 2.318251371383667,
      "learning_rate": 0.00011554054054054056,
      "loss": 1.0513,
      "step": 602
    },
    {
      "epoch": 1.4064233576642335,
      "grad_norm": 2.5470340251922607,
      "learning_rate": 0.00011537162162162163,
      "loss": 1.0008,
      "step": 603
    },
    {
      "epoch": 1.4087591240875912,
      "grad_norm": 2.672858476638794,
      "learning_rate": 0.0001152027027027027,
      "loss": 1.2117,
      "step": 604
    },
    {
      "epoch": 1.4110948905109488,
      "grad_norm": 1.7124364376068115,
      "learning_rate": 0.0001150337837837838,
      "loss": 0.7316,
      "step": 605
    },
    {
      "epoch": 1.4134306569343065,
      "grad_norm": 2.279566764831543,
      "learning_rate": 0.00011486486486486487,
      "loss": 0.8848,
      "step": 606
    },
    {
      "epoch": 1.4157664233576641,
      "grad_norm": 6.179100513458252,
      "learning_rate": 0.00011469594594594595,
      "loss": 0.6691,
      "step": 607
    },
    {
      "epoch": 1.4181021897810218,
      "grad_norm": 4.798680305480957,
      "learning_rate": 0.00011452702702702703,
      "loss": 0.7309,
      "step": 608
    },
    {
      "epoch": 1.4204379562043796,
      "grad_norm": 3.5797367095947266,
      "learning_rate": 0.00011435810810810812,
      "loss": 0.4912,
      "step": 609
    },
    {
      "epoch": 1.4227737226277373,
      "grad_norm": 3.282083034515381,
      "learning_rate": 0.00011418918918918919,
      "loss": 1.4057,
      "step": 610
    },
    {
      "epoch": 1.425109489051095,
      "grad_norm": 1.4568601846694946,
      "learning_rate": 0.00011402027027027027,
      "loss": 2.2722,
      "step": 611
    },
    {
      "epoch": 1.4274452554744526,
      "grad_norm": 4.0540266036987305,
      "learning_rate": 0.00011385135135135137,
      "loss": 0.5347,
      "step": 612
    },
    {
      "epoch": 1.4297810218978102,
      "grad_norm": 2.1410505771636963,
      "learning_rate": 0.00011368243243243245,
      "loss": 0.6729,
      "step": 613
    },
    {
      "epoch": 1.4321167883211678,
      "grad_norm": 4.41209602355957,
      "learning_rate": 0.00011351351351351351,
      "loss": 0.9643,
      "step": 614
    },
    {
      "epoch": 1.4344525547445255,
      "grad_norm": 11.362465858459473,
      "learning_rate": 0.00011334459459459461,
      "loss": 0.4595,
      "step": 615
    },
    {
      "epoch": 1.4367883211678831,
      "grad_norm": 5.234164237976074,
      "learning_rate": 0.00011317567567567569,
      "loss": 0.4942,
      "step": 616
    },
    {
      "epoch": 1.439124087591241,
      "grad_norm": 2.0687155723571777,
      "learning_rate": 0.00011300675675675676,
      "loss": 0.1824,
      "step": 617
    },
    {
      "epoch": 1.4414598540145986,
      "grad_norm": 2.4064748287200928,
      "learning_rate": 0.00011283783783783784,
      "loss": 0.82,
      "step": 618
    },
    {
      "epoch": 1.4437956204379563,
      "grad_norm": 1.843794822692871,
      "learning_rate": 0.00011266891891891893,
      "loss": 0.4365,
      "step": 619
    },
    {
      "epoch": 1.446131386861314,
      "grad_norm": 3.1282215118408203,
      "learning_rate": 0.00011250000000000001,
      "loss": 1.8627,
      "step": 620
    },
    {
      "epoch": 1.4484671532846716,
      "grad_norm": 4.35580587387085,
      "learning_rate": 0.00011233108108108108,
      "loss": 0.3752,
      "step": 621
    },
    {
      "epoch": 1.4508029197080292,
      "grad_norm": 5.441629409790039,
      "learning_rate": 0.00011216216216216217,
      "loss": 0.9648,
      "step": 622
    },
    {
      "epoch": 1.4531386861313869,
      "grad_norm": 2.4105618000030518,
      "learning_rate": 0.00011199324324324325,
      "loss": 0.4248,
      "step": 623
    },
    {
      "epoch": 1.4554744525547445,
      "grad_norm": 3.7055203914642334,
      "learning_rate": 0.00011182432432432432,
      "loss": 1.8297,
      "step": 624
    },
    {
      "epoch": 1.4578102189781021,
      "grad_norm": 7.106746673583984,
      "learning_rate": 0.0001116554054054054,
      "loss": 0.7361,
      "step": 625
    },
    {
      "epoch": 1.4601459854014598,
      "grad_norm": 1.9916157722473145,
      "learning_rate": 0.0001114864864864865,
      "loss": 0.7475,
      "step": 626
    },
    {
      "epoch": 1.4624817518248174,
      "grad_norm": 3.593158721923828,
      "learning_rate": 0.00011131756756756757,
      "loss": 0.5966,
      "step": 627
    },
    {
      "epoch": 1.464817518248175,
      "grad_norm": 2.2155237197875977,
      "learning_rate": 0.00011114864864864864,
      "loss": 0.9571,
      "step": 628
    },
    {
      "epoch": 1.4671532846715327,
      "grad_norm": 5.471320629119873,
      "learning_rate": 0.00011097972972972974,
      "loss": 0.4447,
      "step": 629
    },
    {
      "epoch": 1.4694890510948906,
      "grad_norm": 3.977742910385132,
      "learning_rate": 0.00011081081081081082,
      "loss": 1.1247,
      "step": 630
    },
    {
      "epoch": 1.4718248175182482,
      "grad_norm": 2.913297176361084,
      "learning_rate": 0.0001106418918918919,
      "loss": 0.2943,
      "step": 631
    },
    {
      "epoch": 1.4741605839416059,
      "grad_norm": 3.522810220718384,
      "learning_rate": 0.00011047297297297299,
      "loss": 0.2876,
      "step": 632
    },
    {
      "epoch": 1.4764963503649635,
      "grad_norm": 4.361451148986816,
      "learning_rate": 0.00011030405405405406,
      "loss": 0.3203,
      "step": 633
    },
    {
      "epoch": 1.4788321167883212,
      "grad_norm": 2.0349650382995605,
      "learning_rate": 0.00011013513513513514,
      "loss": 0.4098,
      "step": 634
    },
    {
      "epoch": 1.4811678832116788,
      "grad_norm": 4.556489944458008,
      "learning_rate": 0.0001099662162162162,
      "loss": 0.5166,
      "step": 635
    },
    {
      "epoch": 1.4835036496350364,
      "grad_norm": 3.4703524112701416,
      "learning_rate": 0.00010979729729729731,
      "loss": 1.8076,
      "step": 636
    },
    {
      "epoch": 1.485839416058394,
      "grad_norm": 3.376215696334839,
      "learning_rate": 0.00010962837837837838,
      "loss": 1.1445,
      "step": 637
    },
    {
      "epoch": 1.488175182481752,
      "grad_norm": 1.9961419105529785,
      "learning_rate": 0.00010945945945945946,
      "loss": 0.5183,
      "step": 638
    },
    {
      "epoch": 1.4905109489051096,
      "grad_norm": 3.4434192180633545,
      "learning_rate": 0.00010929054054054056,
      "loss": 0.4881,
      "step": 639
    },
    {
      "epoch": 1.4928467153284672,
      "grad_norm": 1.827154278755188,
      "learning_rate": 0.00010912162162162162,
      "loss": 1.0542,
      "step": 640
    },
    {
      "epoch": 1.4951824817518249,
      "grad_norm": 4.5911030769348145,
      "learning_rate": 0.0001089527027027027,
      "loss": 1.489,
      "step": 641
    },
    {
      "epoch": 1.4975182481751825,
      "grad_norm": 10.032126426696777,
      "learning_rate": 0.0001087837837837838,
      "loss": 0.85,
      "step": 642
    },
    {
      "epoch": 1.4998540145985402,
      "grad_norm": 2.181333065032959,
      "learning_rate": 0.00010861486486486488,
      "loss": 0.5642,
      "step": 643
    },
    {
      "epoch": 1.5021897810218978,
      "grad_norm": 4.140478610992432,
      "learning_rate": 0.00010844594594594595,
      "loss": 0.861,
      "step": 644
    },
    {
      "epoch": 1.5045255474452555,
      "grad_norm": 11.7352933883667,
      "learning_rate": 0.00010827702702702703,
      "loss": 0.6686,
      "step": 645
    },
    {
      "epoch": 1.506861313868613,
      "grad_norm": 5.048130035400391,
      "learning_rate": 0.00010810810810810812,
      "loss": 0.6724,
      "step": 646
    },
    {
      "epoch": 1.5091970802919707,
      "grad_norm": 2.8917198181152344,
      "learning_rate": 0.00010793918918918919,
      "loss": 0.5818,
      "step": 647
    },
    {
      "epoch": 1.5115328467153284,
      "grad_norm": 4.773207664489746,
      "learning_rate": 0.00010777027027027027,
      "loss": 0.4062,
      "step": 648
    },
    {
      "epoch": 1.513868613138686,
      "grad_norm": 1.7943518161773682,
      "learning_rate": 0.00010760135135135136,
      "loss": 0.7472,
      "step": 649
    },
    {
      "epoch": 1.5162043795620437,
      "grad_norm": 11.546401023864746,
      "learning_rate": 0.00010743243243243244,
      "loss": 0.7363,
      "step": 650
    },
    {
      "epoch": 1.5185401459854013,
      "grad_norm": 3.11329984664917,
      "learning_rate": 0.00010726351351351351,
      "loss": 1.0096,
      "step": 651
    },
    {
      "epoch": 1.520875912408759,
      "grad_norm": 2.93340802192688,
      "learning_rate": 0.0001070945945945946,
      "loss": 1.1178,
      "step": 652
    },
    {
      "epoch": 1.5232116788321168,
      "grad_norm": 3.908690929412842,
      "learning_rate": 0.00010692567567567569,
      "loss": 0.8864,
      "step": 653
    },
    {
      "epoch": 1.5255474452554745,
      "grad_norm": 1.4461666345596313,
      "learning_rate": 0.00010675675675675677,
      "loss": 0.221,
      "step": 654
    },
    {
      "epoch": 1.5278832116788321,
      "grad_norm": 2.3317744731903076,
      "learning_rate": 0.00010658783783783783,
      "loss": 0.6973,
      "step": 655
    },
    {
      "epoch": 1.5302189781021898,
      "grad_norm": 7.71266508102417,
      "learning_rate": 0.00010641891891891893,
      "loss": 0.8281,
      "step": 656
    },
    {
      "epoch": 1.5325547445255474,
      "grad_norm": 1.6612719297409058,
      "learning_rate": 0.00010625000000000001,
      "loss": 0.8743,
      "step": 657
    },
    {
      "epoch": 1.5348905109489053,
      "grad_norm": 8.536913871765137,
      "learning_rate": 0.00010608108108108107,
      "loss": 0.4902,
      "step": 658
    },
    {
      "epoch": 1.537226277372263,
      "grad_norm": 0.9081225395202637,
      "learning_rate": 0.00010591216216216218,
      "loss": 0.3463,
      "step": 659
    },
    {
      "epoch": 1.5395620437956206,
      "grad_norm": 2.2914392948150635,
      "learning_rate": 0.00010574324324324325,
      "loss": 0.72,
      "step": 660
    },
    {
      "epoch": 1.5418978102189782,
      "grad_norm": 4.994395732879639,
      "learning_rate": 0.00010557432432432433,
      "loss": 0.4149,
      "step": 661
    },
    {
      "epoch": 1.5442335766423358,
      "grad_norm": 3.84348726272583,
      "learning_rate": 0.0001054054054054054,
      "loss": 0.2924,
      "step": 662
    },
    {
      "epoch": 1.5465693430656935,
      "grad_norm": 5.477197647094727,
      "learning_rate": 0.00010523648648648649,
      "loss": 0.3637,
      "step": 663
    },
    {
      "epoch": 1.5489051094890511,
      "grad_norm": 1.7362767457962036,
      "learning_rate": 0.00010506756756756757,
      "loss": 0.3042,
      "step": 664
    },
    {
      "epoch": 1.5512408759124088,
      "grad_norm": 2.576918840408325,
      "learning_rate": 0.00010489864864864864,
      "loss": 1.1193,
      "step": 665
    },
    {
      "epoch": 1.5535766423357664,
      "grad_norm": 1.1713285446166992,
      "learning_rate": 0.00010472972972972975,
      "loss": 0.1636,
      "step": 666
    },
    {
      "epoch": 1.555912408759124,
      "grad_norm": 2.994955539703369,
      "learning_rate": 0.00010456081081081081,
      "loss": 0.6216,
      "step": 667
    },
    {
      "epoch": 1.5582481751824817,
      "grad_norm": 3.652167320251465,
      "learning_rate": 0.0001043918918918919,
      "loss": 0.7414,
      "step": 668
    },
    {
      "epoch": 1.5605839416058394,
      "grad_norm": 3.5755207538604736,
      "learning_rate": 0.00010422297297297299,
      "loss": 0.5464,
      "step": 669
    },
    {
      "epoch": 1.562919708029197,
      "grad_norm": 4.736115455627441,
      "learning_rate": 0.00010405405405405406,
      "loss": 0.4061,
      "step": 670
    },
    {
      "epoch": 1.5652554744525546,
      "grad_norm": 3.577451229095459,
      "learning_rate": 0.00010388513513513514,
      "loss": 0.3746,
      "step": 671
    },
    {
      "epoch": 1.5675912408759123,
      "grad_norm": 3.117264986038208,
      "learning_rate": 0.00010371621621621622,
      "loss": 0.2893,
      "step": 672
    },
    {
      "epoch": 1.56992700729927,
      "grad_norm": 12.026823043823242,
      "learning_rate": 0.00010354729729729731,
      "loss": 0.6086,
      "step": 673
    },
    {
      "epoch": 1.5722627737226276,
      "grad_norm": 7.848832607269287,
      "learning_rate": 0.00010337837837837838,
      "loss": 0.5149,
      "step": 674
    },
    {
      "epoch": 1.5745985401459854,
      "grad_norm": 3.130671977996826,
      "learning_rate": 0.00010320945945945946,
      "loss": 1.1201,
      "step": 675
    },
    {
      "epoch": 1.576934306569343,
      "grad_norm": 4.248033046722412,
      "learning_rate": 0.00010304054054054055,
      "loss": 0.8417,
      "step": 676
    },
    {
      "epoch": 1.5792700729927007,
      "grad_norm": 5.046517372131348,
      "learning_rate": 0.00010287162162162163,
      "loss": 0.6943,
      "step": 677
    },
    {
      "epoch": 1.5816058394160584,
      "grad_norm": 3.2405636310577393,
      "learning_rate": 0.0001027027027027027,
      "loss": 0.7098,
      "step": 678
    },
    {
      "epoch": 1.583941605839416,
      "grad_norm": 2.9253616333007812,
      "learning_rate": 0.0001025337837837838,
      "loss": 0.3766,
      "step": 679
    },
    {
      "epoch": 1.5862773722627739,
      "grad_norm": 7.463962554931641,
      "learning_rate": 0.00010236486486486488,
      "loss": 0.3237,
      "step": 680
    },
    {
      "epoch": 1.5886131386861315,
      "grad_norm": 3.924046277999878,
      "learning_rate": 0.00010219594594594594,
      "loss": 1.0963,
      "step": 681
    },
    {
      "epoch": 1.5909489051094892,
      "grad_norm": 3.732506513595581,
      "learning_rate": 0.00010202702702702702,
      "loss": 1.454,
      "step": 682
    },
    {
      "epoch": 1.5932846715328468,
      "grad_norm": 2.732954740524292,
      "learning_rate": 0.00010185810810810812,
      "loss": 0.6018,
      "step": 683
    },
    {
      "epoch": 1.5956204379562045,
      "grad_norm": 8.321537017822266,
      "learning_rate": 0.0001016891891891892,
      "loss": 0.623,
      "step": 684
    },
    {
      "epoch": 1.597956204379562,
      "grad_norm": 2.186129093170166,
      "learning_rate": 0.00010152027027027027,
      "loss": 0.8822,
      "step": 685
    },
    {
      "epoch": 1.6002919708029197,
      "grad_norm": 3.583017349243164,
      "learning_rate": 0.00010135135135135136,
      "loss": 0.8432,
      "step": 686
    },
    {
      "epoch": 1.6026277372262774,
      "grad_norm": 1.5582187175750732,
      "learning_rate": 0.00010118243243243244,
      "loss": 0.1877,
      "step": 687
    },
    {
      "epoch": 1.604963503649635,
      "grad_norm": 8.83817195892334,
      "learning_rate": 0.00010101351351351351,
      "loss": 0.5459,
      "step": 688
    },
    {
      "epoch": 1.6072992700729927,
      "grad_norm": 5.0341386795043945,
      "learning_rate": 0.00010084459459459462,
      "loss": 0.9006,
      "step": 689
    },
    {
      "epoch": 1.6096350364963503,
      "grad_norm": 3.765071392059326,
      "learning_rate": 0.00010067567567567568,
      "loss": 0.6596,
      "step": 690
    },
    {
      "epoch": 1.611970802919708,
      "grad_norm": 1.8243200778961182,
      "learning_rate": 0.00010050675675675676,
      "loss": 0.4112,
      "step": 691
    },
    {
      "epoch": 1.6143065693430656,
      "grad_norm": 4.258193016052246,
      "learning_rate": 0.00010033783783783783,
      "loss": 0.7603,
      "step": 692
    },
    {
      "epoch": 1.6166423357664232,
      "grad_norm": 5.206897735595703,
      "learning_rate": 0.00010016891891891892,
      "loss": 0.4698,
      "step": 693
    },
    {
      "epoch": 1.6189781021897809,
      "grad_norm": 4.4159746170043945,
      "learning_rate": 0.0001,
      "loss": 0.095,
      "step": 694
    },
    {
      "epoch": 1.6213138686131385,
      "grad_norm": 2.1320066452026367,
      "learning_rate": 9.983108108108109e-05,
      "loss": 1.277,
      "step": 695
    },
    {
      "epoch": 1.6236496350364964,
      "grad_norm": 2.8510799407958984,
      "learning_rate": 9.966216216216217e-05,
      "loss": 0.9932,
      "step": 696
    },
    {
      "epoch": 1.625985401459854,
      "grad_norm": 5.579512596130371,
      "learning_rate": 9.949324324324325e-05,
      "loss": 0.2497,
      "step": 697
    },
    {
      "epoch": 1.6283211678832117,
      "grad_norm": 1.9686282873153687,
      "learning_rate": 9.932432432432433e-05,
      "loss": 0.992,
      "step": 698
    },
    {
      "epoch": 1.6306569343065693,
      "grad_norm": 2.872020721435547,
      "learning_rate": 9.915540540540541e-05,
      "loss": 0.6483,
      "step": 699
    },
    {
      "epoch": 1.632992700729927,
      "grad_norm": 5.737732410430908,
      "learning_rate": 9.89864864864865e-05,
      "loss": 0.2836,
      "step": 700
    },
    {
      "epoch": 1.6353284671532848,
      "grad_norm": 4.171028137207031,
      "learning_rate": 9.881756756756757e-05,
      "loss": 1.2283,
      "step": 701
    },
    {
      "epoch": 1.6376642335766425,
      "grad_norm": 2.9083030223846436,
      "learning_rate": 9.864864864864865e-05,
      "loss": 0.1488,
      "step": 702
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 3.5066449642181396,
      "learning_rate": 9.847972972972973e-05,
      "loss": 0.7527,
      "step": 703
    },
    {
      "epoch": 1.6423357664233578,
      "grad_norm": 3.1500797271728516,
      "learning_rate": 9.831081081081081e-05,
      "loss": 0.8188,
      "step": 704
    },
    {
      "epoch": 1.6446715328467154,
      "grad_norm": 2.7323265075683594,
      "learning_rate": 9.81418918918919e-05,
      "loss": 0.3601,
      "step": 705
    },
    {
      "epoch": 1.647007299270073,
      "grad_norm": 1.902578592300415,
      "learning_rate": 9.797297297297297e-05,
      "loss": 0.6702,
      "step": 706
    },
    {
      "epoch": 1.6493430656934307,
      "grad_norm": 2.2383873462677,
      "learning_rate": 9.780405405405407e-05,
      "loss": 0.5022,
      "step": 707
    },
    {
      "epoch": 1.6516788321167883,
      "grad_norm": 2.505258560180664,
      "learning_rate": 9.763513513513513e-05,
      "loss": 1.2823,
      "step": 708
    },
    {
      "epoch": 1.654014598540146,
      "grad_norm": 2.44865083694458,
      "learning_rate": 9.746621621621623e-05,
      "loss": 0.7538,
      "step": 709
    },
    {
      "epoch": 1.6563503649635036,
      "grad_norm": 5.517407417297363,
      "learning_rate": 9.729729729729731e-05,
      "loss": 1.1788,
      "step": 710
    },
    {
      "epoch": 1.6586861313868613,
      "grad_norm": 1.441866397857666,
      "learning_rate": 9.712837837837838e-05,
      "loss": 0.2791,
      "step": 711
    },
    {
      "epoch": 1.661021897810219,
      "grad_norm": 2.6683249473571777,
      "learning_rate": 9.695945945945947e-05,
      "loss": 0.6101,
      "step": 712
    },
    {
      "epoch": 1.6633576642335766,
      "grad_norm": 7.595923900604248,
      "learning_rate": 9.679054054054054e-05,
      "loss": 1.0055,
      "step": 713
    },
    {
      "epoch": 1.6656934306569342,
      "grad_norm": 14.108657836914062,
      "learning_rate": 9.662162162162163e-05,
      "loss": 0.6771,
      "step": 714
    },
    {
      "epoch": 1.6680291970802918,
      "grad_norm": 3.310673236846924,
      "learning_rate": 9.64527027027027e-05,
      "loss": 1.0714,
      "step": 715
    },
    {
      "epoch": 1.6703649635036495,
      "grad_norm": 1.6417088508605957,
      "learning_rate": 9.628378378378379e-05,
      "loss": 0.7898,
      "step": 716
    },
    {
      "epoch": 1.6727007299270071,
      "grad_norm": 2.7925374507904053,
      "learning_rate": 9.611486486486487e-05,
      "loss": 0.7355,
      "step": 717
    },
    {
      "epoch": 1.675036496350365,
      "grad_norm": 2.168229341506958,
      "learning_rate": 9.594594594594595e-05,
      "loss": 1.0331,
      "step": 718
    },
    {
      "epoch": 1.6773722627737226,
      "grad_norm": 3.453239679336548,
      "learning_rate": 9.577702702702703e-05,
      "loss": 1.3235,
      "step": 719
    },
    {
      "epoch": 1.6797080291970803,
      "grad_norm": 2.2075698375701904,
      "learning_rate": 9.56081081081081e-05,
      "loss": 0.8517,
      "step": 720
    },
    {
      "epoch": 1.682043795620438,
      "grad_norm": 3.4590375423431396,
      "learning_rate": 9.54391891891892e-05,
      "loss": 0.3231,
      "step": 721
    },
    {
      "epoch": 1.6843795620437956,
      "grad_norm": 2.078613519668579,
      "learning_rate": 9.527027027027028e-05,
      "loss": 0.3392,
      "step": 722
    },
    {
      "epoch": 1.6867153284671534,
      "grad_norm": 1.701611876487732,
      "learning_rate": 9.510135135135136e-05,
      "loss": 0.7119,
      "step": 723
    },
    {
      "epoch": 1.689051094890511,
      "grad_norm": 5.056382656097412,
      "learning_rate": 9.493243243243244e-05,
      "loss": 0.5341,
      "step": 724
    },
    {
      "epoch": 1.6913868613138687,
      "grad_norm": 2.5468578338623047,
      "learning_rate": 9.476351351351352e-05,
      "loss": 0.935,
      "step": 725
    },
    {
      "epoch": 1.6937226277372264,
      "grad_norm": 2.0083796977996826,
      "learning_rate": 9.45945945945946e-05,
      "loss": 0.4603,
      "step": 726
    },
    {
      "epoch": 1.696058394160584,
      "grad_norm": 1.362557291984558,
      "learning_rate": 9.442567567567568e-05,
      "loss": 0.2831,
      "step": 727
    },
    {
      "epoch": 1.6983941605839417,
      "grad_norm": 2.5650722980499268,
      "learning_rate": 9.425675675675676e-05,
      "loss": 0.8503,
      "step": 728
    },
    {
      "epoch": 1.7007299270072993,
      "grad_norm": 6.365414619445801,
      "learning_rate": 9.408783783783784e-05,
      "loss": 1.1766,
      "step": 729
    },
    {
      "epoch": 1.703065693430657,
      "grad_norm": 2.5979502201080322,
      "learning_rate": 9.391891891891892e-05,
      "loss": 0.281,
      "step": 730
    },
    {
      "epoch": 1.7054014598540146,
      "grad_norm": 2.478529930114746,
      "learning_rate": 9.375e-05,
      "loss": 0.4345,
      "step": 731
    },
    {
      "epoch": 1.7077372262773722,
      "grad_norm": 3.5869412422180176,
      "learning_rate": 9.35810810810811e-05,
      "loss": 0.3344,
      "step": 732
    },
    {
      "epoch": 1.7100729927007299,
      "grad_norm": 2.4104716777801514,
      "learning_rate": 9.341216216216216e-05,
      "loss": 1.0117,
      "step": 733
    },
    {
      "epoch": 1.7124087591240875,
      "grad_norm": 4.11953067779541,
      "learning_rate": 9.324324324324324e-05,
      "loss": 0.3132,
      "step": 734
    },
    {
      "epoch": 1.7147445255474452,
      "grad_norm": 4.843230724334717,
      "learning_rate": 9.307432432432432e-05,
      "loss": 0.2211,
      "step": 735
    },
    {
      "epoch": 1.7170802919708028,
      "grad_norm": 6.930241584777832,
      "learning_rate": 9.29054054054054e-05,
      "loss": 0.4285,
      "step": 736
    },
    {
      "epoch": 1.7194160583941605,
      "grad_norm": 6.2592315673828125,
      "learning_rate": 9.27364864864865e-05,
      "loss": 0.513,
      "step": 737
    },
    {
      "epoch": 1.721751824817518,
      "grad_norm": 3.9329638481140137,
      "learning_rate": 9.256756756756757e-05,
      "loss": 1.4519,
      "step": 738
    },
    {
      "epoch": 1.724087591240876,
      "grad_norm": 1.6861193180084229,
      "learning_rate": 9.239864864864866e-05,
      "loss": 0.209,
      "step": 739
    },
    {
      "epoch": 1.7264233576642336,
      "grad_norm": 3.750836133956909,
      "learning_rate": 9.222972972972973e-05,
      "loss": 0.8281,
      "step": 740
    },
    {
      "epoch": 1.7287591240875912,
      "grad_norm": 3.5062170028686523,
      "learning_rate": 9.206081081081082e-05,
      "loss": 0.4327,
      "step": 741
    },
    {
      "epoch": 1.731094890510949,
      "grad_norm": 3.58709454536438,
      "learning_rate": 9.18918918918919e-05,
      "loss": 0.2751,
      "step": 742
    },
    {
      "epoch": 1.7334306569343065,
      "grad_norm": 7.028427600860596,
      "learning_rate": 9.172297297297297e-05,
      "loss": 0.465,
      "step": 743
    },
    {
      "epoch": 1.7357664233576642,
      "grad_norm": 5.250696182250977,
      "learning_rate": 9.155405405405406e-05,
      "loss": 0.4702,
      "step": 744
    },
    {
      "epoch": 1.738102189781022,
      "grad_norm": 2.977492332458496,
      "learning_rate": 9.138513513513513e-05,
      "loss": 0.8418,
      "step": 745
    },
    {
      "epoch": 1.7404379562043797,
      "grad_norm": 3.9193289279937744,
      "learning_rate": 9.121621621621623e-05,
      "loss": 0.5403,
      "step": 746
    },
    {
      "epoch": 1.7427737226277373,
      "grad_norm": 1.6734037399291992,
      "learning_rate": 9.10472972972973e-05,
      "loss": 0.5783,
      "step": 747
    },
    {
      "epoch": 1.745109489051095,
      "grad_norm": 8.282578468322754,
      "learning_rate": 9.087837837837839e-05,
      "loss": 0.6231,
      "step": 748
    },
    {
      "epoch": 1.7474452554744526,
      "grad_norm": 1.587561011314392,
      "learning_rate": 9.070945945945947e-05,
      "loss": 0.1414,
      "step": 749
    },
    {
      "epoch": 1.7497810218978103,
      "grad_norm": 5.174975395202637,
      "learning_rate": 9.054054054054055e-05,
      "loss": 0.3855,
      "step": 750
    },
    {
      "epoch": 1.752116788321168,
      "grad_norm": 2.5676677227020264,
      "learning_rate": 9.037162162162163e-05,
      "loss": 0.7305,
      "step": 751
    },
    {
      "epoch": 1.7544525547445255,
      "grad_norm": 3.003197431564331,
      "learning_rate": 9.02027027027027e-05,
      "loss": 0.7212,
      "step": 752
    },
    {
      "epoch": 1.7567883211678832,
      "grad_norm": 3.200751543045044,
      "learning_rate": 9.003378378378379e-05,
      "loss": 0.6456,
      "step": 753
    },
    {
      "epoch": 1.7591240875912408,
      "grad_norm": 2.4484567642211914,
      "learning_rate": 8.986486486486487e-05,
      "loss": 0.465,
      "step": 754
    },
    {
      "epoch": 1.7614598540145985,
      "grad_norm": 3.4464855194091797,
      "learning_rate": 8.969594594594595e-05,
      "loss": 0.3142,
      "step": 755
    },
    {
      "epoch": 1.7637956204379561,
      "grad_norm": 1.9477895498275757,
      "learning_rate": 8.952702702702703e-05,
      "loss": 0.4184,
      "step": 756
    },
    {
      "epoch": 1.7661313868613138,
      "grad_norm": 2.8252999782562256,
      "learning_rate": 8.935810810810811e-05,
      "loss": 0.3806,
      "step": 757
    },
    {
      "epoch": 1.7684671532846714,
      "grad_norm": 5.112573623657227,
      "learning_rate": 8.918918918918919e-05,
      "loss": 0.7196,
      "step": 758
    },
    {
      "epoch": 1.770802919708029,
      "grad_norm": 4.71992826461792,
      "learning_rate": 8.902027027027027e-05,
      "loss": 0.396,
      "step": 759
    },
    {
      "epoch": 1.7731386861313867,
      "grad_norm": 5.004505634307861,
      "learning_rate": 8.885135135135135e-05,
      "loss": 0.2757,
      "step": 760
    },
    {
      "epoch": 1.7754744525547446,
      "grad_norm": 4.534180641174316,
      "learning_rate": 8.868243243243243e-05,
      "loss": 0.4166,
      "step": 761
    },
    {
      "epoch": 1.7778102189781022,
      "grad_norm": 2.332465887069702,
      "learning_rate": 8.851351351351352e-05,
      "loss": 1.0765,
      "step": 762
    },
    {
      "epoch": 1.7801459854014599,
      "grad_norm": 4.7417802810668945,
      "learning_rate": 8.83445945945946e-05,
      "loss": 0.7679,
      "step": 763
    },
    {
      "epoch": 1.7824817518248175,
      "grad_norm": 3.239825963973999,
      "learning_rate": 8.817567567567569e-05,
      "loss": 1.1928,
      "step": 764
    },
    {
      "epoch": 1.7848175182481751,
      "grad_norm": 3.1138038635253906,
      "learning_rate": 8.800675675675676e-05,
      "loss": 0.9498,
      "step": 765
    },
    {
      "epoch": 1.787153284671533,
      "grad_norm": 5.0409932136535645,
      "learning_rate": 8.783783783783784e-05,
      "loss": 0.2974,
      "step": 766
    },
    {
      "epoch": 1.7894890510948906,
      "grad_norm": 2.6781959533691406,
      "learning_rate": 8.766891891891892e-05,
      "loss": 0.9125,
      "step": 767
    },
    {
      "epoch": 1.7918248175182483,
      "grad_norm": 1.8235749006271362,
      "learning_rate": 8.75e-05,
      "loss": 0.1015,
      "step": 768
    },
    {
      "epoch": 1.794160583941606,
      "grad_norm": 2.0344767570495605,
      "learning_rate": 8.73310810810811e-05,
      "loss": 0.3363,
      "step": 769
    },
    {
      "epoch": 1.7964963503649636,
      "grad_norm": 3.758363962173462,
      "learning_rate": 8.716216216216216e-05,
      "loss": 0.5932,
      "step": 770
    },
    {
      "epoch": 1.7988321167883212,
      "grad_norm": 4.221787452697754,
      "learning_rate": 8.699324324324325e-05,
      "loss": 0.6804,
      "step": 771
    },
    {
      "epoch": 1.8011678832116789,
      "grad_norm": 6.333806037902832,
      "learning_rate": 8.682432432432432e-05,
      "loss": 0.578,
      "step": 772
    },
    {
      "epoch": 1.8035036496350365,
      "grad_norm": 3.2403383255004883,
      "learning_rate": 8.665540540540542e-05,
      "loss": 0.6077,
      "step": 773
    },
    {
      "epoch": 1.8058394160583942,
      "grad_norm": 12.982953071594238,
      "learning_rate": 8.64864864864865e-05,
      "loss": 0.6516,
      "step": 774
    },
    {
      "epoch": 1.8081751824817518,
      "grad_norm": 2.015773296356201,
      "learning_rate": 8.631756756756756e-05,
      "loss": 0.6251,
      "step": 775
    },
    {
      "epoch": 1.8105109489051094,
      "grad_norm": 3.5190584659576416,
      "learning_rate": 8.614864864864866e-05,
      "loss": 0.9263,
      "step": 776
    },
    {
      "epoch": 1.812846715328467,
      "grad_norm": 4.728823184967041,
      "learning_rate": 8.597972972972972e-05,
      "loss": 0.7671,
      "step": 777
    },
    {
      "epoch": 1.8151824817518247,
      "grad_norm": 4.472836017608643,
      "learning_rate": 8.581081081081082e-05,
      "loss": 1.2747,
      "step": 778
    },
    {
      "epoch": 1.8175182481751824,
      "grad_norm": 2.658233880996704,
      "learning_rate": 8.56418918918919e-05,
      "loss": 0.4506,
      "step": 779
    },
    {
      "epoch": 1.81985401459854,
      "grad_norm": 4.2554779052734375,
      "learning_rate": 8.547297297297298e-05,
      "loss": 1.0214,
      "step": 780
    },
    {
      "epoch": 1.8221897810218977,
      "grad_norm": 4.15176248550415,
      "learning_rate": 8.530405405405406e-05,
      "loss": 0.8279,
      "step": 781
    },
    {
      "epoch": 1.8245255474452555,
      "grad_norm": 3.4354467391967773,
      "learning_rate": 8.513513513513514e-05,
      "loss": 0.9302,
      "step": 782
    },
    {
      "epoch": 1.8268613138686132,
      "grad_norm": 3.107985258102417,
      "learning_rate": 8.496621621621622e-05,
      "loss": 0.3242,
      "step": 783
    },
    {
      "epoch": 1.8291970802919708,
      "grad_norm": 4.1693291664123535,
      "learning_rate": 8.47972972972973e-05,
      "loss": 0.6932,
      "step": 784
    },
    {
      "epoch": 1.8315328467153285,
      "grad_norm": 6.266641139984131,
      "learning_rate": 8.462837837837838e-05,
      "loss": 0.3595,
      "step": 785
    },
    {
      "epoch": 1.833868613138686,
      "grad_norm": 4.02304220199585,
      "learning_rate": 8.445945945945946e-05,
      "loss": 1.4859,
      "step": 786
    },
    {
      "epoch": 1.8362043795620437,
      "grad_norm": 2.3799893856048584,
      "learning_rate": 8.429054054054054e-05,
      "loss": 0.6774,
      "step": 787
    },
    {
      "epoch": 1.8385401459854016,
      "grad_norm": 4.235531806945801,
      "learning_rate": 8.412162162162163e-05,
      "loss": 1.0059,
      "step": 788
    },
    {
      "epoch": 1.8408759124087593,
      "grad_norm": 2.696357011795044,
      "learning_rate": 8.39527027027027e-05,
      "loss": 0.6595,
      "step": 789
    },
    {
      "epoch": 1.843211678832117,
      "grad_norm": 7.440539360046387,
      "learning_rate": 8.378378378378379e-05,
      "loss": 0.5544,
      "step": 790
    },
    {
      "epoch": 1.8455474452554745,
      "grad_norm": 11.355242729187012,
      "learning_rate": 8.361486486486487e-05,
      "loss": 0.8516,
      "step": 791
    },
    {
      "epoch": 1.8478832116788322,
      "grad_norm": 3.2896621227264404,
      "learning_rate": 8.344594594594595e-05,
      "loss": 0.292,
      "step": 792
    },
    {
      "epoch": 1.8502189781021898,
      "grad_norm": 3.127014398574829,
      "learning_rate": 8.327702702702703e-05,
      "loss": 1.2652,
      "step": 793
    },
    {
      "epoch": 1.8525547445255475,
      "grad_norm": 7.15187406539917,
      "learning_rate": 8.310810810810811e-05,
      "loss": 0.3176,
      "step": 794
    },
    {
      "epoch": 1.8548905109489051,
      "grad_norm": 6.066694736480713,
      "learning_rate": 8.293918918918919e-05,
      "loss": 0.9639,
      "step": 795
    },
    {
      "epoch": 1.8572262773722628,
      "grad_norm": 2.843838691711426,
      "learning_rate": 8.277027027027028e-05,
      "loss": 0.2205,
      "step": 796
    },
    {
      "epoch": 1.8595620437956204,
      "grad_norm": 1.736275553703308,
      "learning_rate": 8.260135135135135e-05,
      "loss": 0.8952,
      "step": 797
    },
    {
      "epoch": 1.861897810218978,
      "grad_norm": 7.273983001708984,
      "learning_rate": 8.243243243243243e-05,
      "loss": 0.4207,
      "step": 798
    },
    {
      "epoch": 1.8642335766423357,
      "grad_norm": 2.0139927864074707,
      "learning_rate": 8.226351351351351e-05,
      "loss": 0.6092,
      "step": 799
    },
    {
      "epoch": 1.8665693430656933,
      "grad_norm": 2.3850505352020264,
      "learning_rate": 8.209459459459459e-05,
      "loss": 0.2753,
      "step": 800
    },
    {
      "epoch": 1.868905109489051,
      "grad_norm": 2.4123432636260986,
      "learning_rate": 8.192567567567569e-05,
      "loss": 0.4341,
      "step": 801
    },
    {
      "epoch": 1.8712408759124086,
      "grad_norm": 7.713757038116455,
      "learning_rate": 8.175675675675675e-05,
      "loss": 0.7935,
      "step": 802
    },
    {
      "epoch": 1.8735766423357663,
      "grad_norm": 5.157093524932861,
      "learning_rate": 8.158783783783785e-05,
      "loss": 0.2105,
      "step": 803
    },
    {
      "epoch": 1.8759124087591241,
      "grad_norm": 3.288761854171753,
      "learning_rate": 8.141891891891892e-05,
      "loss": 0.2541,
      "step": 804
    },
    {
      "epoch": 1.8782481751824818,
      "grad_norm": 7.8985514640808105,
      "learning_rate": 8.125000000000001e-05,
      "loss": 0.3548,
      "step": 805
    },
    {
      "epoch": 1.8805839416058394,
      "grad_norm": 4.909501075744629,
      "learning_rate": 8.108108108108109e-05,
      "loss": 0.6577,
      "step": 806
    },
    {
      "epoch": 1.882919708029197,
      "grad_norm": 2.541043281555176,
      "learning_rate": 8.091216216216216e-05,
      "loss": 0.6589,
      "step": 807
    },
    {
      "epoch": 1.8852554744525547,
      "grad_norm": 2.261115312576294,
      "learning_rate": 8.074324324324325e-05,
      "loss": 0.8842,
      "step": 808
    },
    {
      "epoch": 1.8875912408759126,
      "grad_norm": 2.0874195098876953,
      "learning_rate": 8.057432432432432e-05,
      "loss": 0.5469,
      "step": 809
    },
    {
      "epoch": 1.8899270072992702,
      "grad_norm": 1.4215418100357056,
      "learning_rate": 8.040540540540541e-05,
      "loss": 0.3609,
      "step": 810
    },
    {
      "epoch": 1.8922627737226279,
      "grad_norm": 5.258823394775391,
      "learning_rate": 8.02364864864865e-05,
      "loss": 0.4902,
      "step": 811
    },
    {
      "epoch": 1.8945985401459855,
      "grad_norm": 3.4092185497283936,
      "learning_rate": 8.006756756756757e-05,
      "loss": 0.3433,
      "step": 812
    },
    {
      "epoch": 1.8969343065693431,
      "grad_norm": 2.62457275390625,
      "learning_rate": 7.989864864864865e-05,
      "loss": 0.5858,
      "step": 813
    },
    {
      "epoch": 1.8992700729927008,
      "grad_norm": 2.505448818206787,
      "learning_rate": 7.972972972972974e-05,
      "loss": 1.0782,
      "step": 814
    },
    {
      "epoch": 1.9016058394160584,
      "grad_norm": 2.836024284362793,
      "learning_rate": 7.956081081081082e-05,
      "loss": 1.5534,
      "step": 815
    },
    {
      "epoch": 1.903941605839416,
      "grad_norm": 2.043119192123413,
      "learning_rate": 7.93918918918919e-05,
      "loss": 2.3583,
      "step": 816
    },
    {
      "epoch": 1.9062773722627737,
      "grad_norm": 1.9111624956130981,
      "learning_rate": 7.922297297297298e-05,
      "loss": 0.5435,
      "step": 817
    },
    {
      "epoch": 1.9086131386861314,
      "grad_norm": 4.4624857902526855,
      "learning_rate": 7.905405405405406e-05,
      "loss": 0.8333,
      "step": 818
    },
    {
      "epoch": 1.910948905109489,
      "grad_norm": 3.667557716369629,
      "learning_rate": 7.888513513513514e-05,
      "loss": 0.9762,
      "step": 819
    },
    {
      "epoch": 1.9132846715328466,
      "grad_norm": 10.088139533996582,
      "learning_rate": 7.871621621621622e-05,
      "loss": 0.715,
      "step": 820
    },
    {
      "epoch": 1.9156204379562043,
      "grad_norm": 3.0816493034362793,
      "learning_rate": 7.85472972972973e-05,
      "loss": 0.9938,
      "step": 821
    },
    {
      "epoch": 1.917956204379562,
      "grad_norm": 3.5934062004089355,
      "learning_rate": 7.837837837837838e-05,
      "loss": 1.4386,
      "step": 822
    },
    {
      "epoch": 1.9202919708029196,
      "grad_norm": 4.6205267906188965,
      "learning_rate": 7.820945945945946e-05,
      "loss": 0.3765,
      "step": 823
    },
    {
      "epoch": 1.9226277372262772,
      "grad_norm": 2.2667362689971924,
      "learning_rate": 7.804054054054054e-05,
      "loss": 0.2982,
      "step": 824
    },
    {
      "epoch": 1.924963503649635,
      "grad_norm": 1.1307059526443481,
      "learning_rate": 7.787162162162162e-05,
      "loss": 0.1918,
      "step": 825
    },
    {
      "epoch": 1.9272992700729927,
      "grad_norm": 5.640353202819824,
      "learning_rate": 7.77027027027027e-05,
      "loss": 1.4945,
      "step": 826
    },
    {
      "epoch": 1.9296350364963504,
      "grad_norm": 12.40133285522461,
      "learning_rate": 7.753378378378378e-05,
      "loss": 0.8055,
      "step": 827
    },
    {
      "epoch": 1.931970802919708,
      "grad_norm": 2.2629895210266113,
      "learning_rate": 7.736486486486488e-05,
      "loss": 0.237,
      "step": 828
    },
    {
      "epoch": 1.9343065693430657,
      "grad_norm": 4.996242046356201,
      "learning_rate": 7.719594594594595e-05,
      "loss": 0.6487,
      "step": 829
    },
    {
      "epoch": 1.9366423357664233,
      "grad_norm": 4.478871822357178,
      "learning_rate": 7.702702702702703e-05,
      "loss": 1.1309,
      "step": 830
    },
    {
      "epoch": 1.9389781021897812,
      "grad_norm": 2.1830949783325195,
      "learning_rate": 7.68581081081081e-05,
      "loss": 0.8632,
      "step": 831
    },
    {
      "epoch": 1.9413138686131388,
      "grad_norm": 3.4052014350891113,
      "learning_rate": 7.668918918918919e-05,
      "loss": 0.7989,
      "step": 832
    },
    {
      "epoch": 1.9436496350364965,
      "grad_norm": 12.006397247314453,
      "learning_rate": 7.652027027027028e-05,
      "loss": 0.1781,
      "step": 833
    },
    {
      "epoch": 1.945985401459854,
      "grad_norm": 1.563804030418396,
      "learning_rate": 7.635135135135135e-05,
      "loss": 0.3982,
      "step": 834
    },
    {
      "epoch": 1.9483211678832117,
      "grad_norm": 3.225255012512207,
      "learning_rate": 7.618243243243244e-05,
      "loss": 1.1376,
      "step": 835
    },
    {
      "epoch": 1.9506569343065694,
      "grad_norm": 2.4521355628967285,
      "learning_rate": 7.601351351351351e-05,
      "loss": 0.3617,
      "step": 836
    },
    {
      "epoch": 1.952992700729927,
      "grad_norm": 7.1383490562438965,
      "learning_rate": 7.58445945945946e-05,
      "loss": 0.4714,
      "step": 837
    },
    {
      "epoch": 1.9553284671532847,
      "grad_norm": 2.6958816051483154,
      "learning_rate": 7.567567567567568e-05,
      "loss": 0.6991,
      "step": 838
    },
    {
      "epoch": 1.9576642335766423,
      "grad_norm": 5.117405414581299,
      "learning_rate": 7.550675675675675e-05,
      "loss": 0.8458,
      "step": 839
    },
    {
      "epoch": 1.96,
      "grad_norm": 2.4842875003814697,
      "learning_rate": 7.533783783783785e-05,
      "loss": 1.3692,
      "step": 840
    },
    {
      "epoch": 1.9623357664233576,
      "grad_norm": 2.1540510654449463,
      "learning_rate": 7.516891891891891e-05,
      "loss": 0.4546,
      "step": 841
    },
    {
      "epoch": 1.9646715328467153,
      "grad_norm": 3.2405524253845215,
      "learning_rate": 7.500000000000001e-05,
      "loss": 2.0918,
      "step": 842
    },
    {
      "epoch": 1.967007299270073,
      "grad_norm": 5.433254241943359,
      "learning_rate": 7.483108108108109e-05,
      "loss": 0.5251,
      "step": 843
    },
    {
      "epoch": 1.9693430656934305,
      "grad_norm": 1.8967519998550415,
      "learning_rate": 7.466216216216217e-05,
      "loss": 0.5189,
      "step": 844
    },
    {
      "epoch": 1.9716788321167882,
      "grad_norm": 2.744269371032715,
      "learning_rate": 7.449324324324325e-05,
      "loss": 1.42,
      "step": 845
    },
    {
      "epoch": 1.9740145985401458,
      "grad_norm": 6.745337009429932,
      "learning_rate": 7.432432432432433e-05,
      "loss": 1.2907,
      "step": 846
    },
    {
      "epoch": 1.9763503649635037,
      "grad_norm": 4.138030529022217,
      "learning_rate": 7.415540540540541e-05,
      "loss": 0.7531,
      "step": 847
    },
    {
      "epoch": 1.9786861313868613,
      "grad_norm": 3.4601454734802246,
      "learning_rate": 7.398648648648649e-05,
      "loss": 0.3898,
      "step": 848
    },
    {
      "epoch": 1.981021897810219,
      "grad_norm": 2.8015682697296143,
      "learning_rate": 7.381756756756757e-05,
      "loss": 1.0692,
      "step": 849
    },
    {
      "epoch": 1.9833576642335766,
      "grad_norm": 5.741995811462402,
      "learning_rate": 7.364864864864865e-05,
      "loss": 0.9832,
      "step": 850
    },
    {
      "epoch": 1.9856934306569343,
      "grad_norm": 6.533180236816406,
      "learning_rate": 7.347972972972973e-05,
      "loss": 0.5638,
      "step": 851
    },
    {
      "epoch": 1.9880291970802921,
      "grad_norm": 3.177546739578247,
      "learning_rate": 7.331081081081081e-05,
      "loss": 0.9893,
      "step": 852
    },
    {
      "epoch": 1.9903649635036498,
      "grad_norm": 2.5459446907043457,
      "learning_rate": 7.31418918918919e-05,
      "loss": 0.9605,
      "step": 853
    },
    {
      "epoch": 1.9927007299270074,
      "grad_norm": 6.440364360809326,
      "learning_rate": 7.297297297297297e-05,
      "loss": 0.4246,
      "step": 854
    },
    {
      "epoch": 1.995036496350365,
      "grad_norm": 10.47679328918457,
      "learning_rate": 7.280405405405406e-05,
      "loss": 0.5084,
      "step": 855
    },
    {
      "epoch": 1.9973722627737227,
      "grad_norm": 1.877692461013794,
      "learning_rate": 7.263513513513514e-05,
      "loss": 0.6952,
      "step": 856
    },
    {
      "epoch": 1.9997080291970804,
      "grad_norm": 4.358474254608154,
      "learning_rate": 7.246621621621622e-05,
      "loss": 0.551,
      "step": 857
    },
    {
      "epoch": 2.0,
      "grad_norm": 6.011026859283447,
      "learning_rate": 7.229729729729731e-05,
      "loss": 0.1451,
      "step": 858
    },
    {
      "epoch": 2.0023357664233576,
      "grad_norm": 2.549516201019287,
      "learning_rate": 7.212837837837838e-05,
      "loss": 0.563,
      "step": 859
    },
    {
      "epoch": 2.0046715328467153,
      "grad_norm": 1.59159255027771,
      "learning_rate": 7.195945945945947e-05,
      "loss": 1.2613,
      "step": 860
    },
    {
      "epoch": 2.007007299270073,
      "grad_norm": 1.9208664894104004,
      "learning_rate": 7.179054054054054e-05,
      "loss": 0.3429,
      "step": 861
    },
    {
      "epoch": 2.0093430656934306,
      "grad_norm": 6.256471633911133,
      "learning_rate": 7.162162162162162e-05,
      "loss": 0.3102,
      "step": 862
    },
    {
      "epoch": 2.011678832116788,
      "grad_norm": 2.4677910804748535,
      "learning_rate": 7.14527027027027e-05,
      "loss": 0.4009,
      "step": 863
    },
    {
      "epoch": 2.014014598540146,
      "grad_norm": 2.2900593280792236,
      "learning_rate": 7.128378378378378e-05,
      "loss": 0.7393,
      "step": 864
    },
    {
      "epoch": 2.0163503649635035,
      "grad_norm": 3.0649263858795166,
      "learning_rate": 7.111486486486488e-05,
      "loss": 0.2434,
      "step": 865
    },
    {
      "epoch": 2.018686131386861,
      "grad_norm": 1.2289831638336182,
      "learning_rate": 7.094594594594594e-05,
      "loss": 0.2774,
      "step": 866
    },
    {
      "epoch": 2.021021897810219,
      "grad_norm": 4.085710048675537,
      "learning_rate": 7.077702702702704e-05,
      "loss": 0.2137,
      "step": 867
    },
    {
      "epoch": 2.0233576642335764,
      "grad_norm": 2.90207576751709,
      "learning_rate": 7.06081081081081e-05,
      "loss": 0.4308,
      "step": 868
    },
    {
      "epoch": 2.0256934306569345,
      "grad_norm": 2.303713321685791,
      "learning_rate": 7.04391891891892e-05,
      "loss": 0.5201,
      "step": 869
    },
    {
      "epoch": 2.028029197080292,
      "grad_norm": 1.3894965648651123,
      "learning_rate": 7.027027027027028e-05,
      "loss": 0.1351,
      "step": 870
    },
    {
      "epoch": 2.03036496350365,
      "grad_norm": 1.405822515487671,
      "learning_rate": 7.010135135135135e-05,
      "loss": 0.2887,
      "step": 871
    },
    {
      "epoch": 2.0327007299270075,
      "grad_norm": 4.153357028961182,
      "learning_rate": 6.993243243243244e-05,
      "loss": 0.2461,
      "step": 872
    },
    {
      "epoch": 2.035036496350365,
      "grad_norm": 1.2694629430770874,
      "learning_rate": 6.97635135135135e-05,
      "loss": 0.3539,
      "step": 873
    },
    {
      "epoch": 2.0373722627737227,
      "grad_norm": 1.9028578996658325,
      "learning_rate": 6.95945945945946e-05,
      "loss": 0.1966,
      "step": 874
    },
    {
      "epoch": 2.0397080291970804,
      "grad_norm": 8.177751541137695,
      "learning_rate": 6.942567567567568e-05,
      "loss": 0.3536,
      "step": 875
    },
    {
      "epoch": 2.042043795620438,
      "grad_norm": 2.7347354888916016,
      "learning_rate": 6.925675675675676e-05,
      "loss": 0.5354,
      "step": 876
    },
    {
      "epoch": 2.0443795620437957,
      "grad_norm": 3.1156153678894043,
      "learning_rate": 6.908783783783784e-05,
      "loss": 0.7862,
      "step": 877
    },
    {
      "epoch": 2.0467153284671533,
      "grad_norm": 5.928127288818359,
      "learning_rate": 6.891891891891892e-05,
      "loss": 0.5697,
      "step": 878
    },
    {
      "epoch": 2.049051094890511,
      "grad_norm": 2.1385326385498047,
      "learning_rate": 6.875e-05,
      "loss": 0.2365,
      "step": 879
    },
    {
      "epoch": 2.0513868613138686,
      "grad_norm": 2.9885988235473633,
      "learning_rate": 6.858108108108108e-05,
      "loss": 0.1044,
      "step": 880
    },
    {
      "epoch": 2.0537226277372262,
      "grad_norm": 2.412435531616211,
      "learning_rate": 6.841216216216217e-05,
      "loss": 0.4425,
      "step": 881
    },
    {
      "epoch": 2.056058394160584,
      "grad_norm": 1.563101053237915,
      "learning_rate": 6.824324324324325e-05,
      "loss": 0.583,
      "step": 882
    },
    {
      "epoch": 2.0583941605839415,
      "grad_norm": 2.2908918857574463,
      "learning_rate": 6.807432432432433e-05,
      "loss": 0.8879,
      "step": 883
    },
    {
      "epoch": 2.060729927007299,
      "grad_norm": 3.0449328422546387,
      "learning_rate": 6.790540540540541e-05,
      "loss": 1.0967,
      "step": 884
    },
    {
      "epoch": 2.063065693430657,
      "grad_norm": 2.9083430767059326,
      "learning_rate": 6.773648648648649e-05,
      "loss": 0.1112,
      "step": 885
    },
    {
      "epoch": 2.0654014598540145,
      "grad_norm": 1.6447218656539917,
      "learning_rate": 6.756756756756757e-05,
      "loss": 0.0772,
      "step": 886
    },
    {
      "epoch": 2.067737226277372,
      "grad_norm": 4.0950140953063965,
      "learning_rate": 6.739864864864865e-05,
      "loss": 1.0676,
      "step": 887
    },
    {
      "epoch": 2.0700729927007298,
      "grad_norm": 3.7273943424224854,
      "learning_rate": 6.722972972972973e-05,
      "loss": 0.7016,
      "step": 888
    },
    {
      "epoch": 2.0724087591240874,
      "grad_norm": 3.77974534034729,
      "learning_rate": 6.706081081081081e-05,
      "loss": 0.2108,
      "step": 889
    },
    {
      "epoch": 2.0747445255474455,
      "grad_norm": 7.007154941558838,
      "learning_rate": 6.68918918918919e-05,
      "loss": 0.3217,
      "step": 890
    },
    {
      "epoch": 2.077080291970803,
      "grad_norm": 2.62229585647583,
      "learning_rate": 6.672297297297297e-05,
      "loss": 0.6185,
      "step": 891
    },
    {
      "epoch": 2.0794160583941608,
      "grad_norm": 1.6459064483642578,
      "learning_rate": 6.655405405405407e-05,
      "loss": 0.0188,
      "step": 892
    },
    {
      "epoch": 2.0817518248175184,
      "grad_norm": 2.493633985519409,
      "learning_rate": 6.638513513513513e-05,
      "loss": 0.0284,
      "step": 893
    },
    {
      "epoch": 2.084087591240876,
      "grad_norm": 2.9803714752197266,
      "learning_rate": 6.621621621621621e-05,
      "loss": 0.2723,
      "step": 894
    },
    {
      "epoch": 2.0864233576642337,
      "grad_norm": 1.089147686958313,
      "learning_rate": 6.604729729729731e-05,
      "loss": 0.2009,
      "step": 895
    },
    {
      "epoch": 2.0887591240875913,
      "grad_norm": 3.1411468982696533,
      "learning_rate": 6.587837837837837e-05,
      "loss": 0.3681,
      "step": 896
    },
    {
      "epoch": 2.091094890510949,
      "grad_norm": 2.6389262676239014,
      "learning_rate": 6.570945945945947e-05,
      "loss": 0.6476,
      "step": 897
    },
    {
      "epoch": 2.0934306569343066,
      "grad_norm": 4.051703929901123,
      "learning_rate": 6.554054054054054e-05,
      "loss": 0.2444,
      "step": 898
    },
    {
      "epoch": 2.0957664233576643,
      "grad_norm": 6.480287551879883,
      "learning_rate": 6.537162162162163e-05,
      "loss": 0.4898,
      "step": 899
    },
    {
      "epoch": 2.098102189781022,
      "grad_norm": 2.855970621109009,
      "learning_rate": 6.52027027027027e-05,
      "loss": 0.6433,
      "step": 900
    },
    {
      "epoch": 2.1004379562043796,
      "grad_norm": 3.599397659301758,
      "learning_rate": 6.503378378378379e-05,
      "loss": 0.5911,
      "step": 901
    },
    {
      "epoch": 2.102773722627737,
      "grad_norm": 6.221733093261719,
      "learning_rate": 6.486486486486487e-05,
      "loss": 0.4055,
      "step": 902
    },
    {
      "epoch": 2.105109489051095,
      "grad_norm": 4.838057518005371,
      "learning_rate": 6.469594594594594e-05,
      "loss": 0.9455,
      "step": 903
    },
    {
      "epoch": 2.1074452554744525,
      "grad_norm": 2.1442389488220215,
      "learning_rate": 6.452702702702703e-05,
      "loss": 0.4106,
      "step": 904
    },
    {
      "epoch": 2.10978102189781,
      "grad_norm": 2.4224789142608643,
      "learning_rate": 6.43581081081081e-05,
      "loss": 0.1287,
      "step": 905
    },
    {
      "epoch": 2.112116788321168,
      "grad_norm": 6.136740684509277,
      "learning_rate": 6.41891891891892e-05,
      "loss": 0.958,
      "step": 906
    },
    {
      "epoch": 2.1144525547445254,
      "grad_norm": 3.6891586780548096,
      "learning_rate": 6.402027027027028e-05,
      "loss": 0.2197,
      "step": 907
    },
    {
      "epoch": 2.116788321167883,
      "grad_norm": 2.5691583156585693,
      "learning_rate": 6.385135135135136e-05,
      "loss": 0.6386,
      "step": 908
    },
    {
      "epoch": 2.1191240875912407,
      "grad_norm": 4.189398288726807,
      "learning_rate": 6.368243243243244e-05,
      "loss": 0.1933,
      "step": 909
    },
    {
      "epoch": 2.1214598540145984,
      "grad_norm": 2.489356517791748,
      "learning_rate": 6.351351351351352e-05,
      "loss": 0.5945,
      "step": 910
    },
    {
      "epoch": 2.123795620437956,
      "grad_norm": 3.1765573024749756,
      "learning_rate": 6.33445945945946e-05,
      "loss": 0.2881,
      "step": 911
    },
    {
      "epoch": 2.1261313868613136,
      "grad_norm": 7.170624732971191,
      "learning_rate": 6.317567567567568e-05,
      "loss": 0.3824,
      "step": 912
    },
    {
      "epoch": 2.1284671532846717,
      "grad_norm": 2.069223165512085,
      "learning_rate": 6.300675675675676e-05,
      "loss": 0.6636,
      "step": 913
    },
    {
      "epoch": 2.1308029197080294,
      "grad_norm": 10.887247085571289,
      "learning_rate": 6.283783783783784e-05,
      "loss": 0.2373,
      "step": 914
    },
    {
      "epoch": 2.133138686131387,
      "grad_norm": 2.135774850845337,
      "learning_rate": 6.266891891891892e-05,
      "loss": 0.3848,
      "step": 915
    },
    {
      "epoch": 2.1354744525547447,
      "grad_norm": 2.0304224491119385,
      "learning_rate": 6.25e-05,
      "loss": 0.4455,
      "step": 916
    },
    {
      "epoch": 2.1378102189781023,
      "grad_norm": 6.1585612297058105,
      "learning_rate": 6.233108108108108e-05,
      "loss": 0.3266,
      "step": 917
    },
    {
      "epoch": 2.14014598540146,
      "grad_norm": 4.375021457672119,
      "learning_rate": 6.216216216216216e-05,
      "loss": 0.1923,
      "step": 918
    },
    {
      "epoch": 2.1424817518248176,
      "grad_norm": 7.245336532592773,
      "learning_rate": 6.199324324324324e-05,
      "loss": 0.6,
      "step": 919
    },
    {
      "epoch": 2.1448175182481752,
      "grad_norm": 3.7283122539520264,
      "learning_rate": 6.182432432432432e-05,
      "loss": 1.0708,
      "step": 920
    },
    {
      "epoch": 2.147153284671533,
      "grad_norm": 3.303046464920044,
      "learning_rate": 6.16554054054054e-05,
      "loss": 0.5817,
      "step": 921
    },
    {
      "epoch": 2.1494890510948905,
      "grad_norm": 2.6475954055786133,
      "learning_rate": 6.14864864864865e-05,
      "loss": 0.5143,
      "step": 922
    },
    {
      "epoch": 2.151824817518248,
      "grad_norm": 3.8911240100860596,
      "learning_rate": 6.131756756756757e-05,
      "loss": 0.5967,
      "step": 923
    },
    {
      "epoch": 2.154160583941606,
      "grad_norm": 2.3196866512298584,
      "learning_rate": 6.114864864864866e-05,
      "loss": 0.5816,
      "step": 924
    },
    {
      "epoch": 2.1564963503649635,
      "grad_norm": 4.061775207519531,
      "learning_rate": 6.097972972972973e-05,
      "loss": 0.1035,
      "step": 925
    },
    {
      "epoch": 2.158832116788321,
      "grad_norm": 3.0402848720550537,
      "learning_rate": 6.0810810810810814e-05,
      "loss": 0.5725,
      "step": 926
    },
    {
      "epoch": 2.1611678832116787,
      "grad_norm": 2.0959644317626953,
      "learning_rate": 6.06418918918919e-05,
      "loss": 1.1165,
      "step": 927
    },
    {
      "epoch": 2.1635036496350364,
      "grad_norm": 4.336485385894775,
      "learning_rate": 6.0472972972972976e-05,
      "loss": 0.5438,
      "step": 928
    },
    {
      "epoch": 2.165839416058394,
      "grad_norm": 6.514632701873779,
      "learning_rate": 6.030405405405406e-05,
      "loss": 0.2303,
      "step": 929
    },
    {
      "epoch": 2.1681751824817517,
      "grad_norm": 5.562371253967285,
      "learning_rate": 6.013513513513514e-05,
      "loss": 0.3941,
      "step": 930
    },
    {
      "epoch": 2.1705109489051093,
      "grad_norm": 4.448342323303223,
      "learning_rate": 5.996621621621622e-05,
      "loss": 0.8173,
      "step": 931
    },
    {
      "epoch": 2.172846715328467,
      "grad_norm": 2.605350971221924,
      "learning_rate": 5.9797297297297305e-05,
      "loss": 0.833,
      "step": 932
    },
    {
      "epoch": 2.1751824817518246,
      "grad_norm": 3.968754768371582,
      "learning_rate": 5.962837837837838e-05,
      "loss": 0.3858,
      "step": 933
    },
    {
      "epoch": 2.1775182481751827,
      "grad_norm": 6.209712505340576,
      "learning_rate": 5.9459459459459466e-05,
      "loss": 0.2235,
      "step": 934
    },
    {
      "epoch": 2.1798540145985403,
      "grad_norm": 9.96572208404541,
      "learning_rate": 5.929054054054054e-05,
      "loss": 0.2838,
      "step": 935
    },
    {
      "epoch": 2.182189781021898,
      "grad_norm": 3.7158493995666504,
      "learning_rate": 5.912162162162163e-05,
      "loss": 0.4165,
      "step": 936
    },
    {
      "epoch": 2.1845255474452556,
      "grad_norm": 2.120389938354492,
      "learning_rate": 5.89527027027027e-05,
      "loss": 0.498,
      "step": 937
    },
    {
      "epoch": 2.1868613138686133,
      "grad_norm": 10.089414596557617,
      "learning_rate": 5.878378378378379e-05,
      "loss": 0.5402,
      "step": 938
    },
    {
      "epoch": 2.189197080291971,
      "grad_norm": 2.7520029544830322,
      "learning_rate": 5.861486486486487e-05,
      "loss": 0.1582,
      "step": 939
    },
    {
      "epoch": 2.1915328467153286,
      "grad_norm": 5.875792503356934,
      "learning_rate": 5.8445945945945943e-05,
      "loss": 0.4843,
      "step": 940
    },
    {
      "epoch": 2.193868613138686,
      "grad_norm": 8.653717994689941,
      "learning_rate": 5.827702702702703e-05,
      "loss": 0.1988,
      "step": 941
    },
    {
      "epoch": 2.196204379562044,
      "grad_norm": 1.9530038833618164,
      "learning_rate": 5.8108108108108105e-05,
      "loss": 0.2894,
      "step": 942
    },
    {
      "epoch": 2.1985401459854015,
      "grad_norm": 3.1279473304748535,
      "learning_rate": 5.793918918918919e-05,
      "loss": 0.2906,
      "step": 943
    },
    {
      "epoch": 2.200875912408759,
      "grad_norm": 1.9943861961364746,
      "learning_rate": 5.777027027027028e-05,
      "loss": 0.2475,
      "step": 944
    },
    {
      "epoch": 2.2032116788321168,
      "grad_norm": 5.842929840087891,
      "learning_rate": 5.760135135135135e-05,
      "loss": 0.529,
      "step": 945
    },
    {
      "epoch": 2.2055474452554744,
      "grad_norm": 1.99189031124115,
      "learning_rate": 5.7432432432432434e-05,
      "loss": 1.3792,
      "step": 946
    },
    {
      "epoch": 2.207883211678832,
      "grad_norm": 8.94485855102539,
      "learning_rate": 5.7263513513513515e-05,
      "loss": 0.2305,
      "step": 947
    },
    {
      "epoch": 2.2102189781021897,
      "grad_norm": 3.874072313308716,
      "learning_rate": 5.7094594594594595e-05,
      "loss": 0.1771,
      "step": 948
    },
    {
      "epoch": 2.2125547445255473,
      "grad_norm": 2.4715089797973633,
      "learning_rate": 5.692567567567568e-05,
      "loss": 0.1512,
      "step": 949
    },
    {
      "epoch": 2.214890510948905,
      "grad_norm": 3.5894112586975098,
      "learning_rate": 5.6756756756756757e-05,
      "loss": 0.3652,
      "step": 950
    },
    {
      "epoch": 2.2172262773722626,
      "grad_norm": 2.874941349029541,
      "learning_rate": 5.6587837837837844e-05,
      "loss": 0.2402,
      "step": 951
    },
    {
      "epoch": 2.2195620437956203,
      "grad_norm": 4.118650436401367,
      "learning_rate": 5.641891891891892e-05,
      "loss": 0.3078,
      "step": 952
    },
    {
      "epoch": 2.221897810218978,
      "grad_norm": 5.554677486419678,
      "learning_rate": 5.6250000000000005e-05,
      "loss": 0.5846,
      "step": 953
    },
    {
      "epoch": 2.2242335766423356,
      "grad_norm": 1.873563289642334,
      "learning_rate": 5.6081081081081086e-05,
      "loss": 0.2948,
      "step": 954
    },
    {
      "epoch": 2.2265693430656937,
      "grad_norm": 2.636802911758423,
      "learning_rate": 5.591216216216216e-05,
      "loss": 0.2061,
      "step": 955
    },
    {
      "epoch": 2.2289051094890513,
      "grad_norm": 2.5421671867370605,
      "learning_rate": 5.574324324324325e-05,
      "loss": 0.2474,
      "step": 956
    },
    {
      "epoch": 2.231240875912409,
      "grad_norm": 2.6565401554107666,
      "learning_rate": 5.557432432432432e-05,
      "loss": 0.4153,
      "step": 957
    },
    {
      "epoch": 2.2335766423357666,
      "grad_norm": 5.427212715148926,
      "learning_rate": 5.540540540540541e-05,
      "loss": 0.2713,
      "step": 958
    },
    {
      "epoch": 2.2359124087591242,
      "grad_norm": 2.54404878616333,
      "learning_rate": 5.5236486486486496e-05,
      "loss": 2.2656,
      "step": 959
    },
    {
      "epoch": 2.238248175182482,
      "grad_norm": 2.6739587783813477,
      "learning_rate": 5.506756756756757e-05,
      "loss": 0.6539,
      "step": 960
    },
    {
      "epoch": 2.2405839416058395,
      "grad_norm": 2.285572052001953,
      "learning_rate": 5.489864864864866e-05,
      "loss": 0.3842,
      "step": 961
    },
    {
      "epoch": 2.242919708029197,
      "grad_norm": 12.742300987243652,
      "learning_rate": 5.472972972972973e-05,
      "loss": 0.6537,
      "step": 962
    },
    {
      "epoch": 2.245255474452555,
      "grad_norm": 3.7084944248199463,
      "learning_rate": 5.456081081081081e-05,
      "loss": 0.2336,
      "step": 963
    },
    {
      "epoch": 2.2475912408759124,
      "grad_norm": 2.901505947113037,
      "learning_rate": 5.43918918918919e-05,
      "loss": 0.8383,
      "step": 964
    },
    {
      "epoch": 2.24992700729927,
      "grad_norm": 2.728376626968384,
      "learning_rate": 5.422297297297297e-05,
      "loss": 0.303,
      "step": 965
    },
    {
      "epoch": 2.2522627737226277,
      "grad_norm": 3.5986340045928955,
      "learning_rate": 5.405405405405406e-05,
      "loss": 0.7564,
      "step": 966
    },
    {
      "epoch": 2.2545985401459854,
      "grad_norm": 2.327477216720581,
      "learning_rate": 5.3885135135135134e-05,
      "loss": 0.4504,
      "step": 967
    },
    {
      "epoch": 2.256934306569343,
      "grad_norm": 4.093821048736572,
      "learning_rate": 5.371621621621622e-05,
      "loss": 0.2069,
      "step": 968
    },
    {
      "epoch": 2.2592700729927007,
      "grad_norm": 0.9373311996459961,
      "learning_rate": 5.35472972972973e-05,
      "loss": 0.0309,
      "step": 969
    },
    {
      "epoch": 2.2616058394160583,
      "grad_norm": 9.600424766540527,
      "learning_rate": 5.337837837837838e-05,
      "loss": 0.2769,
      "step": 970
    },
    {
      "epoch": 2.263941605839416,
      "grad_norm": 2.1885085105895996,
      "learning_rate": 5.3209459459459463e-05,
      "loss": 0.2523,
      "step": 971
    },
    {
      "epoch": 2.2662773722627736,
      "grad_norm": 2.0438036918640137,
      "learning_rate": 5.304054054054054e-05,
      "loss": 0.2069,
      "step": 972
    },
    {
      "epoch": 2.2686131386861312,
      "grad_norm": 1.475569248199463,
      "learning_rate": 5.2871621621621625e-05,
      "loss": 0.1193,
      "step": 973
    },
    {
      "epoch": 2.270948905109489,
      "grad_norm": 2.2790653705596924,
      "learning_rate": 5.27027027027027e-05,
      "loss": 0.2575,
      "step": 974
    },
    {
      "epoch": 2.2732846715328465,
      "grad_norm": 3.8297736644744873,
      "learning_rate": 5.2533783783783786e-05,
      "loss": 0.3,
      "step": 975
    },
    {
      "epoch": 2.2756204379562046,
      "grad_norm": 2.106653928756714,
      "learning_rate": 5.2364864864864873e-05,
      "loss": 0.198,
      "step": 976
    },
    {
      "epoch": 2.277956204379562,
      "grad_norm": 1.61527681350708,
      "learning_rate": 5.219594594594595e-05,
      "loss": 0.1446,
      "step": 977
    },
    {
      "epoch": 2.28029197080292,
      "grad_norm": 3.4398958683013916,
      "learning_rate": 5.202702702702703e-05,
      "loss": 0.1185,
      "step": 978
    },
    {
      "epoch": 2.2826277372262775,
      "grad_norm": 20.917814254760742,
      "learning_rate": 5.185810810810811e-05,
      "loss": 0.232,
      "step": 979
    },
    {
      "epoch": 2.284963503649635,
      "grad_norm": 3.7770586013793945,
      "learning_rate": 5.168918918918919e-05,
      "loss": 0.2565,
      "step": 980
    },
    {
      "epoch": 2.287299270072993,
      "grad_norm": 5.305760860443115,
      "learning_rate": 5.152027027027028e-05,
      "loss": 0.2001,
      "step": 981
    },
    {
      "epoch": 2.2896350364963505,
      "grad_norm": 4.517904758453369,
      "learning_rate": 5.135135135135135e-05,
      "loss": 0.8434,
      "step": 982
    },
    {
      "epoch": 2.291970802919708,
      "grad_norm": 6.583492279052734,
      "learning_rate": 5.118243243243244e-05,
      "loss": 0.1198,
      "step": 983
    },
    {
      "epoch": 2.2943065693430658,
      "grad_norm": 4.196925640106201,
      "learning_rate": 5.101351351351351e-05,
      "loss": 0.6283,
      "step": 984
    },
    {
      "epoch": 2.2966423357664234,
      "grad_norm": 3.3608503341674805,
      "learning_rate": 5.08445945945946e-05,
      "loss": 0.2397,
      "step": 985
    },
    {
      "epoch": 2.298978102189781,
      "grad_norm": 3.297017812728882,
      "learning_rate": 5.067567567567568e-05,
      "loss": 0.4582,
      "step": 986
    },
    {
      "epoch": 2.3013138686131387,
      "grad_norm": 3.7771363258361816,
      "learning_rate": 5.0506756756756754e-05,
      "loss": 0.3274,
      "step": 987
    },
    {
      "epoch": 2.3036496350364963,
      "grad_norm": 4.473120212554932,
      "learning_rate": 5.033783783783784e-05,
      "loss": 0.6272,
      "step": 988
    },
    {
      "epoch": 2.305985401459854,
      "grad_norm": 5.265255928039551,
      "learning_rate": 5.0168918918918915e-05,
      "loss": 0.314,
      "step": 989
    },
    {
      "epoch": 2.3083211678832116,
      "grad_norm": 5.5488409996032715,
      "learning_rate": 5e-05,
      "loss": 0.4104,
      "step": 990
    },
    {
      "epoch": 2.3106569343065693,
      "grad_norm": 3.9014394283294678,
      "learning_rate": 4.983108108108108e-05,
      "loss": 0.3031,
      "step": 991
    },
    {
      "epoch": 2.312992700729927,
      "grad_norm": 3.6645891666412354,
      "learning_rate": 4.9662162162162164e-05,
      "loss": 0.5857,
      "step": 992
    },
    {
      "epoch": 2.3153284671532846,
      "grad_norm": 2.8418538570404053,
      "learning_rate": 4.949324324324325e-05,
      "loss": 0.4688,
      "step": 993
    },
    {
      "epoch": 2.317664233576642,
      "grad_norm": 4.982812881469727,
      "learning_rate": 4.9324324324324325e-05,
      "loss": 0.2944,
      "step": 994
    },
    {
      "epoch": 2.32,
      "grad_norm": 3.6300806999206543,
      "learning_rate": 4.9155405405405406e-05,
      "loss": 0.7811,
      "step": 995
    },
    {
      "epoch": 2.3223357664233575,
      "grad_norm": 6.942953109741211,
      "learning_rate": 4.8986486486486486e-05,
      "loss": 0.2932,
      "step": 996
    },
    {
      "epoch": 2.3246715328467156,
      "grad_norm": 2.8825395107269287,
      "learning_rate": 4.881756756756757e-05,
      "loss": 0.8206,
      "step": 997
    },
    {
      "epoch": 2.3270072992700728,
      "grad_norm": 6.006129741668701,
      "learning_rate": 4.8648648648648654e-05,
      "loss": 0.69,
      "step": 998
    },
    {
      "epoch": 2.329343065693431,
      "grad_norm": 13.878213882446289,
      "learning_rate": 4.8479729729729735e-05,
      "loss": 0.417,
      "step": 999
    },
    {
      "epoch": 2.3316788321167885,
      "grad_norm": 3.7131946086883545,
      "learning_rate": 4.8310810810810816e-05,
      "loss": 0.3944,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1284,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5555734575983206e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
