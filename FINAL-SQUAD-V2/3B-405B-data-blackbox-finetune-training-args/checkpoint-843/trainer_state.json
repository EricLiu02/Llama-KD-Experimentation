{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.992,
  "eval_steps": 500,
  "global_step": 843,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035555555555555557,
      "grad_norm": 1.0496021509170532,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.7189,
      "step": 1
    },
    {
      "epoch": 0.0071111111111111115,
      "grad_norm": 0.699831485748291,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.778,
      "step": 2
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.8934116959571838,
      "learning_rate": 6e-06,
      "loss": 1.7357,
      "step": 3
    },
    {
      "epoch": 0.014222222222222223,
      "grad_norm": 0.7389705777168274,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.9031,
      "step": 4
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 0.6188634634017944,
      "learning_rate": 1e-05,
      "loss": 1.8695,
      "step": 5
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.6671124696731567,
      "learning_rate": 1.2e-05,
      "loss": 1.8539,
      "step": 6
    },
    {
      "epoch": 0.024888888888888887,
      "grad_norm": 0.965324342250824,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.4835,
      "step": 7
    },
    {
      "epoch": 0.028444444444444446,
      "grad_norm": 0.9192386269569397,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.5773,
      "step": 8
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.8280025720596313,
      "learning_rate": 1.8e-05,
      "loss": 1.7221,
      "step": 9
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 0.6331899762153625,
      "learning_rate": 2e-05,
      "loss": 1.8756,
      "step": 10
    },
    {
      "epoch": 0.03911111111111111,
      "grad_norm": 0.6624985337257385,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.7064,
      "step": 11
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.710986316204071,
      "learning_rate": 2.4e-05,
      "loss": 1.6204,
      "step": 12
    },
    {
      "epoch": 0.04622222222222222,
      "grad_norm": 0.8325507044792175,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.6978,
      "step": 13
    },
    {
      "epoch": 0.049777777777777775,
      "grad_norm": 0.9269227385520935,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.1179,
      "step": 14
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.8736143112182617,
      "learning_rate": 3e-05,
      "loss": 1.6852,
      "step": 15
    },
    {
      "epoch": 0.05688888888888889,
      "grad_norm": 1.0716755390167236,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.911,
      "step": 16
    },
    {
      "epoch": 0.060444444444444446,
      "grad_norm": 0.7024404406547546,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.6988,
      "step": 17
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.651321291923523,
      "learning_rate": 3.6e-05,
      "loss": 1.7723,
      "step": 18
    },
    {
      "epoch": 0.06755555555555555,
      "grad_norm": 0.6992681622505188,
      "learning_rate": 3.8e-05,
      "loss": 1.7597,
      "step": 19
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 0.8356372714042664,
      "learning_rate": 4e-05,
      "loss": 1.6335,
      "step": 20
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.775214433670044,
      "learning_rate": 4.2e-05,
      "loss": 1.6665,
      "step": 21
    },
    {
      "epoch": 0.07822222222222222,
      "grad_norm": 0.6859831213951111,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.5595,
      "step": 22
    },
    {
      "epoch": 0.08177777777777778,
      "grad_norm": 0.8604557514190674,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.3867,
      "step": 23
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.7105897068977356,
      "learning_rate": 4.8e-05,
      "loss": 1.6644,
      "step": 24
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.5910735726356506,
      "learning_rate": 5e-05,
      "loss": 1.6435,
      "step": 25
    },
    {
      "epoch": 0.09244444444444444,
      "grad_norm": 0.6152008175849915,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 1.7192,
      "step": 26
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.6209008693695068,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 1.7143,
      "step": 27
    },
    {
      "epoch": 0.09955555555555555,
      "grad_norm": 2.035393476486206,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 1.7844,
      "step": 28
    },
    {
      "epoch": 0.10311111111111111,
      "grad_norm": 0.5755760669708252,
      "learning_rate": 5.8e-05,
      "loss": 1.5945,
      "step": 29
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.5616899132728577,
      "learning_rate": 6e-05,
      "loss": 1.7377,
      "step": 30
    },
    {
      "epoch": 0.11022222222222222,
      "grad_norm": 2.078535556793213,
      "learning_rate": 6.2e-05,
      "loss": 0.5414,
      "step": 31
    },
    {
      "epoch": 0.11377777777777778,
      "grad_norm": 0.6818261742591858,
      "learning_rate": 6.400000000000001e-05,
      "loss": 1.2583,
      "step": 32
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 2.012356758117676,
      "learning_rate": 6.6e-05,
      "loss": 1.528,
      "step": 33
    },
    {
      "epoch": 0.12088888888888889,
      "grad_norm": 0.679358184337616,
      "learning_rate": 6.800000000000001e-05,
      "loss": 1.8343,
      "step": 34
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 0.8316290378570557,
      "learning_rate": 7e-05,
      "loss": 1.1916,
      "step": 35
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.946313738822937,
      "learning_rate": 7.2e-05,
      "loss": 1.2201,
      "step": 36
    },
    {
      "epoch": 0.13155555555555556,
      "grad_norm": 0.8514885306358337,
      "learning_rate": 7.4e-05,
      "loss": 1.3534,
      "step": 37
    },
    {
      "epoch": 0.1351111111111111,
      "grad_norm": 0.6170944571495056,
      "learning_rate": 7.6e-05,
      "loss": 1.2827,
      "step": 38
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.790472149848938,
      "learning_rate": 7.800000000000001e-05,
      "loss": 1.4393,
      "step": 39
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 0.8437612056732178,
      "learning_rate": 8e-05,
      "loss": 1.3528,
      "step": 40
    },
    {
      "epoch": 0.14577777777777778,
      "grad_norm": 0.7593702077865601,
      "learning_rate": 8.2e-05,
      "loss": 1.3864,
      "step": 41
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.7103708386421204,
      "learning_rate": 8.4e-05,
      "loss": 1.399,
      "step": 42
    },
    {
      "epoch": 0.15288888888888888,
      "grad_norm": 0.9441720843315125,
      "learning_rate": 8.6e-05,
      "loss": 1.1639,
      "step": 43
    },
    {
      "epoch": 0.15644444444444444,
      "grad_norm": 0.802440881729126,
      "learning_rate": 8.800000000000001e-05,
      "loss": 1.5949,
      "step": 44
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.0494697093963623,
      "learning_rate": 9e-05,
      "loss": 1.1386,
      "step": 45
    },
    {
      "epoch": 0.16355555555555557,
      "grad_norm": 1.2716161012649536,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.6154,
      "step": 46
    },
    {
      "epoch": 0.1671111111111111,
      "grad_norm": 0.8360788226127625,
      "learning_rate": 9.4e-05,
      "loss": 1.3078,
      "step": 47
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.85890132188797,
      "learning_rate": 9.6e-05,
      "loss": 1.4567,
      "step": 48
    },
    {
      "epoch": 0.17422222222222222,
      "grad_norm": 0.9399837851524353,
      "learning_rate": 9.8e-05,
      "loss": 1.5402,
      "step": 49
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.0960140228271484,
      "learning_rate": 0.0001,
      "loss": 1.1925,
      "step": 50
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 1.0918102264404297,
      "learning_rate": 0.00010200000000000001,
      "loss": 1.1583,
      "step": 51
    },
    {
      "epoch": 0.18488888888888888,
      "grad_norm": 1.0110020637512207,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.9806,
      "step": 52
    },
    {
      "epoch": 0.18844444444444444,
      "grad_norm": 1.5649272203445435,
      "learning_rate": 0.00010600000000000002,
      "loss": 1.6915,
      "step": 53
    },
    {
      "epoch": 0.192,
      "grad_norm": 1.0496711730957031,
      "learning_rate": 0.00010800000000000001,
      "loss": 1.3562,
      "step": 54
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 1.140816330909729,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.9786,
      "step": 55
    },
    {
      "epoch": 0.1991111111111111,
      "grad_norm": 1.2262437343597412,
      "learning_rate": 0.00011200000000000001,
      "loss": 1.483,
      "step": 56
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.862920343875885,
      "learning_rate": 0.00011399999999999999,
      "loss": 1.2957,
      "step": 57
    },
    {
      "epoch": 0.20622222222222222,
      "grad_norm": 1.1915334463119507,
      "learning_rate": 0.000116,
      "loss": 1.5575,
      "step": 58
    },
    {
      "epoch": 0.20977777777777779,
      "grad_norm": 1.3480455875396729,
      "learning_rate": 0.000118,
      "loss": 0.8594,
      "step": 59
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 1.058949589729309,
      "learning_rate": 0.00012,
      "loss": 1.1435,
      "step": 60
    },
    {
      "epoch": 0.21688888888888888,
      "grad_norm": 0.8377956748008728,
      "learning_rate": 0.000122,
      "loss": 1.1198,
      "step": 61
    },
    {
      "epoch": 0.22044444444444444,
      "grad_norm": 0.7277732491493225,
      "learning_rate": 0.000124,
      "loss": 1.0566,
      "step": 62
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.7248883843421936,
      "learning_rate": 0.000126,
      "loss": 1.1656,
      "step": 63
    },
    {
      "epoch": 0.22755555555555557,
      "grad_norm": 0.8512604832649231,
      "learning_rate": 0.00012800000000000002,
      "loss": 1.0637,
      "step": 64
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 0.6806027889251709,
      "learning_rate": 0.00013000000000000002,
      "loss": 1.1492,
      "step": 65
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.8843218088150024,
      "learning_rate": 0.000132,
      "loss": 1.1306,
      "step": 66
    },
    {
      "epoch": 0.23822222222222222,
      "grad_norm": 0.7718207836151123,
      "learning_rate": 0.000134,
      "loss": 1.2706,
      "step": 67
    },
    {
      "epoch": 0.24177777777777779,
      "grad_norm": 0.6748335361480713,
      "learning_rate": 0.00013600000000000003,
      "loss": 1.302,
      "step": 68
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.5956426858901978,
      "learning_rate": 0.000138,
      "loss": 1.0723,
      "step": 69
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 0.6580986380577087,
      "learning_rate": 0.00014,
      "loss": 0.9885,
      "step": 70
    },
    {
      "epoch": 0.25244444444444447,
      "grad_norm": 1.5414533615112305,
      "learning_rate": 0.000142,
      "loss": 0.8492,
      "step": 71
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.5246608853340149,
      "learning_rate": 0.000144,
      "loss": 1.0715,
      "step": 72
    },
    {
      "epoch": 0.25955555555555554,
      "grad_norm": 0.4837822914123535,
      "learning_rate": 0.000146,
      "loss": 1.1721,
      "step": 73
    },
    {
      "epoch": 0.26311111111111113,
      "grad_norm": 0.7696661949157715,
      "learning_rate": 0.000148,
      "loss": 1.4931,
      "step": 74
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.4808294475078583,
      "learning_rate": 0.00015000000000000001,
      "loss": 1.2506,
      "step": 75
    },
    {
      "epoch": 0.2702222222222222,
      "grad_norm": 0.8885055780410767,
      "learning_rate": 0.000152,
      "loss": 1.3189,
      "step": 76
    },
    {
      "epoch": 0.2737777777777778,
      "grad_norm": 0.6306704878807068,
      "learning_rate": 0.000154,
      "loss": 1.3816,
      "step": 77
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.7764801979064941,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.7238,
      "step": 78
    },
    {
      "epoch": 0.2808888888888889,
      "grad_norm": 0.8956315517425537,
      "learning_rate": 0.00015800000000000002,
      "loss": 1.5646,
      "step": 79
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 0.8287359476089478,
      "learning_rate": 0.00016,
      "loss": 1.0724,
      "step": 80
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.9519221186637878,
      "learning_rate": 0.000162,
      "loss": 0.9803,
      "step": 81
    },
    {
      "epoch": 0.29155555555555557,
      "grad_norm": 0.628054141998291,
      "learning_rate": 0.000164,
      "loss": 1.1232,
      "step": 82
    },
    {
      "epoch": 0.2951111111111111,
      "grad_norm": 0.4854539930820465,
      "learning_rate": 0.000166,
      "loss": 1.1147,
      "step": 83
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.8282666802406311,
      "learning_rate": 0.000168,
      "loss": 1.3202,
      "step": 84
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 0.96560138463974,
      "learning_rate": 0.00017,
      "loss": 1.4498,
      "step": 85
    },
    {
      "epoch": 0.30577777777777776,
      "grad_norm": 0.5461382865905762,
      "learning_rate": 0.000172,
      "loss": 1.3838,
      "step": 86
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 1.6854358911514282,
      "learning_rate": 0.000174,
      "loss": 1.5109,
      "step": 87
    },
    {
      "epoch": 0.3128888888888889,
      "grad_norm": 0.8593328595161438,
      "learning_rate": 0.00017600000000000002,
      "loss": 1.2901,
      "step": 88
    },
    {
      "epoch": 0.3164444444444444,
      "grad_norm": 0.4510578513145447,
      "learning_rate": 0.00017800000000000002,
      "loss": 1.1522,
      "step": 89
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.2090189456939697,
      "learning_rate": 0.00018,
      "loss": 2.008,
      "step": 90
    },
    {
      "epoch": 0.32355555555555554,
      "grad_norm": 0.7407180070877075,
      "learning_rate": 0.000182,
      "loss": 1.0654,
      "step": 91
    },
    {
      "epoch": 0.32711111111111113,
      "grad_norm": 0.8816343545913696,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.1722,
      "step": 92
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.6926984786987305,
      "learning_rate": 0.00018600000000000002,
      "loss": 1.2231,
      "step": 93
    },
    {
      "epoch": 0.3342222222222222,
      "grad_norm": 3.6535544395446777,
      "learning_rate": 0.000188,
      "loss": 0.2878,
      "step": 94
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 0.6837925910949707,
      "learning_rate": 0.00019,
      "loss": 1.2099,
      "step": 95
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 1.5838969945907593,
      "learning_rate": 0.000192,
      "loss": 0.9175,
      "step": 96
    },
    {
      "epoch": 0.3448888888888889,
      "grad_norm": 0.9290364980697632,
      "learning_rate": 0.000194,
      "loss": 0.6948,
      "step": 97
    },
    {
      "epoch": 0.34844444444444445,
      "grad_norm": 1.2045925855636597,
      "learning_rate": 0.000196,
      "loss": 1.0607,
      "step": 98
    },
    {
      "epoch": 0.352,
      "grad_norm": 1.2093307971954346,
      "learning_rate": 0.00019800000000000002,
      "loss": 0.6564,
      "step": 99
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 1.350990653038025,
      "learning_rate": 0.0002,
      "loss": 1.848,
      "step": 100
    },
    {
      "epoch": 0.3591111111111111,
      "grad_norm": 0.4330463707447052,
      "learning_rate": 0.0001997308209959623,
      "loss": 0.9301,
      "step": 101
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 2.0327327251434326,
      "learning_rate": 0.00019946164199192463,
      "loss": 1.5023,
      "step": 102
    },
    {
      "epoch": 0.3662222222222222,
      "grad_norm": 0.577812910079956,
      "learning_rate": 0.00019919246298788696,
      "loss": 1.0138,
      "step": 103
    },
    {
      "epoch": 0.36977777777777776,
      "grad_norm": 0.43015825748443604,
      "learning_rate": 0.0001989232839838493,
      "loss": 0.7905,
      "step": 104
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.7396837472915649,
      "learning_rate": 0.00019865410497981159,
      "loss": 0.6367,
      "step": 105
    },
    {
      "epoch": 0.3768888888888889,
      "grad_norm": 0.5012692213058472,
      "learning_rate": 0.00019838492597577389,
      "loss": 1.348,
      "step": 106
    },
    {
      "epoch": 0.3804444444444444,
      "grad_norm": 0.6901695728302002,
      "learning_rate": 0.0001981157469717362,
      "loss": 1.7483,
      "step": 107
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.6713254451751709,
      "learning_rate": 0.00019784656796769854,
      "loss": 1.1905,
      "step": 108
    },
    {
      "epoch": 0.38755555555555554,
      "grad_norm": 0.7102264165878296,
      "learning_rate": 0.00019757738896366084,
      "loss": 1.5769,
      "step": 109
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 1.30527925491333,
      "learning_rate": 0.00019730820995962316,
      "loss": 1.8035,
      "step": 110
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.8422040343284607,
      "learning_rate": 0.00019703903095558546,
      "loss": 1.3623,
      "step": 111
    },
    {
      "epoch": 0.3982222222222222,
      "grad_norm": 0.5617761611938477,
      "learning_rate": 0.0001967698519515478,
      "loss": 1.1446,
      "step": 112
    },
    {
      "epoch": 0.4017777777777778,
      "grad_norm": 1.1235250234603882,
      "learning_rate": 0.00019650067294751011,
      "loss": 2.1151,
      "step": 113
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.9642476439476013,
      "learning_rate": 0.0001962314939434724,
      "loss": 1.3684,
      "step": 114
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 0.6631832122802734,
      "learning_rate": 0.00019596231493943474,
      "loss": 1.2366,
      "step": 115
    },
    {
      "epoch": 0.41244444444444445,
      "grad_norm": 0.5110630989074707,
      "learning_rate": 0.00019569313593539704,
      "loss": 1.2201,
      "step": 116
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.48804789781570435,
      "learning_rate": 0.00019542395693135936,
      "loss": 1.3454,
      "step": 117
    },
    {
      "epoch": 0.41955555555555557,
      "grad_norm": 1.5733678340911865,
      "learning_rate": 0.0001951547779273217,
      "loss": 1.0821,
      "step": 118
    },
    {
      "epoch": 0.4231111111111111,
      "grad_norm": 0.7834073901176453,
      "learning_rate": 0.000194885598923284,
      "loss": 1.4193,
      "step": 119
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.5930349826812744,
      "learning_rate": 0.00019461641991924632,
      "loss": 1.1611,
      "step": 120
    },
    {
      "epoch": 0.43022222222222223,
      "grad_norm": 0.43651703000068665,
      "learning_rate": 0.00019434724091520861,
      "loss": 0.9157,
      "step": 121
    },
    {
      "epoch": 0.43377777777777776,
      "grad_norm": 0.8871474862098694,
      "learning_rate": 0.00019407806191117094,
      "loss": 0.7413,
      "step": 122
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.49216988682746887,
      "learning_rate": 0.00019380888290713327,
      "loss": 1.3435,
      "step": 123
    },
    {
      "epoch": 0.4408888888888889,
      "grad_norm": 1.3751505613327026,
      "learning_rate": 0.00019353970390309557,
      "loss": 1.6574,
      "step": 124
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.5222814679145813,
      "learning_rate": 0.0001932705248990579,
      "loss": 1.008,
      "step": 125
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.5151017904281616,
      "learning_rate": 0.0001930013458950202,
      "loss": 1.2464,
      "step": 126
    },
    {
      "epoch": 0.45155555555555554,
      "grad_norm": 0.5718678832054138,
      "learning_rate": 0.0001927321668909825,
      "loss": 0.9871,
      "step": 127
    },
    {
      "epoch": 0.45511111111111113,
      "grad_norm": 1.23574697971344,
      "learning_rate": 0.00019246298788694484,
      "loss": 2.0865,
      "step": 128
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.45002615451812744,
      "learning_rate": 0.00019219380888290714,
      "loss": 1.5408,
      "step": 129
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 0.7219682335853577,
      "learning_rate": 0.00019192462987886947,
      "loss": 1.0868,
      "step": 130
    },
    {
      "epoch": 0.4657777777777778,
      "grad_norm": 0.6695947051048279,
      "learning_rate": 0.00019165545087483177,
      "loss": 1.4775,
      "step": 131
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.6133909821510315,
      "learning_rate": 0.0001913862718707941,
      "loss": 0.7883,
      "step": 132
    },
    {
      "epoch": 0.4728888888888889,
      "grad_norm": 0.5814960598945618,
      "learning_rate": 0.00019111709286675642,
      "loss": 1.2102,
      "step": 133
    },
    {
      "epoch": 0.47644444444444445,
      "grad_norm": 0.5922862887382507,
      "learning_rate": 0.00019084791386271872,
      "loss": 0.997,
      "step": 134
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.5335346460342407,
      "learning_rate": 0.00019057873485868102,
      "loss": 0.7855,
      "step": 135
    },
    {
      "epoch": 0.48355555555555557,
      "grad_norm": 0.5321141481399536,
      "learning_rate": 0.00019030955585464334,
      "loss": 1.1751,
      "step": 136
    },
    {
      "epoch": 0.4871111111111111,
      "grad_norm": 0.5106782913208008,
      "learning_rate": 0.00019004037685060567,
      "loss": 1.1227,
      "step": 137
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.5629293918609619,
      "learning_rate": 0.000189771197846568,
      "loss": 1.3005,
      "step": 138
    },
    {
      "epoch": 0.49422222222222223,
      "grad_norm": 0.378921777009964,
      "learning_rate": 0.0001895020188425303,
      "loss": 1.0884,
      "step": 139
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 0.8850380182266235,
      "learning_rate": 0.0001892328398384926,
      "loss": 1.192,
      "step": 140
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.572763204574585,
      "learning_rate": 0.00018896366083445492,
      "loss": 0.6845,
      "step": 141
    },
    {
      "epoch": 0.5048888888888889,
      "grad_norm": 0.584597647190094,
      "learning_rate": 0.00018869448183041725,
      "loss": 0.9947,
      "step": 142
    },
    {
      "epoch": 0.5084444444444445,
      "grad_norm": 0.4791162610054016,
      "learning_rate": 0.00018842530282637955,
      "loss": 1.2347,
      "step": 143
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.6760646104812622,
      "learning_rate": 0.00018815612382234187,
      "loss": 1.2606,
      "step": 144
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 0.6713061332702637,
      "learning_rate": 0.00018788694481830417,
      "loss": 1.2867,
      "step": 145
    },
    {
      "epoch": 0.5191111111111111,
      "grad_norm": 2.5703744888305664,
      "learning_rate": 0.0001876177658142665,
      "loss": 0.4341,
      "step": 146
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.7214426398277283,
      "learning_rate": 0.00018734858681022882,
      "loss": 1.1408,
      "step": 147
    },
    {
      "epoch": 0.5262222222222223,
      "grad_norm": 0.5359913110733032,
      "learning_rate": 0.00018707940780619112,
      "loss": 1.1818,
      "step": 148
    },
    {
      "epoch": 0.5297777777777778,
      "grad_norm": 0.48528075218200684,
      "learning_rate": 0.00018681022880215345,
      "loss": 1.3345,
      "step": 149
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.6618868112564087,
      "learning_rate": 0.00018654104979811575,
      "loss": 1.4868,
      "step": 150
    },
    {
      "epoch": 0.5368888888888889,
      "grad_norm": 0.5592797994613647,
      "learning_rate": 0.00018627187079407807,
      "loss": 1.5118,
      "step": 151
    },
    {
      "epoch": 0.5404444444444444,
      "grad_norm": 0.7681078314781189,
      "learning_rate": 0.0001860026917900404,
      "loss": 1.184,
      "step": 152
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.7294600605964661,
      "learning_rate": 0.0001857335127860027,
      "loss": 1.1922,
      "step": 153
    },
    {
      "epoch": 0.5475555555555556,
      "grad_norm": 0.5436679124832153,
      "learning_rate": 0.00018546433378196502,
      "loss": 1.0991,
      "step": 154
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 0.5317585468292236,
      "learning_rate": 0.00018519515477792732,
      "loss": 1.0717,
      "step": 155
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 1.3958908319473267,
      "learning_rate": 0.00018492597577388965,
      "loss": 0.9477,
      "step": 156
    },
    {
      "epoch": 0.5582222222222222,
      "grad_norm": 0.4654548764228821,
      "learning_rate": 0.00018465679676985198,
      "loss": 1.5322,
      "step": 157
    },
    {
      "epoch": 0.5617777777777778,
      "grad_norm": 1.3589075803756714,
      "learning_rate": 0.00018438761776581427,
      "loss": 1.1833,
      "step": 158
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 1.261024832725525,
      "learning_rate": 0.0001841184387617766,
      "loss": 0.9546,
      "step": 159
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 0.3779989182949066,
      "learning_rate": 0.0001838492597577389,
      "loss": 1.0497,
      "step": 160
    },
    {
      "epoch": 0.5724444444444444,
      "grad_norm": 0.5307259559631348,
      "learning_rate": 0.0001835800807537012,
      "loss": 1.1049,
      "step": 161
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.461746484041214,
      "learning_rate": 0.00018331090174966355,
      "loss": 0.9394,
      "step": 162
    },
    {
      "epoch": 0.5795555555555556,
      "grad_norm": 0.3630843460559845,
      "learning_rate": 0.00018304172274562585,
      "loss": 1.1587,
      "step": 163
    },
    {
      "epoch": 0.5831111111111111,
      "grad_norm": 1.5686595439910889,
      "learning_rate": 0.00018277254374158818,
      "loss": 1.1538,
      "step": 164
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.6291707158088684,
      "learning_rate": 0.00018250336473755048,
      "loss": 1.2383,
      "step": 165
    },
    {
      "epoch": 0.5902222222222222,
      "grad_norm": 0.7741327881813049,
      "learning_rate": 0.00018223418573351277,
      "loss": 1.0396,
      "step": 166
    },
    {
      "epoch": 0.5937777777777777,
      "grad_norm": 0.5122392177581787,
      "learning_rate": 0.00018196500672947513,
      "loss": 0.7663,
      "step": 167
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.5217849612236023,
      "learning_rate": 0.00018169582772543743,
      "loss": 1.222,
      "step": 168
    },
    {
      "epoch": 0.6008888888888889,
      "grad_norm": 0.6501277089118958,
      "learning_rate": 0.00018142664872139973,
      "loss": 0.9019,
      "step": 169
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 0.4106917679309845,
      "learning_rate": 0.00018115746971736205,
      "loss": 1.2087,
      "step": 170
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.4611952006816864,
      "learning_rate": 0.00018088829071332435,
      "loss": 0.9025,
      "step": 171
    },
    {
      "epoch": 0.6115555555555555,
      "grad_norm": 2.2620315551757812,
      "learning_rate": 0.0001806191117092867,
      "loss": 1.055,
      "step": 172
    },
    {
      "epoch": 0.6151111111111112,
      "grad_norm": 0.5724611282348633,
      "learning_rate": 0.000180349932705249,
      "loss": 1.589,
      "step": 173
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.6973462104797363,
      "learning_rate": 0.0001800807537012113,
      "loss": 1.4342,
      "step": 174
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.6673855781555176,
      "learning_rate": 0.00017981157469717363,
      "loss": 1.3324,
      "step": 175
    },
    {
      "epoch": 0.6257777777777778,
      "grad_norm": 0.9670286178588867,
      "learning_rate": 0.00017954239569313593,
      "loss": 1.1528,
      "step": 176
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.7826244235038757,
      "learning_rate": 0.00017927321668909825,
      "loss": 1.3613,
      "step": 177
    },
    {
      "epoch": 0.6328888888888888,
      "grad_norm": 0.5583764314651489,
      "learning_rate": 0.00017900403768506058,
      "loss": 1.2596,
      "step": 178
    },
    {
      "epoch": 0.6364444444444445,
      "grad_norm": 1.4546701908111572,
      "learning_rate": 0.00017873485868102288,
      "loss": 1.1234,
      "step": 179
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.514494001865387,
      "learning_rate": 0.0001784656796769852,
      "loss": 0.9313,
      "step": 180
    },
    {
      "epoch": 0.6435555555555555,
      "grad_norm": 0.9494575262069702,
      "learning_rate": 0.0001781965006729475,
      "loss": 2.3181,
      "step": 181
    },
    {
      "epoch": 0.6471111111111111,
      "grad_norm": 0.6291656494140625,
      "learning_rate": 0.00017792732166890983,
      "loss": 0.9611,
      "step": 182
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 1.0900291204452515,
      "learning_rate": 0.00017765814266487216,
      "loss": 2.2243,
      "step": 183
    },
    {
      "epoch": 0.6542222222222223,
      "grad_norm": 1.7421339750289917,
      "learning_rate": 0.00017738896366083446,
      "loss": 1.5088,
      "step": 184
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 1.1131839752197266,
      "learning_rate": 0.00017711978465679678,
      "loss": 1.243,
      "step": 185
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.9183111786842346,
      "learning_rate": 0.00017685060565275908,
      "loss": 1.671,
      "step": 186
    },
    {
      "epoch": 0.6648888888888889,
      "grad_norm": 1.0032020807266235,
      "learning_rate": 0.0001765814266487214,
      "loss": 1.1944,
      "step": 187
    },
    {
      "epoch": 0.6684444444444444,
      "grad_norm": 0.5245067477226257,
      "learning_rate": 0.00017631224764468373,
      "loss": 1.1521,
      "step": 188
    },
    {
      "epoch": 0.672,
      "grad_norm": 1.6315789222717285,
      "learning_rate": 0.00017604306864064603,
      "loss": 0.8746,
      "step": 189
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 0.7185429930686951,
      "learning_rate": 0.00017577388963660836,
      "loss": 1.2791,
      "step": 190
    },
    {
      "epoch": 0.6791111111111111,
      "grad_norm": 0.5553684234619141,
      "learning_rate": 0.00017550471063257066,
      "loss": 1.2469,
      "step": 191
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.6155862212181091,
      "learning_rate": 0.00017523553162853298,
      "loss": 0.9643,
      "step": 192
    },
    {
      "epoch": 0.6862222222222222,
      "grad_norm": 0.5407004356384277,
      "learning_rate": 0.0001749663526244953,
      "loss": 1.1121,
      "step": 193
    },
    {
      "epoch": 0.6897777777777778,
      "grad_norm": 0.8713154792785645,
      "learning_rate": 0.0001746971736204576,
      "loss": 1.3437,
      "step": 194
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.35754144191741943,
      "learning_rate": 0.0001744279946164199,
      "loss": 1.3003,
      "step": 195
    },
    {
      "epoch": 0.6968888888888889,
      "grad_norm": 0.5295566916465759,
      "learning_rate": 0.00017415881561238226,
      "loss": 1.0948,
      "step": 196
    },
    {
      "epoch": 0.7004444444444444,
      "grad_norm": 0.5680763721466064,
      "learning_rate": 0.00017388963660834456,
      "loss": 0.873,
      "step": 197
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.5312590599060059,
      "learning_rate": 0.00017362045760430689,
      "loss": 1.3737,
      "step": 198
    },
    {
      "epoch": 0.7075555555555556,
      "grad_norm": 0.4810865521430969,
      "learning_rate": 0.00017335127860026918,
      "loss": 1.034,
      "step": 199
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.6006820201873779,
      "learning_rate": 0.00017308209959623148,
      "loss": 1.4625,
      "step": 200
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.5798536539077759,
      "learning_rate": 0.00017281292059219384,
      "loss": 0.9986,
      "step": 201
    },
    {
      "epoch": 0.7182222222222222,
      "grad_norm": 0.38229236006736755,
      "learning_rate": 0.00017254374158815614,
      "loss": 0.8592,
      "step": 202
    },
    {
      "epoch": 0.7217777777777777,
      "grad_norm": 0.5694853067398071,
      "learning_rate": 0.00017227456258411844,
      "loss": 1.5698,
      "step": 203
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.5129810571670532,
      "learning_rate": 0.00017200538358008076,
      "loss": 1.137,
      "step": 204
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 0.6529664397239685,
      "learning_rate": 0.00017173620457604306,
      "loss": 1.3738,
      "step": 205
    },
    {
      "epoch": 0.7324444444444445,
      "grad_norm": 0.6725929975509644,
      "learning_rate": 0.0001714670255720054,
      "loss": 0.9657,
      "step": 206
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.5176121592521667,
      "learning_rate": 0.0001711978465679677,
      "loss": 1.3465,
      "step": 207
    },
    {
      "epoch": 0.7395555555555555,
      "grad_norm": 0.6820777654647827,
      "learning_rate": 0.00017092866756393,
      "loss": 1.3023,
      "step": 208
    },
    {
      "epoch": 0.7431111111111111,
      "grad_norm": 0.6149845123291016,
      "learning_rate": 0.00017065948855989234,
      "loss": 0.9118,
      "step": 209
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.8230158686637878,
      "learning_rate": 0.00017039030955585464,
      "loss": 0.5452,
      "step": 210
    },
    {
      "epoch": 0.7502222222222222,
      "grad_norm": 0.8835077881813049,
      "learning_rate": 0.00017012113055181696,
      "loss": 1.7679,
      "step": 211
    },
    {
      "epoch": 0.7537777777777778,
      "grad_norm": 0.8833909034729004,
      "learning_rate": 0.0001698519515477793,
      "loss": 1.1987,
      "step": 212
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.566779613494873,
      "learning_rate": 0.0001695827725437416,
      "loss": 1.2695,
      "step": 213
    },
    {
      "epoch": 0.7608888888888888,
      "grad_norm": 0.5161460638046265,
      "learning_rate": 0.00016931359353970391,
      "loss": 1.1229,
      "step": 214
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 0.6026802062988281,
      "learning_rate": 0.0001690444145356662,
      "loss": 0.8391,
      "step": 215
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.41746753454208374,
      "learning_rate": 0.00016877523553162854,
      "loss": 0.8475,
      "step": 216
    },
    {
      "epoch": 0.7715555555555556,
      "grad_norm": 0.468995064496994,
      "learning_rate": 0.00016850605652759087,
      "loss": 1.1433,
      "step": 217
    },
    {
      "epoch": 0.7751111111111111,
      "grad_norm": 0.7184010148048401,
      "learning_rate": 0.00016823687752355316,
      "loss": 1.1225,
      "step": 218
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.7040696740150452,
      "learning_rate": 0.0001679676985195155,
      "loss": 0.8035,
      "step": 219
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 0.4247884452342987,
      "learning_rate": 0.0001676985195154778,
      "loss": 1.0806,
      "step": 220
    },
    {
      "epoch": 0.7857777777777778,
      "grad_norm": 0.48878809809684753,
      "learning_rate": 0.00016742934051144012,
      "loss": 1.2708,
      "step": 221
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.3975940942764282,
      "learning_rate": 0.00016716016150740244,
      "loss": 1.1185,
      "step": 222
    },
    {
      "epoch": 0.7928888888888889,
      "grad_norm": 0.47210660576820374,
      "learning_rate": 0.00016689098250336474,
      "loss": 1.106,
      "step": 223
    },
    {
      "epoch": 0.7964444444444444,
      "grad_norm": 0.5187931060791016,
      "learning_rate": 0.00016662180349932707,
      "loss": 1.0283,
      "step": 224
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.5324670076370239,
      "learning_rate": 0.00016635262449528937,
      "loss": 1.2222,
      "step": 225
    },
    {
      "epoch": 0.8035555555555556,
      "grad_norm": 0.5222822427749634,
      "learning_rate": 0.0001660834454912517,
      "loss": 1.2133,
      "step": 226
    },
    {
      "epoch": 0.8071111111111111,
      "grad_norm": 0.46047264337539673,
      "learning_rate": 0.00016581426648721402,
      "loss": 1.1059,
      "step": 227
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.644741952419281,
      "learning_rate": 0.00016554508748317632,
      "loss": 1.3312,
      "step": 228
    },
    {
      "epoch": 0.8142222222222222,
      "grad_norm": 1.013222575187683,
      "learning_rate": 0.00016527590847913864,
      "loss": 1.5379,
      "step": 229
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 0.7131029963493347,
      "learning_rate": 0.00016500672947510094,
      "loss": 0.794,
      "step": 230
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.6251517534255981,
      "learning_rate": 0.00016473755047106327,
      "loss": 1.009,
      "step": 231
    },
    {
      "epoch": 0.8248888888888889,
      "grad_norm": 0.5318983197212219,
      "learning_rate": 0.0001644683714670256,
      "loss": 1.1121,
      "step": 232
    },
    {
      "epoch": 0.8284444444444444,
      "grad_norm": 1.289717435836792,
      "learning_rate": 0.0001641991924629879,
      "loss": 1.2509,
      "step": 233
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.3509086072444916,
      "learning_rate": 0.0001639300134589502,
      "loss": 1.0111,
      "step": 234
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 0.6093185544013977,
      "learning_rate": 0.00016366083445491252,
      "loss": 1.5503,
      "step": 235
    },
    {
      "epoch": 0.8391111111111111,
      "grad_norm": 0.3081625998020172,
      "learning_rate": 0.00016339165545087484,
      "loss": 0.9779,
      "step": 236
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.5099732875823975,
      "learning_rate": 0.00016312247644683717,
      "loss": 1.2745,
      "step": 237
    },
    {
      "epoch": 0.8462222222222222,
      "grad_norm": 0.4560704529285431,
      "learning_rate": 0.00016285329744279947,
      "loss": 1.3095,
      "step": 238
    },
    {
      "epoch": 0.8497777777777777,
      "grad_norm": 0.6672989726066589,
      "learning_rate": 0.00016258411843876177,
      "loss": 1.3676,
      "step": 239
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.9427176713943481,
      "learning_rate": 0.0001623149394347241,
      "loss": 1.8367,
      "step": 240
    },
    {
      "epoch": 0.8568888888888889,
      "grad_norm": 0.45965632796287537,
      "learning_rate": 0.00016204576043068642,
      "loss": 1.3296,
      "step": 241
    },
    {
      "epoch": 0.8604444444444445,
      "grad_norm": 0.5767579674720764,
      "learning_rate": 0.00016177658142664872,
      "loss": 0.9071,
      "step": 242
    },
    {
      "epoch": 0.864,
      "grad_norm": 1.0995452404022217,
      "learning_rate": 0.00016150740242261105,
      "loss": 1.9467,
      "step": 243
    },
    {
      "epoch": 0.8675555555555555,
      "grad_norm": 1.997581124305725,
      "learning_rate": 0.00016123822341857335,
      "loss": 0.4737,
      "step": 244
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 0.5704809427261353,
      "learning_rate": 0.00016096904441453567,
      "loss": 1.2059,
      "step": 245
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.5709977746009827,
      "learning_rate": 0.000160699865410498,
      "loss": 1.0681,
      "step": 246
    },
    {
      "epoch": 0.8782222222222222,
      "grad_norm": 0.4789314866065979,
      "learning_rate": 0.0001604306864064603,
      "loss": 1.3911,
      "step": 247
    },
    {
      "epoch": 0.8817777777777778,
      "grad_norm": 0.6251077055931091,
      "learning_rate": 0.00016016150740242262,
      "loss": 0.8446,
      "step": 248
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.6198777556419373,
      "learning_rate": 0.00015989232839838492,
      "loss": 1.2123,
      "step": 249
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.48808401823043823,
      "learning_rate": 0.00015962314939434725,
      "loss": 1.3844,
      "step": 250
    },
    {
      "epoch": 0.8924444444444445,
      "grad_norm": 0.8783051371574402,
      "learning_rate": 0.00015935397039030957,
      "loss": 1.0221,
      "step": 251
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.4624428451061249,
      "learning_rate": 0.00015908479138627187,
      "loss": 1.0763,
      "step": 252
    },
    {
      "epoch": 0.8995555555555556,
      "grad_norm": 0.5417936444282532,
      "learning_rate": 0.0001588156123822342,
      "loss": 1.3057,
      "step": 253
    },
    {
      "epoch": 0.9031111111111111,
      "grad_norm": 0.43751218914985657,
      "learning_rate": 0.0001585464333781965,
      "loss": 1.1134,
      "step": 254
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.943323016166687,
      "learning_rate": 0.00015827725437415882,
      "loss": 1.1528,
      "step": 255
    },
    {
      "epoch": 0.9102222222222223,
      "grad_norm": 0.5172732472419739,
      "learning_rate": 0.00015800807537012115,
      "loss": 0.9659,
      "step": 256
    },
    {
      "epoch": 0.9137777777777778,
      "grad_norm": 1.233431100845337,
      "learning_rate": 0.00015773889636608345,
      "loss": 1.7117,
      "step": 257
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.39350539445877075,
      "learning_rate": 0.00015746971736204578,
      "loss": 0.9438,
      "step": 258
    },
    {
      "epoch": 0.9208888888888889,
      "grad_norm": 1.268738031387329,
      "learning_rate": 0.00015720053835800807,
      "loss": 1.5171,
      "step": 259
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 0.56369948387146,
      "learning_rate": 0.0001569313593539704,
      "loss": 1.484,
      "step": 260
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.4048101007938385,
      "learning_rate": 0.00015666218034993273,
      "loss": 0.875,
      "step": 261
    },
    {
      "epoch": 0.9315555555555556,
      "grad_norm": 0.5403534173965454,
      "learning_rate": 0.00015639300134589503,
      "loss": 0.9223,
      "step": 262
    },
    {
      "epoch": 0.9351111111111111,
      "grad_norm": 1.744361400604248,
      "learning_rate": 0.00015612382234185735,
      "loss": 0.9628,
      "step": 263
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.6678670644760132,
      "learning_rate": 0.00015585464333781965,
      "loss": 1.3679,
      "step": 264
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 0.3641666769981384,
      "learning_rate": 0.00015558546433378198,
      "loss": 1.505,
      "step": 265
    },
    {
      "epoch": 0.9457777777777778,
      "grad_norm": 0.5681832432746887,
      "learning_rate": 0.0001553162853297443,
      "loss": 1.3195,
      "step": 266
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.3981543779373169,
      "learning_rate": 0.0001550471063257066,
      "loss": 1.0068,
      "step": 267
    },
    {
      "epoch": 0.9528888888888889,
      "grad_norm": 0.5515474677085876,
      "learning_rate": 0.0001547779273216689,
      "loss": 1.4179,
      "step": 268
    },
    {
      "epoch": 0.9564444444444444,
      "grad_norm": 1.3329676389694214,
      "learning_rate": 0.00015450874831763123,
      "loss": 1.3393,
      "step": 269
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.341877818107605,
      "learning_rate": 0.00015423956931359355,
      "loss": 1.1206,
      "step": 270
    },
    {
      "epoch": 0.9635555555555556,
      "grad_norm": 0.44232168793678284,
      "learning_rate": 0.00015397039030955588,
      "loss": 1.2508,
      "step": 271
    },
    {
      "epoch": 0.9671111111111111,
      "grad_norm": 0.6992319226264954,
      "learning_rate": 0.00015370121130551818,
      "loss": 1.0084,
      "step": 272
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.5592799186706543,
      "learning_rate": 0.00015343203230148048,
      "loss": 1.4097,
      "step": 273
    },
    {
      "epoch": 0.9742222222222222,
      "grad_norm": 0.5160696506500244,
      "learning_rate": 0.0001531628532974428,
      "loss": 1.3497,
      "step": 274
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.6935634016990662,
      "learning_rate": 0.00015289367429340513,
      "loss": 1.6339,
      "step": 275
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.9417396783828735,
      "learning_rate": 0.00015262449528936743,
      "loss": 1.59,
      "step": 276
    },
    {
      "epoch": 0.9848888888888889,
      "grad_norm": 0.6722342371940613,
      "learning_rate": 0.00015235531628532976,
      "loss": 1.3673,
      "step": 277
    },
    {
      "epoch": 0.9884444444444445,
      "grad_norm": 2.3245015144348145,
      "learning_rate": 0.00015208613728129205,
      "loss": 0.6895,
      "step": 278
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.4311911463737488,
      "learning_rate": 0.00015181695827725438,
      "loss": 1.4836,
      "step": 279
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 0.3776307702064514,
      "learning_rate": 0.0001515477792732167,
      "loss": 1.4433,
      "step": 280
    },
    {
      "epoch": 0.9991111111111111,
      "grad_norm": 0.35465294122695923,
      "learning_rate": 0.000151278600269179,
      "loss": 1.2047,
      "step": 281
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.6660449504852295,
      "learning_rate": 0.00015100942126514133,
      "loss": 1.3034,
      "step": 282
    },
    {
      "epoch": 1.0035555555555555,
      "grad_norm": 0.491671621799469,
      "learning_rate": 0.00015074024226110363,
      "loss": 1.1289,
      "step": 283
    },
    {
      "epoch": 1.007111111111111,
      "grad_norm": 0.6420548558235168,
      "learning_rate": 0.00015047106325706596,
      "loss": 0.8111,
      "step": 284
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.348878413438797,
      "learning_rate": 0.00015020188425302828,
      "loss": 0.8367,
      "step": 285
    },
    {
      "epoch": 1.0142222222222221,
      "grad_norm": 0.49409574270248413,
      "learning_rate": 0.00014993270524899058,
      "loss": 0.6623,
      "step": 286
    },
    {
      "epoch": 1.0177777777777777,
      "grad_norm": 1.9392786026000977,
      "learning_rate": 0.0001496635262449529,
      "loss": 0.5144,
      "step": 287
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.7135207653045654,
      "learning_rate": 0.0001493943472409152,
      "loss": 1.6439,
      "step": 288
    },
    {
      "epoch": 1.024888888888889,
      "grad_norm": 0.4018026888370514,
      "learning_rate": 0.00014912516823687753,
      "loss": 0.8314,
      "step": 289
    },
    {
      "epoch": 1.0284444444444445,
      "grad_norm": 0.4371011257171631,
      "learning_rate": 0.00014885598923283986,
      "loss": 0.8353,
      "step": 290
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.5494061708450317,
      "learning_rate": 0.00014858681022880216,
      "loss": 1.2242,
      "step": 291
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 0.5114312767982483,
      "learning_rate": 0.00014831763122476448,
      "loss": 0.9304,
      "step": 292
    },
    {
      "epoch": 1.039111111111111,
      "grad_norm": 0.33620643615722656,
      "learning_rate": 0.00014804845222072678,
      "loss": 1.0793,
      "step": 293
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.4570811986923218,
      "learning_rate": 0.00014777927321668908,
      "loss": 0.9469,
      "step": 294
    },
    {
      "epoch": 1.0462222222222222,
      "grad_norm": 0.39089396595954895,
      "learning_rate": 0.00014751009421265144,
      "loss": 1.078,
      "step": 295
    },
    {
      "epoch": 1.0497777777777777,
      "grad_norm": 0.5925427079200745,
      "learning_rate": 0.00014724091520861373,
      "loss": 1.2811,
      "step": 296
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.686424732208252,
      "learning_rate": 0.00014697173620457606,
      "loss": 1.3824,
      "step": 297
    },
    {
      "epoch": 1.056888888888889,
      "grad_norm": 2.2481749057769775,
      "learning_rate": 0.00014670255720053836,
      "loss": 0.9625,
      "step": 298
    },
    {
      "epoch": 1.0604444444444445,
      "grad_norm": 0.4314707815647125,
      "learning_rate": 0.00014643337819650066,
      "loss": 1.4748,
      "step": 299
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.41334810853004456,
      "learning_rate": 0.000146164199192463,
      "loss": 1.1987,
      "step": 300
    },
    {
      "epoch": 1.0675555555555556,
      "grad_norm": 1.418563723564148,
      "learning_rate": 0.0001458950201884253,
      "loss": 1.2506,
      "step": 301
    },
    {
      "epoch": 1.0711111111111111,
      "grad_norm": 0.4225234389305115,
      "learning_rate": 0.0001456258411843876,
      "loss": 0.8938,
      "step": 302
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.6556747555732727,
      "learning_rate": 0.00014535666218034994,
      "loss": 0.8747,
      "step": 303
    },
    {
      "epoch": 1.0782222222222222,
      "grad_norm": 0.4171738624572754,
      "learning_rate": 0.00014508748317631224,
      "loss": 0.7903,
      "step": 304
    },
    {
      "epoch": 1.0817777777777777,
      "grad_norm": 0.407701313495636,
      "learning_rate": 0.0001448183041722746,
      "loss": 1.3107,
      "step": 305
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.8502035140991211,
      "learning_rate": 0.0001445491251682369,
      "loss": 0.9008,
      "step": 306
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 2.8406260013580322,
      "learning_rate": 0.0001442799461641992,
      "loss": 1.876,
      "step": 307
    },
    {
      "epoch": 1.0924444444444443,
      "grad_norm": 0.49091583490371704,
      "learning_rate": 0.0001440107671601615,
      "loss": 1.2037,
      "step": 308
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.7698044776916504,
      "learning_rate": 0.0001437415881561238,
      "loss": 0.6167,
      "step": 309
    },
    {
      "epoch": 1.0995555555555556,
      "grad_norm": 0.5304529070854187,
      "learning_rate": 0.00014347240915208614,
      "loss": 1.0344,
      "step": 310
    },
    {
      "epoch": 1.1031111111111112,
      "grad_norm": 0.3576546609401703,
      "learning_rate": 0.00014320323014804846,
      "loss": 1.1444,
      "step": 311
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.6831066608428955,
      "learning_rate": 0.00014293405114401076,
      "loss": 2.0732,
      "step": 312
    },
    {
      "epoch": 1.1102222222222222,
      "grad_norm": 0.7344862818717957,
      "learning_rate": 0.0001426648721399731,
      "loss": 2.1,
      "step": 313
    },
    {
      "epoch": 1.1137777777777778,
      "grad_norm": 0.6108401417732239,
      "learning_rate": 0.0001423956931359354,
      "loss": 1.2544,
      "step": 314
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.46840226650238037,
      "learning_rate": 0.00014212651413189771,
      "loss": 0.9068,
      "step": 315
    },
    {
      "epoch": 1.1208888888888888,
      "grad_norm": 1.0332311391830444,
      "learning_rate": 0.00014185733512786004,
      "loss": 1.3588,
      "step": 316
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 0.43495795130729675,
      "learning_rate": 0.00014158815612382234,
      "loss": 1.0726,
      "step": 317
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.6963450908660889,
      "learning_rate": 0.00014131897711978467,
      "loss": 0.8869,
      "step": 318
    },
    {
      "epoch": 1.1315555555555556,
      "grad_norm": 0.45892462134361267,
      "learning_rate": 0.000141049798115747,
      "loss": 1.1813,
      "step": 319
    },
    {
      "epoch": 1.1351111111111112,
      "grad_norm": 1.175833821296692,
      "learning_rate": 0.0001407806191117093,
      "loss": 1.2899,
      "step": 320
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.5530577301979065,
      "learning_rate": 0.00014051144010767162,
      "loss": 1.2218,
      "step": 321
    },
    {
      "epoch": 1.1422222222222222,
      "grad_norm": 0.33767277002334595,
      "learning_rate": 0.00014024226110363392,
      "loss": 0.9844,
      "step": 322
    },
    {
      "epoch": 1.1457777777777778,
      "grad_norm": 0.37571579217910767,
      "learning_rate": 0.00013997308209959624,
      "loss": 0.8213,
      "step": 323
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.43501412868499756,
      "learning_rate": 0.00013970390309555857,
      "loss": 0.9494,
      "step": 324
    },
    {
      "epoch": 1.1528888888888889,
      "grad_norm": 0.9383093118667603,
      "learning_rate": 0.00013943472409152087,
      "loss": 1.1826,
      "step": 325
    },
    {
      "epoch": 1.1564444444444444,
      "grad_norm": 1.0093683004379272,
      "learning_rate": 0.0001391655450874832,
      "loss": 1.7745,
      "step": 326
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.47069960832595825,
      "learning_rate": 0.0001388963660834455,
      "loss": 0.7098,
      "step": 327
    },
    {
      "epoch": 1.1635555555555555,
      "grad_norm": 0.35761910676956177,
      "learning_rate": 0.00013862718707940782,
      "loss": 1.4242,
      "step": 328
    },
    {
      "epoch": 1.1671111111111112,
      "grad_norm": 0.6805028319358826,
      "learning_rate": 0.00013835800807537014,
      "loss": 0.6437,
      "step": 329
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.9209582209587097,
      "learning_rate": 0.00013808882907133244,
      "loss": 1.0217,
      "step": 330
    },
    {
      "epoch": 1.1742222222222223,
      "grad_norm": 0.5888932347297668,
      "learning_rate": 0.00013781965006729477,
      "loss": 1.3067,
      "step": 331
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 1.4281781911849976,
      "learning_rate": 0.00013755047106325707,
      "loss": 0.0763,
      "step": 332
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 1.1801198720932007,
      "learning_rate": 0.00013728129205921937,
      "loss": 1.4572,
      "step": 333
    },
    {
      "epoch": 1.1848888888888889,
      "grad_norm": 0.528374433517456,
      "learning_rate": 0.00013701211305518172,
      "loss": 1.3542,
      "step": 334
    },
    {
      "epoch": 1.1884444444444444,
      "grad_norm": 0.6746883392333984,
      "learning_rate": 0.00013674293405114402,
      "loss": 0.8825,
      "step": 335
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.5219385623931885,
      "learning_rate": 0.00013647375504710635,
      "loss": 0.8417,
      "step": 336
    },
    {
      "epoch": 1.1955555555555555,
      "grad_norm": 0.7371846437454224,
      "learning_rate": 0.00013620457604306865,
      "loss": 1.4079,
      "step": 337
    },
    {
      "epoch": 1.199111111111111,
      "grad_norm": 1.5975574254989624,
      "learning_rate": 0.00013593539703903094,
      "loss": 1.6536,
      "step": 338
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.6501990556716919,
      "learning_rate": 0.0001356662180349933,
      "loss": 1.5771,
      "step": 339
    },
    {
      "epoch": 1.2062222222222223,
      "grad_norm": 0.4334208071231842,
      "learning_rate": 0.0001353970390309556,
      "loss": 0.8486,
      "step": 340
    },
    {
      "epoch": 1.2097777777777778,
      "grad_norm": 0.6557787656784058,
      "learning_rate": 0.0001351278600269179,
      "loss": 1.0548,
      "step": 341
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.6312916278839111,
      "learning_rate": 0.00013485868102288022,
      "loss": 0.7501,
      "step": 342
    },
    {
      "epoch": 1.216888888888889,
      "grad_norm": 1.1787288188934326,
      "learning_rate": 0.00013458950201884252,
      "loss": 1.3801,
      "step": 343
    },
    {
      "epoch": 1.2204444444444444,
      "grad_norm": 0.4858352839946747,
      "learning_rate": 0.00013432032301480487,
      "loss": 1.3563,
      "step": 344
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.6948398947715759,
      "learning_rate": 0.00013405114401076717,
      "loss": 1.2242,
      "step": 345
    },
    {
      "epoch": 1.2275555555555555,
      "grad_norm": 0.41810595989227295,
      "learning_rate": 0.00013378196500672947,
      "loss": 0.8599,
      "step": 346
    },
    {
      "epoch": 1.231111111111111,
      "grad_norm": 0.6559593081474304,
      "learning_rate": 0.0001335127860026918,
      "loss": 1.4023,
      "step": 347
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.4936801493167877,
      "learning_rate": 0.0001332436069986541,
      "loss": 1.2044,
      "step": 348
    },
    {
      "epoch": 1.2382222222222223,
      "grad_norm": 0.5381296277046204,
      "learning_rate": 0.00013297442799461642,
      "loss": 1.4087,
      "step": 349
    },
    {
      "epoch": 1.2417777777777779,
      "grad_norm": 0.5157427191734314,
      "learning_rate": 0.00013270524899057875,
      "loss": 0.8946,
      "step": 350
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 1.5028034448623657,
      "learning_rate": 0.00013243606998654105,
      "loss": 1.4573,
      "step": 351
    },
    {
      "epoch": 1.248888888888889,
      "grad_norm": 0.4232273995876312,
      "learning_rate": 0.00013216689098250337,
      "loss": 0.6886,
      "step": 352
    },
    {
      "epoch": 1.2524444444444445,
      "grad_norm": 0.602737545967102,
      "learning_rate": 0.00013189771197846567,
      "loss": 1.2523,
      "step": 353
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.4993952810764313,
      "learning_rate": 0.000131628532974428,
      "loss": 0.8682,
      "step": 354
    },
    {
      "epoch": 1.2595555555555555,
      "grad_norm": 0.5261483192443848,
      "learning_rate": 0.00013135935397039033,
      "loss": 1.0591,
      "step": 355
    },
    {
      "epoch": 1.263111111111111,
      "grad_norm": 0.4837905764579773,
      "learning_rate": 0.00013109017496635262,
      "loss": 1.1756,
      "step": 356
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.832447350025177,
      "learning_rate": 0.00013082099596231495,
      "loss": 1.0163,
      "step": 357
    },
    {
      "epoch": 1.2702222222222221,
      "grad_norm": 0.6625156998634338,
      "learning_rate": 0.00013055181695827725,
      "loss": 1.4777,
      "step": 358
    },
    {
      "epoch": 1.2737777777777777,
      "grad_norm": 0.7598236799240112,
      "learning_rate": 0.00013028263795423958,
      "loss": 1.0017,
      "step": 359
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.4000476896762848,
      "learning_rate": 0.0001300134589502019,
      "loss": 1.0208,
      "step": 360
    },
    {
      "epoch": 1.280888888888889,
      "grad_norm": 0.42712485790252686,
      "learning_rate": 0.0001297442799461642,
      "loss": 1.1063,
      "step": 361
    },
    {
      "epoch": 1.2844444444444445,
      "grad_norm": 1.524664282798767,
      "learning_rate": 0.00012947510094212653,
      "loss": 0.7795,
      "step": 362
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.5034049153327942,
      "learning_rate": 0.00012920592193808883,
      "loss": 1.4956,
      "step": 363
    },
    {
      "epoch": 1.2915555555555556,
      "grad_norm": 0.7318310737609863,
      "learning_rate": 0.00012893674293405115,
      "loss": 0.8895,
      "step": 364
    },
    {
      "epoch": 1.295111111111111,
      "grad_norm": 1.0917840003967285,
      "learning_rate": 0.00012866756393001348,
      "loss": 0.9866,
      "step": 365
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.49054598808288574,
      "learning_rate": 0.00012839838492597578,
      "loss": 0.9419,
      "step": 366
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 0.47545126080513,
      "learning_rate": 0.00012812920592193808,
      "loss": 1.3694,
      "step": 367
    },
    {
      "epoch": 1.3057777777777777,
      "grad_norm": 0.4937813878059387,
      "learning_rate": 0.0001278600269179004,
      "loss": 1.1736,
      "step": 368
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.49558064341545105,
      "learning_rate": 0.00012759084791386273,
      "loss": 1.0889,
      "step": 369
    },
    {
      "epoch": 1.3128888888888888,
      "grad_norm": 0.6193681359291077,
      "learning_rate": 0.00012732166890982505,
      "loss": 0.8664,
      "step": 370
    },
    {
      "epoch": 1.3164444444444445,
      "grad_norm": 0.5346587896347046,
      "learning_rate": 0.00012705248990578735,
      "loss": 1.0578,
      "step": 371
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.5411945581436157,
      "learning_rate": 0.00012678331090174965,
      "loss": 0.94,
      "step": 372
    },
    {
      "epoch": 1.3235555555555556,
      "grad_norm": 0.5450740456581116,
      "learning_rate": 0.00012651413189771198,
      "loss": 1.2285,
      "step": 373
    },
    {
      "epoch": 1.3271111111111111,
      "grad_norm": 0.5201631784439087,
      "learning_rate": 0.0001262449528936743,
      "loss": 1.2156,
      "step": 374
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.8746376633644104,
      "learning_rate": 0.0001259757738896366,
      "loss": 1.5062,
      "step": 375
    },
    {
      "epoch": 1.3342222222222222,
      "grad_norm": 0.5007808208465576,
      "learning_rate": 0.00012570659488559893,
      "loss": 1.2301,
      "step": 376
    },
    {
      "epoch": 1.3377777777777777,
      "grad_norm": 0.5830467939376831,
      "learning_rate": 0.00012543741588156123,
      "loss": 0.8392,
      "step": 377
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.42929258942604065,
      "learning_rate": 0.00012516823687752356,
      "loss": 1.2081,
      "step": 378
    },
    {
      "epoch": 1.3448888888888888,
      "grad_norm": 0.52093905210495,
      "learning_rate": 0.00012489905787348588,
      "loss": 1.0895,
      "step": 379
    },
    {
      "epoch": 1.3484444444444446,
      "grad_norm": 0.627741813659668,
      "learning_rate": 0.00012462987886944818,
      "loss": 1.5656,
      "step": 380
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.7872267961502075,
      "learning_rate": 0.0001243606998654105,
      "loss": 1.2289,
      "step": 381
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.40516775846481323,
      "learning_rate": 0.0001240915208613728,
      "loss": 1.2183,
      "step": 382
    },
    {
      "epoch": 1.3591111111111112,
      "grad_norm": 0.3053956925868988,
      "learning_rate": 0.00012382234185733513,
      "loss": 0.77,
      "step": 383
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.661889374256134,
      "learning_rate": 0.00012355316285329746,
      "loss": 1.1077,
      "step": 384
    },
    {
      "epoch": 1.3662222222222222,
      "grad_norm": 0.6418240070343018,
      "learning_rate": 0.00012328398384925976,
      "loss": 1.0933,
      "step": 385
    },
    {
      "epoch": 1.3697777777777778,
      "grad_norm": 0.4024575650691986,
      "learning_rate": 0.00012301480484522208,
      "loss": 0.8233,
      "step": 386
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.5325844883918762,
      "learning_rate": 0.00012274562584118438,
      "loss": 1.0286,
      "step": 387
    },
    {
      "epoch": 1.3768888888888888,
      "grad_norm": 0.6266524195671082,
      "learning_rate": 0.0001224764468371467,
      "loss": 1.0966,
      "step": 388
    },
    {
      "epoch": 1.3804444444444444,
      "grad_norm": 0.7550873756408691,
      "learning_rate": 0.00012220726783310903,
      "loss": 1.3189,
      "step": 389
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.44828981161117554,
      "learning_rate": 0.00012193808882907133,
      "loss": 0.9153,
      "step": 390
    },
    {
      "epoch": 1.3875555555555557,
      "grad_norm": 0.49068406224250793,
      "learning_rate": 0.00012166890982503365,
      "loss": 0.8217,
      "step": 391
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 0.46209636330604553,
      "learning_rate": 0.00012139973082099596,
      "loss": 0.8939,
      "step": 392
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.562278687953949,
      "learning_rate": 0.0001211305518169583,
      "loss": 1.1071,
      "step": 393
    },
    {
      "epoch": 1.3982222222222223,
      "grad_norm": 0.42748209834098816,
      "learning_rate": 0.0001208613728129206,
      "loss": 1.1919,
      "step": 394
    },
    {
      "epoch": 1.4017777777777778,
      "grad_norm": 0.40763670206069946,
      "learning_rate": 0.00012059219380888291,
      "loss": 1.3548,
      "step": 395
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.5826980471611023,
      "learning_rate": 0.00012032301480484522,
      "loss": 0.8941,
      "step": 396
    },
    {
      "epoch": 1.4088888888888889,
      "grad_norm": 0.4893500506877899,
      "learning_rate": 0.00012005383580080754,
      "loss": 1.0319,
      "step": 397
    },
    {
      "epoch": 1.4124444444444444,
      "grad_norm": 0.6868374347686768,
      "learning_rate": 0.00011978465679676986,
      "loss": 0.9584,
      "step": 398
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.5697224140167236,
      "learning_rate": 0.00011951547779273217,
      "loss": 1.5175,
      "step": 399
    },
    {
      "epoch": 1.4195555555555557,
      "grad_norm": 0.5339218378067017,
      "learning_rate": 0.00011924629878869449,
      "loss": 1.1168,
      "step": 400
    },
    {
      "epoch": 1.423111111111111,
      "grad_norm": 0.7209904789924622,
      "learning_rate": 0.0001189771197846568,
      "loss": 1.5668,
      "step": 401
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.680099368095398,
      "learning_rate": 0.00011870794078061911,
      "loss": 1.2445,
      "step": 402
    },
    {
      "epoch": 1.4302222222222223,
      "grad_norm": 0.5311406254768372,
      "learning_rate": 0.00011843876177658144,
      "loss": 0.9979,
      "step": 403
    },
    {
      "epoch": 1.4337777777777778,
      "grad_norm": 0.658854603767395,
      "learning_rate": 0.00011816958277254375,
      "loss": 1.2837,
      "step": 404
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 2.419548749923706,
      "learning_rate": 0.00011790040376850606,
      "loss": 0.1339,
      "step": 405
    },
    {
      "epoch": 1.4408888888888889,
      "grad_norm": 0.4946020245552063,
      "learning_rate": 0.00011763122476446838,
      "loss": 1.1551,
      "step": 406
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.5353121161460876,
      "learning_rate": 0.00011736204576043069,
      "loss": 1.064,
      "step": 407
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.645849347114563,
      "learning_rate": 0.00011709286675639301,
      "loss": 0.8859,
      "step": 408
    },
    {
      "epoch": 1.4515555555555555,
      "grad_norm": 0.4076879918575287,
      "learning_rate": 0.00011682368775235533,
      "loss": 0.8663,
      "step": 409
    },
    {
      "epoch": 1.455111111111111,
      "grad_norm": 0.4506739377975464,
      "learning_rate": 0.00011655450874831764,
      "loss": 1.1576,
      "step": 410
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.5075539350509644,
      "learning_rate": 0.00011628532974427995,
      "loss": 0.9667,
      "step": 411
    },
    {
      "epoch": 1.462222222222222,
      "grad_norm": 0.6042335033416748,
      "learning_rate": 0.00011601615074024226,
      "loss": 1.0946,
      "step": 412
    },
    {
      "epoch": 1.4657777777777778,
      "grad_norm": 0.9611848592758179,
      "learning_rate": 0.00011574697173620459,
      "loss": 1.5511,
      "step": 413
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 2.7561402320861816,
      "learning_rate": 0.0001154777927321669,
      "loss": 0.1693,
      "step": 414
    },
    {
      "epoch": 1.472888888888889,
      "grad_norm": 0.7777256965637207,
      "learning_rate": 0.00011520861372812922,
      "loss": 1.1326,
      "step": 415
    },
    {
      "epoch": 1.4764444444444444,
      "grad_norm": 0.636185348033905,
      "learning_rate": 0.00011493943472409153,
      "loss": 1.448,
      "step": 416
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.6775022149085999,
      "learning_rate": 0.00011467025572005383,
      "loss": 1.235,
      "step": 417
    },
    {
      "epoch": 1.4835555555555555,
      "grad_norm": 0.4335225522518158,
      "learning_rate": 0.00011440107671601617,
      "loss": 0.8417,
      "step": 418
    },
    {
      "epoch": 1.487111111111111,
      "grad_norm": 0.2955522835254669,
      "learning_rate": 0.00011413189771197848,
      "loss": 1.0075,
      "step": 419
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.669853150844574,
      "learning_rate": 0.00011386271870794079,
      "loss": 1.0987,
      "step": 420
    },
    {
      "epoch": 1.4942222222222221,
      "grad_norm": 0.5401464700698853,
      "learning_rate": 0.00011359353970390309,
      "loss": 0.9357,
      "step": 421
    },
    {
      "epoch": 1.4977777777777779,
      "grad_norm": 0.35327255725860596,
      "learning_rate": 0.0001133243606998654,
      "loss": 0.8894,
      "step": 422
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.6428256630897522,
      "learning_rate": 0.00011305518169582774,
      "loss": 0.9341,
      "step": 423
    },
    {
      "epoch": 1.504888888888889,
      "grad_norm": 0.8231083750724792,
      "learning_rate": 0.00011278600269179006,
      "loss": 1.0232,
      "step": 424
    },
    {
      "epoch": 1.5084444444444445,
      "grad_norm": 1.4006667137145996,
      "learning_rate": 0.00011251682368775235,
      "loss": 1.0929,
      "step": 425
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.6655312180519104,
      "learning_rate": 0.00011224764468371467,
      "loss": 1.5393,
      "step": 426
    },
    {
      "epoch": 1.5155555555555555,
      "grad_norm": 0.692173182964325,
      "learning_rate": 0.00011197846567967698,
      "loss": 1.0666,
      "step": 427
    },
    {
      "epoch": 1.519111111111111,
      "grad_norm": 0.45526015758514404,
      "learning_rate": 0.00011170928667563932,
      "loss": 1.4852,
      "step": 428
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.5109575390815735,
      "learning_rate": 0.00011144010767160162,
      "loss": 1.1658,
      "step": 429
    },
    {
      "epoch": 1.5262222222222221,
      "grad_norm": 0.5618743896484375,
      "learning_rate": 0.00011117092866756393,
      "loss": 1.3046,
      "step": 430
    },
    {
      "epoch": 1.529777777777778,
      "grad_norm": 0.45944514870643616,
      "learning_rate": 0.00011090174966352624,
      "loss": 1.355,
      "step": 431
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.5362595319747925,
      "learning_rate": 0.00011063257065948856,
      "loss": 0.9887,
      "step": 432
    },
    {
      "epoch": 1.536888888888889,
      "grad_norm": 0.4880378246307373,
      "learning_rate": 0.00011036339165545088,
      "loss": 1.3803,
      "step": 433
    },
    {
      "epoch": 1.5404444444444443,
      "grad_norm": 0.7276250123977661,
      "learning_rate": 0.0001100942126514132,
      "loss": 0.7118,
      "step": 434
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.8629196286201477,
      "learning_rate": 0.00010982503364737551,
      "loss": 1.7967,
      "step": 435
    },
    {
      "epoch": 1.5475555555555556,
      "grad_norm": 1.3917365074157715,
      "learning_rate": 0.00010955585464333782,
      "loss": 1.6964,
      "step": 436
    },
    {
      "epoch": 1.551111111111111,
      "grad_norm": 0.5979849696159363,
      "learning_rate": 0.00010928667563930013,
      "loss": 1.1895,
      "step": 437
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.5620782971382141,
      "learning_rate": 0.00010901749663526246,
      "loss": 1.0082,
      "step": 438
    },
    {
      "epoch": 1.5582222222222222,
      "grad_norm": 0.5043663382530212,
      "learning_rate": 0.00010874831763122477,
      "loss": 1.0071,
      "step": 439
    },
    {
      "epoch": 1.561777777777778,
      "grad_norm": 0.659468948841095,
      "learning_rate": 0.00010847913862718708,
      "loss": 1.0779,
      "step": 440
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.7350125908851624,
      "learning_rate": 0.0001082099596231494,
      "loss": 1.2754,
      "step": 441
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 0.874281644821167,
      "learning_rate": 0.00010794078061911172,
      "loss": 0.83,
      "step": 442
    },
    {
      "epoch": 1.5724444444444443,
      "grad_norm": 0.7123392820358276,
      "learning_rate": 0.00010767160161507404,
      "loss": 1.3468,
      "step": 443
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.439998596906662,
      "learning_rate": 0.00010740242261103635,
      "loss": 1.0144,
      "step": 444
    },
    {
      "epoch": 1.5795555555555556,
      "grad_norm": 0.40848302841186523,
      "learning_rate": 0.00010713324360699866,
      "loss": 1.091,
      "step": 445
    },
    {
      "epoch": 1.5831111111111111,
      "grad_norm": 0.7358989119529724,
      "learning_rate": 0.00010686406460296097,
      "loss": 0.726,
      "step": 446
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.48468920588493347,
      "learning_rate": 0.0001065948855989233,
      "loss": 1.2408,
      "step": 447
    },
    {
      "epoch": 1.5902222222222222,
      "grad_norm": 0.5322388410568237,
      "learning_rate": 0.00010632570659488561,
      "loss": 1.1177,
      "step": 448
    },
    {
      "epoch": 1.5937777777777777,
      "grad_norm": 0.47103622555732727,
      "learning_rate": 0.00010605652759084792,
      "loss": 1.2,
      "step": 449
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.4360021650791168,
      "learning_rate": 0.00010578734858681024,
      "loss": 0.9571,
      "step": 450
    },
    {
      "epoch": 1.600888888888889,
      "grad_norm": 0.4869726598262787,
      "learning_rate": 0.00010551816958277254,
      "loss": 1.113,
      "step": 451
    },
    {
      "epoch": 1.6044444444444443,
      "grad_norm": 0.6626381874084473,
      "learning_rate": 0.00010524899057873488,
      "loss": 1.0247,
      "step": 452
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.5871049761772156,
      "learning_rate": 0.00010497981157469719,
      "loss": 1.2926,
      "step": 453
    },
    {
      "epoch": 1.6115555555555554,
      "grad_norm": 0.5724563002586365,
      "learning_rate": 0.0001047106325706595,
      "loss": 0.9195,
      "step": 454
    },
    {
      "epoch": 1.6151111111111112,
      "grad_norm": 0.708459198474884,
      "learning_rate": 0.0001044414535666218,
      "loss": 1.152,
      "step": 455
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.6907461285591125,
      "learning_rate": 0.00010417227456258411,
      "loss": 0.9354,
      "step": 456
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.4663774073123932,
      "learning_rate": 0.00010390309555854645,
      "loss": 0.8643,
      "step": 457
    },
    {
      "epoch": 1.6257777777777778,
      "grad_norm": 1.5924190282821655,
      "learning_rate": 0.00010363391655450876,
      "loss": 1.1769,
      "step": 458
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 1.1578738689422607,
      "learning_rate": 0.00010336473755047106,
      "loss": 1.6854,
      "step": 459
    },
    {
      "epoch": 1.6328888888888888,
      "grad_norm": 0.5077686309814453,
      "learning_rate": 0.00010309555854643338,
      "loss": 1.0461,
      "step": 460
    },
    {
      "epoch": 1.6364444444444444,
      "grad_norm": 0.3927466571331024,
      "learning_rate": 0.00010282637954239569,
      "loss": 0.8703,
      "step": 461
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.6425483822822571,
      "learning_rate": 0.00010255720053835803,
      "loss": 1.7624,
      "step": 462
    },
    {
      "epoch": 1.6435555555555554,
      "grad_norm": 0.725844144821167,
      "learning_rate": 0.00010228802153432033,
      "loss": 1.0301,
      "step": 463
    },
    {
      "epoch": 1.6471111111111112,
      "grad_norm": 0.6518636345863342,
      "learning_rate": 0.00010201884253028264,
      "loss": 1.17,
      "step": 464
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.6274486780166626,
      "learning_rate": 0.00010174966352624495,
      "loss": 1.2543,
      "step": 465
    },
    {
      "epoch": 1.6542222222222223,
      "grad_norm": 0.510281503200531,
      "learning_rate": 0.00010148048452220727,
      "loss": 1.377,
      "step": 466
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 1.129225730895996,
      "learning_rate": 0.00010121130551816959,
      "loss": 1.2855,
      "step": 467
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.7240411639213562,
      "learning_rate": 0.0001009421265141319,
      "loss": 1.3352,
      "step": 468
    },
    {
      "epoch": 1.6648888888888889,
      "grad_norm": 0.6668725609779358,
      "learning_rate": 0.00010067294751009422,
      "loss": 1.1478,
      "step": 469
    },
    {
      "epoch": 1.6684444444444444,
      "grad_norm": 0.7390429377555847,
      "learning_rate": 0.00010040376850605653,
      "loss": 1.2207,
      "step": 470
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.7465084195137024,
      "learning_rate": 0.00010013458950201884,
      "loss": 0.8591,
      "step": 471
    },
    {
      "epoch": 1.6755555555555555,
      "grad_norm": 0.8764381408691406,
      "learning_rate": 9.986541049798115e-05,
      "loss": 1.729,
      "step": 472
    },
    {
      "epoch": 1.6791111111111112,
      "grad_norm": 0.46826648712158203,
      "learning_rate": 9.959623149394348e-05,
      "loss": 0.9387,
      "step": 473
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.6442382335662842,
      "learning_rate": 9.932705248990579e-05,
      "loss": 1.3558,
      "step": 474
    },
    {
      "epoch": 1.6862222222222223,
      "grad_norm": 1.217589020729065,
      "learning_rate": 9.90578734858681e-05,
      "loss": 1.5086,
      "step": 475
    },
    {
      "epoch": 1.6897777777777778,
      "grad_norm": 0.4249441623687744,
      "learning_rate": 9.878869448183042e-05,
      "loss": 0.914,
      "step": 476
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.5283467173576355,
      "learning_rate": 9.851951547779273e-05,
      "loss": 1.0403,
      "step": 477
    },
    {
      "epoch": 1.696888888888889,
      "grad_norm": 0.5963265895843506,
      "learning_rate": 9.825033647375506e-05,
      "loss": 0.8081,
      "step": 478
    },
    {
      "epoch": 1.7004444444444444,
      "grad_norm": 0.6121907830238342,
      "learning_rate": 9.798115746971737e-05,
      "loss": 0.7648,
      "step": 479
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.6706607341766357,
      "learning_rate": 9.771197846567968e-05,
      "loss": 0.9652,
      "step": 480
    },
    {
      "epoch": 1.7075555555555555,
      "grad_norm": 0.7840828895568848,
      "learning_rate": 9.7442799461642e-05,
      "loss": 1.7135,
      "step": 481
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.7045674920082092,
      "learning_rate": 9.717362045760431e-05,
      "loss": 1.2658,
      "step": 482
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.5571746230125427,
      "learning_rate": 9.690444145356663e-05,
      "loss": 0.8469,
      "step": 483
    },
    {
      "epoch": 1.7182222222222223,
      "grad_norm": 1.092525601387024,
      "learning_rate": 9.663526244952895e-05,
      "loss": 1.4951,
      "step": 484
    },
    {
      "epoch": 1.7217777777777776,
      "grad_norm": 1.4281729459762573,
      "learning_rate": 9.636608344549124e-05,
      "loss": 0.9034,
      "step": 485
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.48763972520828247,
      "learning_rate": 9.609690444145357e-05,
      "loss": 1.174,
      "step": 486
    },
    {
      "epoch": 1.728888888888889,
      "grad_norm": 0.7100622057914734,
      "learning_rate": 9.582772543741588e-05,
      "loss": 1.2208,
      "step": 487
    },
    {
      "epoch": 1.7324444444444445,
      "grad_norm": 0.8657350540161133,
      "learning_rate": 9.555854643337821e-05,
      "loss": 1.8077,
      "step": 488
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.5231872200965881,
      "learning_rate": 9.528936742934051e-05,
      "loss": 1.0886,
      "step": 489
    },
    {
      "epoch": 1.7395555555555555,
      "grad_norm": 0.6128417253494263,
      "learning_rate": 9.502018842530283e-05,
      "loss": 1.0034,
      "step": 490
    },
    {
      "epoch": 1.743111111111111,
      "grad_norm": 0.7048588991165161,
      "learning_rate": 9.475100942126515e-05,
      "loss": 1.3726,
      "step": 491
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.544094443321228,
      "learning_rate": 9.448183041722746e-05,
      "loss": 1.1508,
      "step": 492
    },
    {
      "epoch": 1.7502222222222223,
      "grad_norm": 1.5538989305496216,
      "learning_rate": 9.421265141318977e-05,
      "loss": 0.9497,
      "step": 493
    },
    {
      "epoch": 1.7537777777777777,
      "grad_norm": 0.4652959108352661,
      "learning_rate": 9.394347240915209e-05,
      "loss": 1.0842,
      "step": 494
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.4682917594909668,
      "learning_rate": 9.367429340511441e-05,
      "loss": 1.1866,
      "step": 495
    },
    {
      "epoch": 1.7608888888888887,
      "grad_norm": 0.6098275780677795,
      "learning_rate": 9.340511440107672e-05,
      "loss": 1.4466,
      "step": 496
    },
    {
      "epoch": 1.7644444444444445,
      "grad_norm": 0.8003350496292114,
      "learning_rate": 9.313593539703904e-05,
      "loss": 0.6746,
      "step": 497
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.5836489796638489,
      "learning_rate": 9.286675639300135e-05,
      "loss": 0.6901,
      "step": 498
    },
    {
      "epoch": 1.7715555555555556,
      "grad_norm": 0.7085847854614258,
      "learning_rate": 9.259757738896366e-05,
      "loss": 1.2825,
      "step": 499
    },
    {
      "epoch": 1.775111111111111,
      "grad_norm": 0.4334103763103485,
      "learning_rate": 9.232839838492599e-05,
      "loss": 1.1795,
      "step": 500
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 1.2077873945236206,
      "learning_rate": 9.20592193808883e-05,
      "loss": 1.3037,
      "step": 501
    },
    {
      "epoch": 1.7822222222222224,
      "grad_norm": 0.4378529489040375,
      "learning_rate": 9.17900403768506e-05,
      "loss": 1.0883,
      "step": 502
    },
    {
      "epoch": 1.7857777777777777,
      "grad_norm": 0.7212347388267517,
      "learning_rate": 9.152086137281293e-05,
      "loss": 1.3983,
      "step": 503
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.8331217765808105,
      "learning_rate": 9.125168236877524e-05,
      "loss": 1.4158,
      "step": 504
    },
    {
      "epoch": 1.7928888888888888,
      "grad_norm": 0.4780072569847107,
      "learning_rate": 9.098250336473756e-05,
      "loss": 1.3881,
      "step": 505
    },
    {
      "epoch": 1.7964444444444445,
      "grad_norm": 0.68914794921875,
      "learning_rate": 9.071332436069986e-05,
      "loss": 1.4975,
      "step": 506
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.38533565402030945,
      "learning_rate": 9.044414535666218e-05,
      "loss": 1.0871,
      "step": 507
    },
    {
      "epoch": 1.8035555555555556,
      "grad_norm": 0.6993483304977417,
      "learning_rate": 9.01749663526245e-05,
      "loss": 1.4425,
      "step": 508
    },
    {
      "epoch": 1.8071111111111111,
      "grad_norm": 0.951131284236908,
      "learning_rate": 8.990578734858681e-05,
      "loss": 1.6459,
      "step": 509
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 1.3311541080474854,
      "learning_rate": 8.963660834454913e-05,
      "loss": 1.5934,
      "step": 510
    },
    {
      "epoch": 1.8142222222222222,
      "grad_norm": 0.6441740393638611,
      "learning_rate": 8.936742934051144e-05,
      "loss": 1.1967,
      "step": 511
    },
    {
      "epoch": 1.8177777777777777,
      "grad_norm": 0.5312925577163696,
      "learning_rate": 8.909825033647375e-05,
      "loss": 1.2632,
      "step": 512
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.4624108076095581,
      "learning_rate": 8.882907133243608e-05,
      "loss": 0.9519,
      "step": 513
    },
    {
      "epoch": 1.8248888888888888,
      "grad_norm": 0.6966853737831116,
      "learning_rate": 8.855989232839839e-05,
      "loss": 0.9401,
      "step": 514
    },
    {
      "epoch": 1.8284444444444445,
      "grad_norm": 1.0885597467422485,
      "learning_rate": 8.82907133243607e-05,
      "loss": 1.4872,
      "step": 515
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.5230943560600281,
      "learning_rate": 8.802153432032302e-05,
      "loss": 0.9049,
      "step": 516
    },
    {
      "epoch": 1.8355555555555556,
      "grad_norm": 0.5832743644714355,
      "learning_rate": 8.775235531628533e-05,
      "loss": 1.0998,
      "step": 517
    },
    {
      "epoch": 1.8391111111111111,
      "grad_norm": 0.7609084844589233,
      "learning_rate": 8.748317631224765e-05,
      "loss": 0.4536,
      "step": 518
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.4203326404094696,
      "learning_rate": 8.721399730820995e-05,
      "loss": 0.9359,
      "step": 519
    },
    {
      "epoch": 1.8462222222222222,
      "grad_norm": 0.4925440847873688,
      "learning_rate": 8.694481830417228e-05,
      "loss": 0.9234,
      "step": 520
    },
    {
      "epoch": 1.8497777777777777,
      "grad_norm": 0.40122270584106445,
      "learning_rate": 8.667563930013459e-05,
      "loss": 1.0688,
      "step": 521
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.7134899497032166,
      "learning_rate": 8.640646029609692e-05,
      "loss": 1.1287,
      "step": 522
    },
    {
      "epoch": 1.8568888888888888,
      "grad_norm": 0.6880373358726501,
      "learning_rate": 8.613728129205922e-05,
      "loss": 1.4149,
      "step": 523
    },
    {
      "epoch": 1.8604444444444446,
      "grad_norm": 0.7923650741577148,
      "learning_rate": 8.586810228802153e-05,
      "loss": 1.2162,
      "step": 524
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 1.0175784826278687,
      "learning_rate": 8.559892328398386e-05,
      "loss": 1.813,
      "step": 525
    },
    {
      "epoch": 1.8675555555555556,
      "grad_norm": 0.6324077248573303,
      "learning_rate": 8.532974427994617e-05,
      "loss": 1.0095,
      "step": 526
    },
    {
      "epoch": 1.871111111111111,
      "grad_norm": 0.4799656867980957,
      "learning_rate": 8.506056527590848e-05,
      "loss": 1.3629,
      "step": 527
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.5030525326728821,
      "learning_rate": 8.47913862718708e-05,
      "loss": 0.9862,
      "step": 528
    },
    {
      "epoch": 1.8782222222222222,
      "grad_norm": 0.42698934674263,
      "learning_rate": 8.45222072678331e-05,
      "loss": 1.1605,
      "step": 529
    },
    {
      "epoch": 1.8817777777777778,
      "grad_norm": 0.5997945070266724,
      "learning_rate": 8.425302826379543e-05,
      "loss": 0.691,
      "step": 530
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.5988665819168091,
      "learning_rate": 8.398384925975775e-05,
      "loss": 1.2366,
      "step": 531
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.6464232206344604,
      "learning_rate": 8.371467025572006e-05,
      "loss": 0.9663,
      "step": 532
    },
    {
      "epoch": 1.8924444444444446,
      "grad_norm": 0.4934404790401459,
      "learning_rate": 8.344549125168237e-05,
      "loss": 1.271,
      "step": 533
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.6639776229858398,
      "learning_rate": 8.317631224764468e-05,
      "loss": 1.6118,
      "step": 534
    },
    {
      "epoch": 1.8995555555555557,
      "grad_norm": 0.3983529210090637,
      "learning_rate": 8.290713324360701e-05,
      "loss": 1.1853,
      "step": 535
    },
    {
      "epoch": 1.903111111111111,
      "grad_norm": 0.6915216445922852,
      "learning_rate": 8.263795423956932e-05,
      "loss": 1.1557,
      "step": 536
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.5814608931541443,
      "learning_rate": 8.236877523553163e-05,
      "loss": 1.0267,
      "step": 537
    },
    {
      "epoch": 1.9102222222222223,
      "grad_norm": 0.7021940350532532,
      "learning_rate": 8.209959623149395e-05,
      "loss": 1.0475,
      "step": 538
    },
    {
      "epoch": 1.9137777777777778,
      "grad_norm": 0.5517095327377319,
      "learning_rate": 8.183041722745626e-05,
      "loss": 1.1513,
      "step": 539
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 1.3220638036727905,
      "learning_rate": 8.156123822341859e-05,
      "loss": 0.0812,
      "step": 540
    },
    {
      "epoch": 1.9208888888888889,
      "grad_norm": 0.7030591368675232,
      "learning_rate": 8.129205921938088e-05,
      "loss": 1.3434,
      "step": 541
    },
    {
      "epoch": 1.9244444444444444,
      "grad_norm": 0.43189671635627747,
      "learning_rate": 8.102288021534321e-05,
      "loss": 1.1522,
      "step": 542
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.8225013017654419,
      "learning_rate": 8.075370121130552e-05,
      "loss": 0.9952,
      "step": 543
    },
    {
      "epoch": 1.9315555555555557,
      "grad_norm": 0.7634989619255066,
      "learning_rate": 8.048452220726784e-05,
      "loss": 1.3042,
      "step": 544
    },
    {
      "epoch": 1.935111111111111,
      "grad_norm": 0.6682963371276855,
      "learning_rate": 8.021534320323015e-05,
      "loss": 1.2115,
      "step": 545
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.7501078248023987,
      "learning_rate": 7.994616419919246e-05,
      "loss": 1.0013,
      "step": 546
    },
    {
      "epoch": 1.942222222222222,
      "grad_norm": 0.47862279415130615,
      "learning_rate": 7.967698519515479e-05,
      "loss": 1.3659,
      "step": 547
    },
    {
      "epoch": 1.9457777777777778,
      "grad_norm": 0.5586228370666504,
      "learning_rate": 7.94078061911171e-05,
      "loss": 0.9804,
      "step": 548
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.5072651505470276,
      "learning_rate": 7.913862718707941e-05,
      "loss": 0.8596,
      "step": 549
    },
    {
      "epoch": 1.952888888888889,
      "grad_norm": 0.6451563835144043,
      "learning_rate": 7.886944818304172e-05,
      "loss": 1.3499,
      "step": 550
    },
    {
      "epoch": 1.9564444444444444,
      "grad_norm": 0.4265325367450714,
      "learning_rate": 7.860026917900404e-05,
      "loss": 1.2836,
      "step": 551
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.2415803670883179,
      "learning_rate": 7.833109017496636e-05,
      "loss": 1.4541,
      "step": 552
    },
    {
      "epoch": 1.9635555555555557,
      "grad_norm": 0.6654568314552307,
      "learning_rate": 7.806191117092868e-05,
      "loss": 1.1013,
      "step": 553
    },
    {
      "epoch": 1.967111111111111,
      "grad_norm": 0.5239416360855103,
      "learning_rate": 7.779273216689099e-05,
      "loss": 0.8715,
      "step": 554
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.6329180598258972,
      "learning_rate": 7.75235531628533e-05,
      "loss": 1.0119,
      "step": 555
    },
    {
      "epoch": 1.974222222222222,
      "grad_norm": 0.7215194702148438,
      "learning_rate": 7.725437415881561e-05,
      "loss": 1.2201,
      "step": 556
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 2.8280563354492188,
      "learning_rate": 7.698519515477794e-05,
      "loss": 0.1837,
      "step": 557
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.5760174989700317,
      "learning_rate": 7.671601615074024e-05,
      "loss": 1.1147,
      "step": 558
    },
    {
      "epoch": 1.984888888888889,
      "grad_norm": 1.3653147220611572,
      "learning_rate": 7.644683714670257e-05,
      "loss": 1.4104,
      "step": 559
    },
    {
      "epoch": 1.9884444444444445,
      "grad_norm": 1.3420547246932983,
      "learning_rate": 7.617765814266488e-05,
      "loss": 1.3106,
      "step": 560
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.4676682949066162,
      "learning_rate": 7.590847913862719e-05,
      "loss": 1.1648,
      "step": 561
    },
    {
      "epoch": 1.9955555555555555,
      "grad_norm": 0.8035997748374939,
      "learning_rate": 7.56393001345895e-05,
      "loss": 1.3798,
      "step": 562
    },
    {
      "epoch": 1.999111111111111,
      "grad_norm": 1.2720715999603271,
      "learning_rate": 7.537012113055182e-05,
      "loss": 0.8631,
      "step": 563
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.624323844909668,
      "learning_rate": 7.510094212651414e-05,
      "loss": 0.6366,
      "step": 564
    },
    {
      "epoch": 2.0035555555555558,
      "grad_norm": 0.6497079730033875,
      "learning_rate": 7.483176312247645e-05,
      "loss": 1.1337,
      "step": 565
    },
    {
      "epoch": 2.007111111111111,
      "grad_norm": 0.690929651260376,
      "learning_rate": 7.456258411843877e-05,
      "loss": 1.6785,
      "step": 566
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 0.43715307116508484,
      "learning_rate": 7.429340511440108e-05,
      "loss": 1.1016,
      "step": 567
    },
    {
      "epoch": 2.014222222222222,
      "grad_norm": 0.4174605906009674,
      "learning_rate": 7.402422611036339e-05,
      "loss": 1.1251,
      "step": 568
    },
    {
      "epoch": 2.017777777777778,
      "grad_norm": 0.7364493012428284,
      "learning_rate": 7.375504710632572e-05,
      "loss": 0.8252,
      "step": 569
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.775010883808136,
      "learning_rate": 7.348586810228803e-05,
      "loss": 0.8598,
      "step": 570
    },
    {
      "epoch": 2.024888888888889,
      "grad_norm": 0.4731384217739105,
      "learning_rate": 7.321668909825033e-05,
      "loss": 0.9692,
      "step": 571
    },
    {
      "epoch": 2.0284444444444443,
      "grad_norm": 0.5742630362510681,
      "learning_rate": 7.294751009421266e-05,
      "loss": 1.5738,
      "step": 572
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.9448388814926147,
      "learning_rate": 7.267833109017497e-05,
      "loss": 0.0699,
      "step": 573
    },
    {
      "epoch": 2.0355555555555553,
      "grad_norm": 1.072513222694397,
      "learning_rate": 7.24091520861373e-05,
      "loss": 1.4627,
      "step": 574
    },
    {
      "epoch": 2.039111111111111,
      "grad_norm": 0.5802344679832458,
      "learning_rate": 7.21399730820996e-05,
      "loss": 1.2248,
      "step": 575
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.4164784848690033,
      "learning_rate": 7.18707940780619e-05,
      "loss": 1.259,
      "step": 576
    },
    {
      "epoch": 2.046222222222222,
      "grad_norm": 0.4371972978115082,
      "learning_rate": 7.160161507402423e-05,
      "loss": 1.1472,
      "step": 577
    },
    {
      "epoch": 2.049777777777778,
      "grad_norm": 0.5524007081985474,
      "learning_rate": 7.133243606998654e-05,
      "loss": 0.6557,
      "step": 578
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.4765206575393677,
      "learning_rate": 7.106325706594886e-05,
      "loss": 0.8608,
      "step": 579
    },
    {
      "epoch": 2.056888888888889,
      "grad_norm": 0.4467289447784424,
      "learning_rate": 7.079407806191117e-05,
      "loss": 0.8988,
      "step": 580
    },
    {
      "epoch": 2.0604444444444443,
      "grad_norm": 0.4470498859882355,
      "learning_rate": 7.05248990578735e-05,
      "loss": 1.1643,
      "step": 581
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.6290631294250488,
      "learning_rate": 7.025572005383581e-05,
      "loss": 0.9215,
      "step": 582
    },
    {
      "epoch": 2.0675555555555554,
      "grad_norm": 0.519153892993927,
      "learning_rate": 6.998654104979812e-05,
      "loss": 0.8363,
      "step": 583
    },
    {
      "epoch": 2.071111111111111,
      "grad_norm": 0.6715312600135803,
      "learning_rate": 6.971736204576043e-05,
      "loss": 1.0652,
      "step": 584
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.6106319427490234,
      "learning_rate": 6.944818304172275e-05,
      "loss": 0.8416,
      "step": 585
    },
    {
      "epoch": 2.078222222222222,
      "grad_norm": 0.5301198959350586,
      "learning_rate": 6.917900403768507e-05,
      "loss": 1.3024,
      "step": 586
    },
    {
      "epoch": 2.081777777777778,
      "grad_norm": 0.5929179191589355,
      "learning_rate": 6.890982503364738e-05,
      "loss": 1.6604,
      "step": 587
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 0.7133167386054993,
      "learning_rate": 6.864064602960968e-05,
      "loss": 1.3125,
      "step": 588
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 0.8095176815986633,
      "learning_rate": 6.837146702557201e-05,
      "loss": 1.477,
      "step": 589
    },
    {
      "epoch": 2.0924444444444443,
      "grad_norm": 1.0209647417068481,
      "learning_rate": 6.810228802153432e-05,
      "loss": 1.1002,
      "step": 590
    },
    {
      "epoch": 2.096,
      "grad_norm": 1.6877082586288452,
      "learning_rate": 6.783310901749665e-05,
      "loss": 1.5613,
      "step": 591
    },
    {
      "epoch": 2.0995555555555554,
      "grad_norm": 0.5383037328720093,
      "learning_rate": 6.756393001345895e-05,
      "loss": 1.0016,
      "step": 592
    },
    {
      "epoch": 2.103111111111111,
      "grad_norm": 0.5559896230697632,
      "learning_rate": 6.729475100942126e-05,
      "loss": 0.5906,
      "step": 593
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.4739871621131897,
      "learning_rate": 6.702557200538359e-05,
      "loss": 0.8922,
      "step": 594
    },
    {
      "epoch": 2.110222222222222,
      "grad_norm": 0.38233596086502075,
      "learning_rate": 6.67563930013459e-05,
      "loss": 0.971,
      "step": 595
    },
    {
      "epoch": 2.113777777777778,
      "grad_norm": 0.5189812183380127,
      "learning_rate": 6.648721399730821e-05,
      "loss": 1.0816,
      "step": 596
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 0.5845692157745361,
      "learning_rate": 6.621803499327052e-05,
      "loss": 0.9541,
      "step": 597
    },
    {
      "epoch": 2.120888888888889,
      "grad_norm": 1.0066505670547485,
      "learning_rate": 6.594885598923284e-05,
      "loss": 1.2388,
      "step": 598
    },
    {
      "epoch": 2.1244444444444444,
      "grad_norm": 0.791719377040863,
      "learning_rate": 6.567967698519516e-05,
      "loss": 1.2879,
      "step": 599
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.41786256432533264,
      "learning_rate": 6.541049798115748e-05,
      "loss": 1.1718,
      "step": 600
    },
    {
      "epoch": 2.1315555555555554,
      "grad_norm": 0.47162505984306335,
      "learning_rate": 6.514131897711979e-05,
      "loss": 0.9943,
      "step": 601
    },
    {
      "epoch": 2.135111111111111,
      "grad_norm": 0.7279420495033264,
      "learning_rate": 6.48721399730821e-05,
      "loss": 1.2712,
      "step": 602
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 0.4656890630722046,
      "learning_rate": 6.460296096904441e-05,
      "loss": 0.9421,
      "step": 603
    },
    {
      "epoch": 2.1422222222222222,
      "grad_norm": 0.4718046486377716,
      "learning_rate": 6.433378196500674e-05,
      "loss": 0.7454,
      "step": 604
    },
    {
      "epoch": 2.145777777777778,
      "grad_norm": 0.7755267024040222,
      "learning_rate": 6.406460296096904e-05,
      "loss": 1.3423,
      "step": 605
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 0.471174031496048,
      "learning_rate": 6.379542395693136e-05,
      "loss": 1.0021,
      "step": 606
    },
    {
      "epoch": 2.152888888888889,
      "grad_norm": 0.6906159520149231,
      "learning_rate": 6.352624495289368e-05,
      "loss": 0.7623,
      "step": 607
    },
    {
      "epoch": 2.1564444444444444,
      "grad_norm": 0.3814750909805298,
      "learning_rate": 6.325706594885599e-05,
      "loss": 0.8582,
      "step": 608
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.5915864706039429,
      "learning_rate": 6.29878869448183e-05,
      "loss": 0.9484,
      "step": 609
    },
    {
      "epoch": 2.1635555555555555,
      "grad_norm": 0.48182761669158936,
      "learning_rate": 6.271870794078061e-05,
      "loss": 0.8639,
      "step": 610
    },
    {
      "epoch": 2.167111111111111,
      "grad_norm": 0.5830905437469482,
      "learning_rate": 6.244952893674294e-05,
      "loss": 1.4439,
      "step": 611
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 0.45302626490592957,
      "learning_rate": 6.218034993270525e-05,
      "loss": 1.0352,
      "step": 612
    },
    {
      "epoch": 2.1742222222222223,
      "grad_norm": 0.590307354927063,
      "learning_rate": 6.191117092866757e-05,
      "loss": 0.4792,
      "step": 613
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 0.6734315156936646,
      "learning_rate": 6.164199192462988e-05,
      "loss": 0.8503,
      "step": 614
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.8302602171897888,
      "learning_rate": 6.137281292059219e-05,
      "loss": 1.0706,
      "step": 615
    },
    {
      "epoch": 2.1848888888888887,
      "grad_norm": 0.6311891674995422,
      "learning_rate": 6.110363391655452e-05,
      "loss": 0.9,
      "step": 616
    },
    {
      "epoch": 2.1884444444444444,
      "grad_norm": 0.679957926273346,
      "learning_rate": 6.083445491251682e-05,
      "loss": 0.6158,
      "step": 617
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.6713855266571045,
      "learning_rate": 6.056527590847915e-05,
      "loss": 1.4806,
      "step": 618
    },
    {
      "epoch": 2.1955555555555555,
      "grad_norm": 0.8585783839225769,
      "learning_rate": 6.0296096904441455e-05,
      "loss": 0.8349,
      "step": 619
    },
    {
      "epoch": 2.1991111111111112,
      "grad_norm": 0.5206133127212524,
      "learning_rate": 6.002691790040377e-05,
      "loss": 1.0892,
      "step": 620
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 0.6372001767158508,
      "learning_rate": 5.975773889636609e-05,
      "loss": 1.1648,
      "step": 621
    },
    {
      "epoch": 2.2062222222222223,
      "grad_norm": 1.3115934133529663,
      "learning_rate": 5.94885598923284e-05,
      "loss": 1.5622,
      "step": 622
    },
    {
      "epoch": 2.2097777777777776,
      "grad_norm": 0.49725374579429626,
      "learning_rate": 5.921938088829072e-05,
      "loss": 1.0345,
      "step": 623
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 1.1062179803848267,
      "learning_rate": 5.895020188425303e-05,
      "loss": 1.8849,
      "step": 624
    },
    {
      "epoch": 2.2168888888888887,
      "grad_norm": 0.7722679972648621,
      "learning_rate": 5.8681022880215344e-05,
      "loss": 1.2634,
      "step": 625
    },
    {
      "epoch": 2.2204444444444444,
      "grad_norm": 0.570557713508606,
      "learning_rate": 5.841184387617766e-05,
      "loss": 0.8743,
      "step": 626
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.6607696413993835,
      "learning_rate": 5.8142664872139976e-05,
      "loss": 0.9701,
      "step": 627
    },
    {
      "epoch": 2.2275555555555555,
      "grad_norm": 0.6197419166564941,
      "learning_rate": 5.7873485868102295e-05,
      "loss": 1.5193,
      "step": 628
    },
    {
      "epoch": 2.2311111111111113,
      "grad_norm": 0.8279544711112976,
      "learning_rate": 5.760430686406461e-05,
      "loss": 1.0291,
      "step": 629
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 0.6381841897964478,
      "learning_rate": 5.7335127860026914e-05,
      "loss": 0.8372,
      "step": 630
    },
    {
      "epoch": 2.2382222222222223,
      "grad_norm": 0.4786204397678375,
      "learning_rate": 5.706594885598924e-05,
      "loss": 0.7282,
      "step": 631
    },
    {
      "epoch": 2.2417777777777776,
      "grad_norm": 1.1194179058074951,
      "learning_rate": 5.6796769851951546e-05,
      "loss": 1.2467,
      "step": 632
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 0.7304350733757019,
      "learning_rate": 5.652759084791387e-05,
      "loss": 1.345,
      "step": 633
    },
    {
      "epoch": 2.2488888888888887,
      "grad_norm": 0.5488220453262329,
      "learning_rate": 5.625841184387618e-05,
      "loss": 1.2518,
      "step": 634
    },
    {
      "epoch": 2.2524444444444445,
      "grad_norm": 1.0149778127670288,
      "learning_rate": 5.598923283983849e-05,
      "loss": 1.8409,
      "step": 635
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.6992624402046204,
      "learning_rate": 5.572005383580081e-05,
      "loss": 1.016,
      "step": 636
    },
    {
      "epoch": 2.2595555555555555,
      "grad_norm": 0.7465530037879944,
      "learning_rate": 5.545087483176312e-05,
      "loss": 1.2231,
      "step": 637
    },
    {
      "epoch": 2.2631111111111113,
      "grad_norm": 0.7171044945716858,
      "learning_rate": 5.518169582772544e-05,
      "loss": 1.1048,
      "step": 638
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.4583686590194702,
      "learning_rate": 5.4912516823687754e-05,
      "loss": 0.8361,
      "step": 639
    },
    {
      "epoch": 2.2702222222222224,
      "grad_norm": 0.45299941301345825,
      "learning_rate": 5.4643337819650066e-05,
      "loss": 1.3982,
      "step": 640
    },
    {
      "epoch": 2.2737777777777777,
      "grad_norm": 0.8613412976264954,
      "learning_rate": 5.4374158815612386e-05,
      "loss": 1.0429,
      "step": 641
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.5604138374328613,
      "learning_rate": 5.41049798115747e-05,
      "loss": 1.1125,
      "step": 642
    },
    {
      "epoch": 2.2808888888888887,
      "grad_norm": 0.4133080542087555,
      "learning_rate": 5.383580080753702e-05,
      "loss": 0.9069,
      "step": 643
    },
    {
      "epoch": 2.2844444444444445,
      "grad_norm": 0.5327759981155396,
      "learning_rate": 5.356662180349933e-05,
      "loss": 0.7928,
      "step": 644
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.5437770485877991,
      "learning_rate": 5.329744279946165e-05,
      "loss": 0.9749,
      "step": 645
    },
    {
      "epoch": 2.2915555555555556,
      "grad_norm": 0.7474384903907776,
      "learning_rate": 5.302826379542396e-05,
      "loss": 0.6838,
      "step": 646
    },
    {
      "epoch": 2.295111111111111,
      "grad_norm": 0.5596095323562622,
      "learning_rate": 5.275908479138627e-05,
      "loss": 1.0338,
      "step": 647
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 0.5579642057418823,
      "learning_rate": 5.2489905787348594e-05,
      "loss": 0.8916,
      "step": 648
    },
    {
      "epoch": 2.3022222222222224,
      "grad_norm": 1.2214330434799194,
      "learning_rate": 5.22207267833109e-05,
      "loss": 1.7515,
      "step": 649
    },
    {
      "epoch": 2.3057777777777777,
      "grad_norm": 0.8967910408973694,
      "learning_rate": 5.1951547779273226e-05,
      "loss": 1.3471,
      "step": 650
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 0.6159090399742126,
      "learning_rate": 5.168236877523553e-05,
      "loss": 0.7477,
      "step": 651
    },
    {
      "epoch": 2.3128888888888888,
      "grad_norm": 0.7419201135635376,
      "learning_rate": 5.1413189771197844e-05,
      "loss": 0.5467,
      "step": 652
    },
    {
      "epoch": 2.3164444444444445,
      "grad_norm": 0.5457051992416382,
      "learning_rate": 5.1144010767160164e-05,
      "loss": 0.894,
      "step": 653
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.6834602355957031,
      "learning_rate": 5.0874831763122476e-05,
      "loss": 0.9245,
      "step": 654
    },
    {
      "epoch": 2.3235555555555556,
      "grad_norm": 1.324818730354309,
      "learning_rate": 5.0605652759084796e-05,
      "loss": 1.4203,
      "step": 655
    },
    {
      "epoch": 2.327111111111111,
      "grad_norm": 0.44083598256111145,
      "learning_rate": 5.033647375504711e-05,
      "loss": 0.7603,
      "step": 656
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.4627617597579956,
      "learning_rate": 5.006729475100942e-05,
      "loss": 0.8013,
      "step": 657
    },
    {
      "epoch": 2.3342222222222224,
      "grad_norm": 1.6542326211929321,
      "learning_rate": 4.979811574697174e-05,
      "loss": 1.6841,
      "step": 658
    },
    {
      "epoch": 2.3377777777777777,
      "grad_norm": 2.0795552730560303,
      "learning_rate": 4.952893674293405e-05,
      "loss": 0.3742,
      "step": 659
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 0.9829773306846619,
      "learning_rate": 4.9259757738896365e-05,
      "loss": 1.7431,
      "step": 660
    },
    {
      "epoch": 2.344888888888889,
      "grad_norm": 0.5920790433883667,
      "learning_rate": 4.8990578734858685e-05,
      "loss": 1.1294,
      "step": 661
    },
    {
      "epoch": 2.3484444444444446,
      "grad_norm": 0.6716300845146179,
      "learning_rate": 4.8721399730821e-05,
      "loss": 1.4489,
      "step": 662
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.7373400330543518,
      "learning_rate": 4.845222072678332e-05,
      "loss": 1.2799,
      "step": 663
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 0.7228754758834839,
      "learning_rate": 4.818304172274562e-05,
      "loss": 1.5078,
      "step": 664
    },
    {
      "epoch": 2.359111111111111,
      "grad_norm": 0.5461346507072449,
      "learning_rate": 4.791386271870794e-05,
      "loss": 1.1204,
      "step": 665
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.6295203566551208,
      "learning_rate": 4.7644683714670254e-05,
      "loss": 0.9684,
      "step": 666
    },
    {
      "epoch": 2.3662222222222224,
      "grad_norm": 1.5345011949539185,
      "learning_rate": 4.7375504710632574e-05,
      "loss": 1.829,
      "step": 667
    },
    {
      "epoch": 2.3697777777777778,
      "grad_norm": 0.5823960900306702,
      "learning_rate": 4.7106325706594886e-05,
      "loss": 0.7926,
      "step": 668
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 1.2635701894760132,
      "learning_rate": 4.6837146702557206e-05,
      "loss": 1.2085,
      "step": 669
    },
    {
      "epoch": 2.376888888888889,
      "grad_norm": 0.46357300877571106,
      "learning_rate": 4.656796769851952e-05,
      "loss": 0.9632,
      "step": 670
    },
    {
      "epoch": 2.3804444444444446,
      "grad_norm": 0.577725350856781,
      "learning_rate": 4.629878869448183e-05,
      "loss": 0.8655,
      "step": 671
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.3999348282814026,
      "learning_rate": 4.602960969044415e-05,
      "loss": 1.0768,
      "step": 672
    },
    {
      "epoch": 2.3875555555555557,
      "grad_norm": 0.6555511951446533,
      "learning_rate": 4.576043068640646e-05,
      "loss": 1.204,
      "step": 673
    },
    {
      "epoch": 2.391111111111111,
      "grad_norm": 0.5465855598449707,
      "learning_rate": 4.549125168236878e-05,
      "loss": 1.0936,
      "step": 674
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 0.5483496785163879,
      "learning_rate": 4.522207267833109e-05,
      "loss": 1.2832,
      "step": 675
    },
    {
      "epoch": 2.398222222222222,
      "grad_norm": 0.48376432061195374,
      "learning_rate": 4.495289367429341e-05,
      "loss": 1.0838,
      "step": 676
    },
    {
      "epoch": 2.401777777777778,
      "grad_norm": 0.889028787612915,
      "learning_rate": 4.468371467025572e-05,
      "loss": 0.8857,
      "step": 677
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 1.2892614603042603,
      "learning_rate": 4.441453566621804e-05,
      "loss": 1.0026,
      "step": 678
    },
    {
      "epoch": 2.408888888888889,
      "grad_norm": 0.5344879031181335,
      "learning_rate": 4.414535666218035e-05,
      "loss": 0.848,
      "step": 679
    },
    {
      "epoch": 2.4124444444444446,
      "grad_norm": 0.5092227458953857,
      "learning_rate": 4.3876177658142664e-05,
      "loss": 1.1113,
      "step": 680
    },
    {
      "epoch": 2.416,
      "grad_norm": 1.2018983364105225,
      "learning_rate": 4.360699865410498e-05,
      "loss": 1.4892,
      "step": 681
    },
    {
      "epoch": 2.4195555555555557,
      "grad_norm": 0.8624728918075562,
      "learning_rate": 4.3337819650067296e-05,
      "loss": 0.7104,
      "step": 682
    },
    {
      "epoch": 2.423111111111111,
      "grad_norm": 0.6039324998855591,
      "learning_rate": 4.306864064602961e-05,
      "loss": 1.2583,
      "step": 683
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.6529299020767212,
      "learning_rate": 4.279946164199193e-05,
      "loss": 0.8633,
      "step": 684
    },
    {
      "epoch": 2.430222222222222,
      "grad_norm": 0.8109551668167114,
      "learning_rate": 4.253028263795424e-05,
      "loss": 1.1463,
      "step": 685
    },
    {
      "epoch": 2.433777777777778,
      "grad_norm": 0.610715389251709,
      "learning_rate": 4.226110363391655e-05,
      "loss": 0.7637,
      "step": 686
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 1.1556894779205322,
      "learning_rate": 4.199192462987887e-05,
      "loss": 1.8459,
      "step": 687
    },
    {
      "epoch": 2.440888888888889,
      "grad_norm": 0.45483461022377014,
      "learning_rate": 4.1722745625841185e-05,
      "loss": 1.1914,
      "step": 688
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.46075528860092163,
      "learning_rate": 4.1453566621803505e-05,
      "loss": 0.9949,
      "step": 689
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.6858319640159607,
      "learning_rate": 4.118438761776582e-05,
      "loss": 1.524,
      "step": 690
    },
    {
      "epoch": 2.4515555555555557,
      "grad_norm": 0.6181970834732056,
      "learning_rate": 4.091520861372813e-05,
      "loss": 0.7435,
      "step": 691
    },
    {
      "epoch": 2.455111111111111,
      "grad_norm": 0.4856293201446533,
      "learning_rate": 4.064602960969044e-05,
      "loss": 0.705,
      "step": 692
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 0.6077665686607361,
      "learning_rate": 4.037685060565276e-05,
      "loss": 1.1595,
      "step": 693
    },
    {
      "epoch": 2.462222222222222,
      "grad_norm": 0.6468228697776794,
      "learning_rate": 4.0107671601615074e-05,
      "loss": 1.4442,
      "step": 694
    },
    {
      "epoch": 2.465777777777778,
      "grad_norm": 0.5027483105659485,
      "learning_rate": 3.9838492597577394e-05,
      "loss": 1.22,
      "step": 695
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 0.8783169984817505,
      "learning_rate": 3.9569313593539706e-05,
      "loss": 1.0827,
      "step": 696
    },
    {
      "epoch": 2.472888888888889,
      "grad_norm": 0.9238305687904358,
      "learning_rate": 3.930013458950202e-05,
      "loss": 0.9915,
      "step": 697
    },
    {
      "epoch": 2.4764444444444447,
      "grad_norm": 0.8111398816108704,
      "learning_rate": 3.903095558546434e-05,
      "loss": 0.9265,
      "step": 698
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.7568382620811462,
      "learning_rate": 3.876177658142665e-05,
      "loss": 1.5233,
      "step": 699
    },
    {
      "epoch": 2.4835555555555557,
      "grad_norm": 0.5574510097503662,
      "learning_rate": 3.849259757738897e-05,
      "loss": 1.0526,
      "step": 700
    },
    {
      "epoch": 2.487111111111111,
      "grad_norm": 0.8741499781608582,
      "learning_rate": 3.822341857335128e-05,
      "loss": 1.21,
      "step": 701
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 1.3598370552062988,
      "learning_rate": 3.7954239569313595e-05,
      "loss": 1.4266,
      "step": 702
    },
    {
      "epoch": 2.494222222222222,
      "grad_norm": 0.814302921295166,
      "learning_rate": 3.768506056527591e-05,
      "loss": 1.1309,
      "step": 703
    },
    {
      "epoch": 2.497777777777778,
      "grad_norm": 0.7900919318199158,
      "learning_rate": 3.741588156123823e-05,
      "loss": 0.8046,
      "step": 704
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 0.49214181303977966,
      "learning_rate": 3.714670255720054e-05,
      "loss": 0.921,
      "step": 705
    },
    {
      "epoch": 2.504888888888889,
      "grad_norm": 0.8783044219017029,
      "learning_rate": 3.687752355316286e-05,
      "loss": 0.9465,
      "step": 706
    },
    {
      "epoch": 2.5084444444444447,
      "grad_norm": 0.5059338808059692,
      "learning_rate": 3.6608344549125165e-05,
      "loss": 1.0174,
      "step": 707
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.5876105427742004,
      "learning_rate": 3.6339165545087484e-05,
      "loss": 1.1276,
      "step": 708
    },
    {
      "epoch": 2.5155555555555553,
      "grad_norm": 0.6117871999740601,
      "learning_rate": 3.60699865410498e-05,
      "loss": 0.895,
      "step": 709
    },
    {
      "epoch": 2.519111111111111,
      "grad_norm": 0.6062193512916565,
      "learning_rate": 3.5800807537012116e-05,
      "loss": 1.0979,
      "step": 710
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.8240861296653748,
      "learning_rate": 3.553162853297443e-05,
      "loss": 0.8612,
      "step": 711
    },
    {
      "epoch": 2.526222222222222,
      "grad_norm": 0.4281996488571167,
      "learning_rate": 3.526244952893675e-05,
      "loss": 1.0919,
      "step": 712
    },
    {
      "epoch": 2.529777777777778,
      "grad_norm": 2.1272664070129395,
      "learning_rate": 3.499327052489906e-05,
      "loss": 0.087,
      "step": 713
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.8144264817237854,
      "learning_rate": 3.472409152086137e-05,
      "loss": 1.2951,
      "step": 714
    },
    {
      "epoch": 2.536888888888889,
      "grad_norm": 0.7677145004272461,
      "learning_rate": 3.445491251682369e-05,
      "loss": 0.9113,
      "step": 715
    },
    {
      "epoch": 2.5404444444444443,
      "grad_norm": 1.6935011148452759,
      "learning_rate": 3.4185733512786005e-05,
      "loss": 1.2791,
      "step": 716
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.7121945023536682,
      "learning_rate": 3.3916554508748324e-05,
      "loss": 1.3483,
      "step": 717
    },
    {
      "epoch": 2.5475555555555554,
      "grad_norm": 1.2756458520889282,
      "learning_rate": 3.364737550471063e-05,
      "loss": 1.2121,
      "step": 718
    },
    {
      "epoch": 2.551111111111111,
      "grad_norm": 0.6290696263313293,
      "learning_rate": 3.337819650067295e-05,
      "loss": 0.9428,
      "step": 719
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 0.6775375008583069,
      "learning_rate": 3.310901749663526e-05,
      "loss": 1.083,
      "step": 720
    },
    {
      "epoch": 2.558222222222222,
      "grad_norm": 0.5681189298629761,
      "learning_rate": 3.283983849259758e-05,
      "loss": 1.1067,
      "step": 721
    },
    {
      "epoch": 2.561777777777778,
      "grad_norm": 0.6536548733711243,
      "learning_rate": 3.2570659488559894e-05,
      "loss": 0.9484,
      "step": 722
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 1.4594826698303223,
      "learning_rate": 3.2301480484522207e-05,
      "loss": 1.6048,
      "step": 723
    },
    {
      "epoch": 2.568888888888889,
      "grad_norm": 0.8063167333602905,
      "learning_rate": 3.203230148048452e-05,
      "loss": 1.0236,
      "step": 724
    },
    {
      "epoch": 2.5724444444444443,
      "grad_norm": 0.7010015249252319,
      "learning_rate": 3.176312247644684e-05,
      "loss": 0.7757,
      "step": 725
    },
    {
      "epoch": 2.576,
      "grad_norm": 1.2765525579452515,
      "learning_rate": 3.149394347240915e-05,
      "loss": 1.1834,
      "step": 726
    },
    {
      "epoch": 2.5795555555555554,
      "grad_norm": 0.9063960909843445,
      "learning_rate": 3.122476446837147e-05,
      "loss": 1.5289,
      "step": 727
    },
    {
      "epoch": 2.583111111111111,
      "grad_norm": 0.6029309034347534,
      "learning_rate": 3.095558546433378e-05,
      "loss": 0.7658,
      "step": 728
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.5800614953041077,
      "learning_rate": 3.0686406460296096e-05,
      "loss": 0.776,
      "step": 729
    },
    {
      "epoch": 2.590222222222222,
      "grad_norm": 0.8008013367652893,
      "learning_rate": 3.041722745625841e-05,
      "loss": 0.9778,
      "step": 730
    },
    {
      "epoch": 2.5937777777777775,
      "grad_norm": 0.7858405709266663,
      "learning_rate": 3.0148048452220727e-05,
      "loss": 0.7195,
      "step": 731
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 0.5686233639717102,
      "learning_rate": 2.9878869448183043e-05,
      "loss": 0.8414,
      "step": 732
    },
    {
      "epoch": 2.600888888888889,
      "grad_norm": 1.2735246419906616,
      "learning_rate": 2.960969044414536e-05,
      "loss": 1.3985,
      "step": 733
    },
    {
      "epoch": 2.6044444444444443,
      "grad_norm": 0.8949160575866699,
      "learning_rate": 2.9340511440107672e-05,
      "loss": 1.271,
      "step": 734
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.7007665634155273,
      "learning_rate": 2.9071332436069988e-05,
      "loss": 0.66,
      "step": 735
    },
    {
      "epoch": 2.6115555555555554,
      "grad_norm": 0.9850322604179382,
      "learning_rate": 2.8802153432032304e-05,
      "loss": 1.2541,
      "step": 736
    },
    {
      "epoch": 2.615111111111111,
      "grad_norm": 0.7119415998458862,
      "learning_rate": 2.853297442799462e-05,
      "loss": 1.2346,
      "step": 737
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.6392675638198853,
      "learning_rate": 2.8263795423956936e-05,
      "loss": 0.7028,
      "step": 738
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 0.7772757411003113,
      "learning_rate": 2.7994616419919245e-05,
      "loss": 1.3737,
      "step": 739
    },
    {
      "epoch": 2.6257777777777775,
      "grad_norm": 0.5648919939994812,
      "learning_rate": 2.772543741588156e-05,
      "loss": 0.9404,
      "step": 740
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 0.6883232593536377,
      "learning_rate": 2.7456258411843877e-05,
      "loss": 0.6826,
      "step": 741
    },
    {
      "epoch": 2.632888888888889,
      "grad_norm": 0.5757709741592407,
      "learning_rate": 2.7187079407806193e-05,
      "loss": 1.1548,
      "step": 742
    },
    {
      "epoch": 2.6364444444444444,
      "grad_norm": 0.7075737714767456,
      "learning_rate": 2.691790040376851e-05,
      "loss": 0.8981,
      "step": 743
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.9107454419136047,
      "learning_rate": 2.6648721399730825e-05,
      "loss": 0.7262,
      "step": 744
    },
    {
      "epoch": 2.6435555555555554,
      "grad_norm": 0.7411884069442749,
      "learning_rate": 2.6379542395693134e-05,
      "loss": 0.843,
      "step": 745
    },
    {
      "epoch": 2.647111111111111,
      "grad_norm": 0.4897245466709137,
      "learning_rate": 2.611036339165545e-05,
      "loss": 0.9924,
      "step": 746
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 0.64869624376297,
      "learning_rate": 2.5841184387617766e-05,
      "loss": 1.1433,
      "step": 747
    },
    {
      "epoch": 2.6542222222222223,
      "grad_norm": 0.5388240218162537,
      "learning_rate": 2.5572005383580082e-05,
      "loss": 0.7729,
      "step": 748
    },
    {
      "epoch": 2.6577777777777776,
      "grad_norm": 0.6816124320030212,
      "learning_rate": 2.5302826379542398e-05,
      "loss": 0.9415,
      "step": 749
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 0.52239990234375,
      "learning_rate": 2.503364737550471e-05,
      "loss": 0.981,
      "step": 750
    },
    {
      "epoch": 2.664888888888889,
      "grad_norm": 0.5644083619117737,
      "learning_rate": 2.4764468371467026e-05,
      "loss": 0.8499,
      "step": 751
    },
    {
      "epoch": 2.6684444444444444,
      "grad_norm": 0.7091593146324158,
      "learning_rate": 2.4495289367429342e-05,
      "loss": 0.9941,
      "step": 752
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.7058376669883728,
      "learning_rate": 2.422611036339166e-05,
      "loss": 0.6174,
      "step": 753
    },
    {
      "epoch": 2.6755555555555555,
      "grad_norm": 0.8242354989051819,
      "learning_rate": 2.395693135935397e-05,
      "loss": 1.0347,
      "step": 754
    },
    {
      "epoch": 2.679111111111111,
      "grad_norm": 0.8554843068122864,
      "learning_rate": 2.3687752355316287e-05,
      "loss": 1.0664,
      "step": 755
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.5348339080810547,
      "learning_rate": 2.3418573351278603e-05,
      "loss": 0.9659,
      "step": 756
    },
    {
      "epoch": 2.6862222222222223,
      "grad_norm": 0.836499810218811,
      "learning_rate": 2.3149394347240915e-05,
      "loss": 1.1375,
      "step": 757
    },
    {
      "epoch": 2.6897777777777776,
      "grad_norm": 1.6289783716201782,
      "learning_rate": 2.288021534320323e-05,
      "loss": 1.3044,
      "step": 758
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 1.4933010339736938,
      "learning_rate": 2.2611036339165544e-05,
      "loss": 1.4986,
      "step": 759
    },
    {
      "epoch": 2.696888888888889,
      "grad_norm": 0.7782571911811829,
      "learning_rate": 2.234185733512786e-05,
      "loss": 1.5855,
      "step": 760
    },
    {
      "epoch": 2.7004444444444444,
      "grad_norm": 0.8753587007522583,
      "learning_rate": 2.2072678331090176e-05,
      "loss": 1.131,
      "step": 761
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.7286129593849182,
      "learning_rate": 2.180349932705249e-05,
      "loss": 1.0185,
      "step": 762
    },
    {
      "epoch": 2.7075555555555555,
      "grad_norm": 0.7392619848251343,
      "learning_rate": 2.1534320323014804e-05,
      "loss": 1.1963,
      "step": 763
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 0.4407792389392853,
      "learning_rate": 2.126514131897712e-05,
      "loss": 0.8965,
      "step": 764
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 1.002292513847351,
      "learning_rate": 2.0995962314939436e-05,
      "loss": 0.8463,
      "step": 765
    },
    {
      "epoch": 2.7182222222222223,
      "grad_norm": 0.5537858009338379,
      "learning_rate": 2.0726783310901752e-05,
      "loss": 0.8005,
      "step": 766
    },
    {
      "epoch": 2.7217777777777776,
      "grad_norm": 0.5640958547592163,
      "learning_rate": 2.0457604306864065e-05,
      "loss": 1.1967,
      "step": 767
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.6727292537689209,
      "learning_rate": 2.018842530282638e-05,
      "loss": 1.0716,
      "step": 768
    },
    {
      "epoch": 2.728888888888889,
      "grad_norm": 0.8394407629966736,
      "learning_rate": 1.9919246298788697e-05,
      "loss": 0.8457,
      "step": 769
    },
    {
      "epoch": 2.7324444444444445,
      "grad_norm": 0.66130530834198,
      "learning_rate": 1.965006729475101e-05,
      "loss": 1.2377,
      "step": 770
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 1.1252813339233398,
      "learning_rate": 1.9380888290713325e-05,
      "loss": 2.0427,
      "step": 771
    },
    {
      "epoch": 2.7395555555555555,
      "grad_norm": 1.351606011390686,
      "learning_rate": 1.911170928667564e-05,
      "loss": 1.8584,
      "step": 772
    },
    {
      "epoch": 2.7431111111111113,
      "grad_norm": 0.6676983833312988,
      "learning_rate": 1.8842530282637954e-05,
      "loss": 1.1087,
      "step": 773
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 1.1542346477508545,
      "learning_rate": 1.857335127860027e-05,
      "loss": 1.538,
      "step": 774
    },
    {
      "epoch": 2.7502222222222223,
      "grad_norm": 0.8398094177246094,
      "learning_rate": 1.8304172274562582e-05,
      "loss": 1.1817,
      "step": 775
    },
    {
      "epoch": 2.7537777777777777,
      "grad_norm": 1.2079085111618042,
      "learning_rate": 1.80349932705249e-05,
      "loss": 1.4352,
      "step": 776
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.631366491317749,
      "learning_rate": 1.7765814266487214e-05,
      "loss": 0.9279,
      "step": 777
    },
    {
      "epoch": 2.7608888888888887,
      "grad_norm": 0.7819586992263794,
      "learning_rate": 1.749663526244953e-05,
      "loss": 0.8171,
      "step": 778
    },
    {
      "epoch": 2.7644444444444445,
      "grad_norm": 0.7748157978057861,
      "learning_rate": 1.7227456258411846e-05,
      "loss": 1.4452,
      "step": 779
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.7168543934822083,
      "learning_rate": 1.6958277254374162e-05,
      "loss": 1.0655,
      "step": 780
    },
    {
      "epoch": 2.7715555555555556,
      "grad_norm": 0.6334177255630493,
      "learning_rate": 1.6689098250336475e-05,
      "loss": 1.2262,
      "step": 781
    },
    {
      "epoch": 2.7751111111111113,
      "grad_norm": 1.4784623384475708,
      "learning_rate": 1.641991924629879e-05,
      "loss": 1.7249,
      "step": 782
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.8411655426025391,
      "learning_rate": 1.6150740242261103e-05,
      "loss": 1.1628,
      "step": 783
    },
    {
      "epoch": 2.7822222222222224,
      "grad_norm": 0.8204237222671509,
      "learning_rate": 1.588156123822342e-05,
      "loss": 1.0326,
      "step": 784
    },
    {
      "epoch": 2.7857777777777777,
      "grad_norm": 1.4753592014312744,
      "learning_rate": 1.5612382234185735e-05,
      "loss": 1.5491,
      "step": 785
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 0.4424305260181427,
      "learning_rate": 1.5343203230148048e-05,
      "loss": 1.0855,
      "step": 786
    },
    {
      "epoch": 2.7928888888888888,
      "grad_norm": 1.0881586074829102,
      "learning_rate": 1.5074024226110364e-05,
      "loss": 1.3345,
      "step": 787
    },
    {
      "epoch": 2.7964444444444445,
      "grad_norm": 0.48822876811027527,
      "learning_rate": 1.480484522207268e-05,
      "loss": 0.8844,
      "step": 788
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.6421193480491638,
      "learning_rate": 1.4535666218034994e-05,
      "loss": 1.0998,
      "step": 789
    },
    {
      "epoch": 2.8035555555555556,
      "grad_norm": 0.8388862013816833,
      "learning_rate": 1.426648721399731e-05,
      "loss": 1.2395,
      "step": 790
    },
    {
      "epoch": 2.8071111111111113,
      "grad_norm": 0.7895914912223816,
      "learning_rate": 1.3997308209959623e-05,
      "loss": 1.1035,
      "step": 791
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 0.5547356605529785,
      "learning_rate": 1.3728129205921938e-05,
      "loss": 0.9302,
      "step": 792
    },
    {
      "epoch": 2.814222222222222,
      "grad_norm": 0.5233210325241089,
      "learning_rate": 1.3458950201884254e-05,
      "loss": 0.7446,
      "step": 793
    },
    {
      "epoch": 2.8177777777777777,
      "grad_norm": 1.92279851436615,
      "learning_rate": 1.3189771197846567e-05,
      "loss": 0.4369,
      "step": 794
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 0.6377646327018738,
      "learning_rate": 1.2920592193808883e-05,
      "loss": 0.8319,
      "step": 795
    },
    {
      "epoch": 2.824888888888889,
      "grad_norm": 1.1489521265029907,
      "learning_rate": 1.2651413189771199e-05,
      "loss": 1.7766,
      "step": 796
    },
    {
      "epoch": 2.8284444444444445,
      "grad_norm": 0.6648159027099609,
      "learning_rate": 1.2382234185733513e-05,
      "loss": 0.9583,
      "step": 797
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.6665043234825134,
      "learning_rate": 1.211305518169583e-05,
      "loss": 1.0741,
      "step": 798
    },
    {
      "epoch": 2.8355555555555556,
      "grad_norm": 0.40367457270622253,
      "learning_rate": 1.1843876177658143e-05,
      "loss": 1.1187,
      "step": 799
    },
    {
      "epoch": 2.8391111111111114,
      "grad_norm": 0.6774334907531738,
      "learning_rate": 1.1574697173620458e-05,
      "loss": 1.1125,
      "step": 800
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 0.4581276774406433,
      "learning_rate": 1.1305518169582772e-05,
      "loss": 1.073,
      "step": 801
    },
    {
      "epoch": 2.846222222222222,
      "grad_norm": 0.6471617221832275,
      "learning_rate": 1.1036339165545088e-05,
      "loss": 1.1462,
      "step": 802
    },
    {
      "epoch": 2.8497777777777777,
      "grad_norm": 0.7770751118659973,
      "learning_rate": 1.0767160161507402e-05,
      "loss": 1.2347,
      "step": 803
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.8824921250343323,
      "learning_rate": 1.0497981157469718e-05,
      "loss": 0.9825,
      "step": 804
    },
    {
      "epoch": 2.856888888888889,
      "grad_norm": 1.1167031526565552,
      "learning_rate": 1.0228802153432032e-05,
      "loss": 1.7942,
      "step": 805
    },
    {
      "epoch": 2.8604444444444446,
      "grad_norm": 1.0315450429916382,
      "learning_rate": 9.959623149394348e-06,
      "loss": 1.8154,
      "step": 806
    },
    {
      "epoch": 2.864,
      "grad_norm": 1.5783082246780396,
      "learning_rate": 9.690444145356663e-06,
      "loss": 1.2291,
      "step": 807
    },
    {
      "epoch": 2.8675555555555556,
      "grad_norm": 0.5732436776161194,
      "learning_rate": 9.421265141318977e-06,
      "loss": 1.0752,
      "step": 808
    },
    {
      "epoch": 2.871111111111111,
      "grad_norm": 1.4812356233596802,
      "learning_rate": 9.152086137281291e-06,
      "loss": 1.9507,
      "step": 809
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.8821057081222534,
      "learning_rate": 8.882907133243607e-06,
      "loss": 0.6514,
      "step": 810
    },
    {
      "epoch": 2.878222222222222,
      "grad_norm": 0.8418892621994019,
      "learning_rate": 8.613728129205923e-06,
      "loss": 1.228,
      "step": 811
    },
    {
      "epoch": 2.8817777777777778,
      "grad_norm": 0.8078755736351013,
      "learning_rate": 8.344549125168237e-06,
      "loss": 1.1887,
      "step": 812
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 1.1020050048828125,
      "learning_rate": 8.075370121130552e-06,
      "loss": 0.9993,
      "step": 813
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.9220108389854431,
      "learning_rate": 7.806191117092868e-06,
      "loss": 0.9458,
      "step": 814
    },
    {
      "epoch": 2.8924444444444446,
      "grad_norm": 1.1472235918045044,
      "learning_rate": 7.537012113055182e-06,
      "loss": 0.9467,
      "step": 815
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.8423529267311096,
      "learning_rate": 7.267833109017497e-06,
      "loss": 1.3311,
      "step": 816
    },
    {
      "epoch": 2.8995555555555557,
      "grad_norm": 0.5875098705291748,
      "learning_rate": 6.998654104979811e-06,
      "loss": 1.1892,
      "step": 817
    },
    {
      "epoch": 2.903111111111111,
      "grad_norm": 0.7035292983055115,
      "learning_rate": 6.729475100942127e-06,
      "loss": 1.0651,
      "step": 818
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.9320534467697144,
      "learning_rate": 6.4602960969044415e-06,
      "loss": 1.3216,
      "step": 819
    },
    {
      "epoch": 2.910222222222222,
      "grad_norm": 0.817031979560852,
      "learning_rate": 6.191117092866757e-06,
      "loss": 1.3079,
      "step": 820
    },
    {
      "epoch": 2.913777777777778,
      "grad_norm": 0.742536723613739,
      "learning_rate": 5.921938088829072e-06,
      "loss": 1.3851,
      "step": 821
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 0.9289960861206055,
      "learning_rate": 5.652759084791386e-06,
      "loss": 1.4553,
      "step": 822
    },
    {
      "epoch": 2.920888888888889,
      "grad_norm": 0.6337317228317261,
      "learning_rate": 5.383580080753701e-06,
      "loss": 1.2465,
      "step": 823
    },
    {
      "epoch": 2.924444444444444,
      "grad_norm": 0.6163718104362488,
      "learning_rate": 5.114401076716016e-06,
      "loss": 1.2114,
      "step": 824
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.819563627243042,
      "learning_rate": 4.845222072678331e-06,
      "loss": 1.4667,
      "step": 825
    },
    {
      "epoch": 2.9315555555555557,
      "grad_norm": 0.8512040376663208,
      "learning_rate": 4.576043068640646e-06,
      "loss": 1.4827,
      "step": 826
    },
    {
      "epoch": 2.935111111111111,
      "grad_norm": 1.1842681169509888,
      "learning_rate": 4.3068640646029616e-06,
      "loss": 0.9069,
      "step": 827
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 0.647922694683075,
      "learning_rate": 4.037685060565276e-06,
      "loss": 0.7306,
      "step": 828
    },
    {
      "epoch": 2.942222222222222,
      "grad_norm": 0.7497511506080627,
      "learning_rate": 3.768506056527591e-06,
      "loss": 0.3976,
      "step": 829
    },
    {
      "epoch": 2.945777777777778,
      "grad_norm": 0.8400673866271973,
      "learning_rate": 3.4993270524899056e-06,
      "loss": 1.2297,
      "step": 830
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 0.5682816505432129,
      "learning_rate": 3.2301480484522207e-06,
      "loss": 1.1489,
      "step": 831
    },
    {
      "epoch": 2.952888888888889,
      "grad_norm": 0.6120803952217102,
      "learning_rate": 2.960969044414536e-06,
      "loss": 1.0724,
      "step": 832
    },
    {
      "epoch": 2.956444444444444,
      "grad_norm": 2.0758049488067627,
      "learning_rate": 2.6917900403768505e-06,
      "loss": 0.8382,
      "step": 833
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.6377274394035339,
      "learning_rate": 2.4226110363391657e-06,
      "loss": 1.1026,
      "step": 834
    },
    {
      "epoch": 2.9635555555555557,
      "grad_norm": 0.8500564098358154,
      "learning_rate": 2.1534320323014808e-06,
      "loss": 0.5329,
      "step": 835
    },
    {
      "epoch": 2.967111111111111,
      "grad_norm": 0.6048608422279358,
      "learning_rate": 1.8842530282637955e-06,
      "loss": 1.2544,
      "step": 836
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 1.221474528312683,
      "learning_rate": 1.6150740242261104e-06,
      "loss": 1.0519,
      "step": 837
    },
    {
      "epoch": 2.974222222222222,
      "grad_norm": 0.7941517233848572,
      "learning_rate": 1.3458950201884253e-06,
      "loss": 1.2416,
      "step": 838
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 0.7208853960037231,
      "learning_rate": 1.0767160161507404e-06,
      "loss": 1.1248,
      "step": 839
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 0.5877748131752014,
      "learning_rate": 8.075370121130552e-07,
      "loss": 1.0614,
      "step": 840
    },
    {
      "epoch": 2.984888888888889,
      "grad_norm": 1.9518003463745117,
      "learning_rate": 5.383580080753702e-07,
      "loss": 1.0315,
      "step": 841
    },
    {
      "epoch": 2.9884444444444442,
      "grad_norm": 0.8810288906097412,
      "learning_rate": 2.691790040376851e-07,
      "loss": 1.4635,
      "step": 842
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.8968720436096191,
      "learning_rate": 0.0,
      "loss": 0.8777,
      "step": 843
    }
  ],
  "logging_steps": 1,
  "max_steps": 843,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0401077727758746e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
