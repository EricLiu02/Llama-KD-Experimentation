{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.775111111111111,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035555555555555557,
      "grad_norm": 1.0496021509170532,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.7189,
      "step": 1
    },
    {
      "epoch": 0.0071111111111111115,
      "grad_norm": 0.699831485748291,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.778,
      "step": 2
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.8934116959571838,
      "learning_rate": 6e-06,
      "loss": 1.7357,
      "step": 3
    },
    {
      "epoch": 0.014222222222222223,
      "grad_norm": 0.7389705777168274,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.9031,
      "step": 4
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 0.6188634634017944,
      "learning_rate": 1e-05,
      "loss": 1.8695,
      "step": 5
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.6671124696731567,
      "learning_rate": 1.2e-05,
      "loss": 1.8539,
      "step": 6
    },
    {
      "epoch": 0.024888888888888887,
      "grad_norm": 0.965324342250824,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.4835,
      "step": 7
    },
    {
      "epoch": 0.028444444444444446,
      "grad_norm": 0.9192386269569397,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.5773,
      "step": 8
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.8280025720596313,
      "learning_rate": 1.8e-05,
      "loss": 1.7221,
      "step": 9
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 0.6331899762153625,
      "learning_rate": 2e-05,
      "loss": 1.8756,
      "step": 10
    },
    {
      "epoch": 0.03911111111111111,
      "grad_norm": 0.6624985337257385,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.7064,
      "step": 11
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.710986316204071,
      "learning_rate": 2.4e-05,
      "loss": 1.6204,
      "step": 12
    },
    {
      "epoch": 0.04622222222222222,
      "grad_norm": 0.8325507044792175,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.6978,
      "step": 13
    },
    {
      "epoch": 0.049777777777777775,
      "grad_norm": 0.9269227385520935,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.1179,
      "step": 14
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.8736143112182617,
      "learning_rate": 3e-05,
      "loss": 1.6852,
      "step": 15
    },
    {
      "epoch": 0.05688888888888889,
      "grad_norm": 1.0716755390167236,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.911,
      "step": 16
    },
    {
      "epoch": 0.060444444444444446,
      "grad_norm": 0.7024404406547546,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.6988,
      "step": 17
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.651321291923523,
      "learning_rate": 3.6e-05,
      "loss": 1.7723,
      "step": 18
    },
    {
      "epoch": 0.06755555555555555,
      "grad_norm": 0.6992681622505188,
      "learning_rate": 3.8e-05,
      "loss": 1.7597,
      "step": 19
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 0.8356372714042664,
      "learning_rate": 4e-05,
      "loss": 1.6335,
      "step": 20
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.775214433670044,
      "learning_rate": 4.2e-05,
      "loss": 1.6665,
      "step": 21
    },
    {
      "epoch": 0.07822222222222222,
      "grad_norm": 0.6859831213951111,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.5595,
      "step": 22
    },
    {
      "epoch": 0.08177777777777778,
      "grad_norm": 0.8604557514190674,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.3867,
      "step": 23
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.7105897068977356,
      "learning_rate": 4.8e-05,
      "loss": 1.6644,
      "step": 24
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.5910735726356506,
      "learning_rate": 5e-05,
      "loss": 1.6435,
      "step": 25
    },
    {
      "epoch": 0.09244444444444444,
      "grad_norm": 0.6152008175849915,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 1.7192,
      "step": 26
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.6209008693695068,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 1.7143,
      "step": 27
    },
    {
      "epoch": 0.09955555555555555,
      "grad_norm": 2.035393476486206,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 1.7844,
      "step": 28
    },
    {
      "epoch": 0.10311111111111111,
      "grad_norm": 0.5755760669708252,
      "learning_rate": 5.8e-05,
      "loss": 1.5945,
      "step": 29
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.5616899132728577,
      "learning_rate": 6e-05,
      "loss": 1.7377,
      "step": 30
    },
    {
      "epoch": 0.11022222222222222,
      "grad_norm": 2.078535556793213,
      "learning_rate": 6.2e-05,
      "loss": 0.5414,
      "step": 31
    },
    {
      "epoch": 0.11377777777777778,
      "grad_norm": 0.6818261742591858,
      "learning_rate": 6.400000000000001e-05,
      "loss": 1.2583,
      "step": 32
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 2.012356758117676,
      "learning_rate": 6.6e-05,
      "loss": 1.528,
      "step": 33
    },
    {
      "epoch": 0.12088888888888889,
      "grad_norm": 0.679358184337616,
      "learning_rate": 6.800000000000001e-05,
      "loss": 1.8343,
      "step": 34
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 0.8316290378570557,
      "learning_rate": 7e-05,
      "loss": 1.1916,
      "step": 35
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.946313738822937,
      "learning_rate": 7.2e-05,
      "loss": 1.2201,
      "step": 36
    },
    {
      "epoch": 0.13155555555555556,
      "grad_norm": 0.8514885306358337,
      "learning_rate": 7.4e-05,
      "loss": 1.3534,
      "step": 37
    },
    {
      "epoch": 0.1351111111111111,
      "grad_norm": 0.6170944571495056,
      "learning_rate": 7.6e-05,
      "loss": 1.2827,
      "step": 38
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.790472149848938,
      "learning_rate": 7.800000000000001e-05,
      "loss": 1.4393,
      "step": 39
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 0.8437612056732178,
      "learning_rate": 8e-05,
      "loss": 1.3528,
      "step": 40
    },
    {
      "epoch": 0.14577777777777778,
      "grad_norm": 0.7593702077865601,
      "learning_rate": 8.2e-05,
      "loss": 1.3864,
      "step": 41
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.7103708386421204,
      "learning_rate": 8.4e-05,
      "loss": 1.399,
      "step": 42
    },
    {
      "epoch": 0.15288888888888888,
      "grad_norm": 0.9441720843315125,
      "learning_rate": 8.6e-05,
      "loss": 1.1639,
      "step": 43
    },
    {
      "epoch": 0.15644444444444444,
      "grad_norm": 0.802440881729126,
      "learning_rate": 8.800000000000001e-05,
      "loss": 1.5949,
      "step": 44
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.0494697093963623,
      "learning_rate": 9e-05,
      "loss": 1.1386,
      "step": 45
    },
    {
      "epoch": 0.16355555555555557,
      "grad_norm": 1.2716161012649536,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.6154,
      "step": 46
    },
    {
      "epoch": 0.1671111111111111,
      "grad_norm": 0.8360788226127625,
      "learning_rate": 9.4e-05,
      "loss": 1.3078,
      "step": 47
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.85890132188797,
      "learning_rate": 9.6e-05,
      "loss": 1.4567,
      "step": 48
    },
    {
      "epoch": 0.17422222222222222,
      "grad_norm": 0.9399837851524353,
      "learning_rate": 9.8e-05,
      "loss": 1.5402,
      "step": 49
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.0960140228271484,
      "learning_rate": 0.0001,
      "loss": 1.1925,
      "step": 50
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 1.0918102264404297,
      "learning_rate": 0.00010200000000000001,
      "loss": 1.1583,
      "step": 51
    },
    {
      "epoch": 0.18488888888888888,
      "grad_norm": 1.0110020637512207,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.9806,
      "step": 52
    },
    {
      "epoch": 0.18844444444444444,
      "grad_norm": 1.5649272203445435,
      "learning_rate": 0.00010600000000000002,
      "loss": 1.6915,
      "step": 53
    },
    {
      "epoch": 0.192,
      "grad_norm": 1.0496711730957031,
      "learning_rate": 0.00010800000000000001,
      "loss": 1.3562,
      "step": 54
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 1.140816330909729,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.9786,
      "step": 55
    },
    {
      "epoch": 0.1991111111111111,
      "grad_norm": 1.2262437343597412,
      "learning_rate": 0.00011200000000000001,
      "loss": 1.483,
      "step": 56
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.862920343875885,
      "learning_rate": 0.00011399999999999999,
      "loss": 1.2957,
      "step": 57
    },
    {
      "epoch": 0.20622222222222222,
      "grad_norm": 1.1915334463119507,
      "learning_rate": 0.000116,
      "loss": 1.5575,
      "step": 58
    },
    {
      "epoch": 0.20977777777777779,
      "grad_norm": 1.3480455875396729,
      "learning_rate": 0.000118,
      "loss": 0.8594,
      "step": 59
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 1.058949589729309,
      "learning_rate": 0.00012,
      "loss": 1.1435,
      "step": 60
    },
    {
      "epoch": 0.21688888888888888,
      "grad_norm": 0.8377956748008728,
      "learning_rate": 0.000122,
      "loss": 1.1198,
      "step": 61
    },
    {
      "epoch": 0.22044444444444444,
      "grad_norm": 0.7277732491493225,
      "learning_rate": 0.000124,
      "loss": 1.0566,
      "step": 62
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.7248883843421936,
      "learning_rate": 0.000126,
      "loss": 1.1656,
      "step": 63
    },
    {
      "epoch": 0.22755555555555557,
      "grad_norm": 0.8512604832649231,
      "learning_rate": 0.00012800000000000002,
      "loss": 1.0637,
      "step": 64
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 0.6806027889251709,
      "learning_rate": 0.00013000000000000002,
      "loss": 1.1492,
      "step": 65
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.8843218088150024,
      "learning_rate": 0.000132,
      "loss": 1.1306,
      "step": 66
    },
    {
      "epoch": 0.23822222222222222,
      "grad_norm": 0.7718207836151123,
      "learning_rate": 0.000134,
      "loss": 1.2706,
      "step": 67
    },
    {
      "epoch": 0.24177777777777779,
      "grad_norm": 0.6748335361480713,
      "learning_rate": 0.00013600000000000003,
      "loss": 1.302,
      "step": 68
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.5956426858901978,
      "learning_rate": 0.000138,
      "loss": 1.0723,
      "step": 69
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 0.6580986380577087,
      "learning_rate": 0.00014,
      "loss": 0.9885,
      "step": 70
    },
    {
      "epoch": 0.25244444444444447,
      "grad_norm": 1.5414533615112305,
      "learning_rate": 0.000142,
      "loss": 0.8492,
      "step": 71
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.5246608853340149,
      "learning_rate": 0.000144,
      "loss": 1.0715,
      "step": 72
    },
    {
      "epoch": 0.25955555555555554,
      "grad_norm": 0.4837822914123535,
      "learning_rate": 0.000146,
      "loss": 1.1721,
      "step": 73
    },
    {
      "epoch": 0.26311111111111113,
      "grad_norm": 0.7696661949157715,
      "learning_rate": 0.000148,
      "loss": 1.4931,
      "step": 74
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.4808294475078583,
      "learning_rate": 0.00015000000000000001,
      "loss": 1.2506,
      "step": 75
    },
    {
      "epoch": 0.2702222222222222,
      "grad_norm": 0.8885055780410767,
      "learning_rate": 0.000152,
      "loss": 1.3189,
      "step": 76
    },
    {
      "epoch": 0.2737777777777778,
      "grad_norm": 0.6306704878807068,
      "learning_rate": 0.000154,
      "loss": 1.3816,
      "step": 77
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.7764801979064941,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.7238,
      "step": 78
    },
    {
      "epoch": 0.2808888888888889,
      "grad_norm": 0.8956315517425537,
      "learning_rate": 0.00015800000000000002,
      "loss": 1.5646,
      "step": 79
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 0.8287359476089478,
      "learning_rate": 0.00016,
      "loss": 1.0724,
      "step": 80
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.9519221186637878,
      "learning_rate": 0.000162,
      "loss": 0.9803,
      "step": 81
    },
    {
      "epoch": 0.29155555555555557,
      "grad_norm": 0.628054141998291,
      "learning_rate": 0.000164,
      "loss": 1.1232,
      "step": 82
    },
    {
      "epoch": 0.2951111111111111,
      "grad_norm": 0.4854539930820465,
      "learning_rate": 0.000166,
      "loss": 1.1147,
      "step": 83
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.8282666802406311,
      "learning_rate": 0.000168,
      "loss": 1.3202,
      "step": 84
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 0.96560138463974,
      "learning_rate": 0.00017,
      "loss": 1.4498,
      "step": 85
    },
    {
      "epoch": 0.30577777777777776,
      "grad_norm": 0.5461382865905762,
      "learning_rate": 0.000172,
      "loss": 1.3838,
      "step": 86
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 1.6854358911514282,
      "learning_rate": 0.000174,
      "loss": 1.5109,
      "step": 87
    },
    {
      "epoch": 0.3128888888888889,
      "grad_norm": 0.8593328595161438,
      "learning_rate": 0.00017600000000000002,
      "loss": 1.2901,
      "step": 88
    },
    {
      "epoch": 0.3164444444444444,
      "grad_norm": 0.4510578513145447,
      "learning_rate": 0.00017800000000000002,
      "loss": 1.1522,
      "step": 89
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.2090189456939697,
      "learning_rate": 0.00018,
      "loss": 2.008,
      "step": 90
    },
    {
      "epoch": 0.32355555555555554,
      "grad_norm": 0.7407180070877075,
      "learning_rate": 0.000182,
      "loss": 1.0654,
      "step": 91
    },
    {
      "epoch": 0.32711111111111113,
      "grad_norm": 0.8816343545913696,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.1722,
      "step": 92
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.6926984786987305,
      "learning_rate": 0.00018600000000000002,
      "loss": 1.2231,
      "step": 93
    },
    {
      "epoch": 0.3342222222222222,
      "grad_norm": 3.6535544395446777,
      "learning_rate": 0.000188,
      "loss": 0.2878,
      "step": 94
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 0.6837925910949707,
      "learning_rate": 0.00019,
      "loss": 1.2099,
      "step": 95
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 1.5838969945907593,
      "learning_rate": 0.000192,
      "loss": 0.9175,
      "step": 96
    },
    {
      "epoch": 0.3448888888888889,
      "grad_norm": 0.9290364980697632,
      "learning_rate": 0.000194,
      "loss": 0.6948,
      "step": 97
    },
    {
      "epoch": 0.34844444444444445,
      "grad_norm": 1.2045925855636597,
      "learning_rate": 0.000196,
      "loss": 1.0607,
      "step": 98
    },
    {
      "epoch": 0.352,
      "grad_norm": 1.2093307971954346,
      "learning_rate": 0.00019800000000000002,
      "loss": 0.6564,
      "step": 99
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 1.350990653038025,
      "learning_rate": 0.0002,
      "loss": 1.848,
      "step": 100
    },
    {
      "epoch": 0.3591111111111111,
      "grad_norm": 0.4330463707447052,
      "learning_rate": 0.0001997308209959623,
      "loss": 0.9301,
      "step": 101
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 2.0327327251434326,
      "learning_rate": 0.00019946164199192463,
      "loss": 1.5023,
      "step": 102
    },
    {
      "epoch": 0.3662222222222222,
      "grad_norm": 0.577812910079956,
      "learning_rate": 0.00019919246298788696,
      "loss": 1.0138,
      "step": 103
    },
    {
      "epoch": 0.36977777777777776,
      "grad_norm": 0.43015825748443604,
      "learning_rate": 0.0001989232839838493,
      "loss": 0.7905,
      "step": 104
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.7396837472915649,
      "learning_rate": 0.00019865410497981159,
      "loss": 0.6367,
      "step": 105
    },
    {
      "epoch": 0.3768888888888889,
      "grad_norm": 0.5012692213058472,
      "learning_rate": 0.00019838492597577389,
      "loss": 1.348,
      "step": 106
    },
    {
      "epoch": 0.3804444444444444,
      "grad_norm": 0.6901695728302002,
      "learning_rate": 0.0001981157469717362,
      "loss": 1.7483,
      "step": 107
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.6713254451751709,
      "learning_rate": 0.00019784656796769854,
      "loss": 1.1905,
      "step": 108
    },
    {
      "epoch": 0.38755555555555554,
      "grad_norm": 0.7102264165878296,
      "learning_rate": 0.00019757738896366084,
      "loss": 1.5769,
      "step": 109
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 1.30527925491333,
      "learning_rate": 0.00019730820995962316,
      "loss": 1.8035,
      "step": 110
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.8422040343284607,
      "learning_rate": 0.00019703903095558546,
      "loss": 1.3623,
      "step": 111
    },
    {
      "epoch": 0.3982222222222222,
      "grad_norm": 0.5617761611938477,
      "learning_rate": 0.0001967698519515478,
      "loss": 1.1446,
      "step": 112
    },
    {
      "epoch": 0.4017777777777778,
      "grad_norm": 1.1235250234603882,
      "learning_rate": 0.00019650067294751011,
      "loss": 2.1151,
      "step": 113
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.9642476439476013,
      "learning_rate": 0.0001962314939434724,
      "loss": 1.3684,
      "step": 114
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 0.6631832122802734,
      "learning_rate": 0.00019596231493943474,
      "loss": 1.2366,
      "step": 115
    },
    {
      "epoch": 0.41244444444444445,
      "grad_norm": 0.5110630989074707,
      "learning_rate": 0.00019569313593539704,
      "loss": 1.2201,
      "step": 116
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.48804789781570435,
      "learning_rate": 0.00019542395693135936,
      "loss": 1.3454,
      "step": 117
    },
    {
      "epoch": 0.41955555555555557,
      "grad_norm": 1.5733678340911865,
      "learning_rate": 0.0001951547779273217,
      "loss": 1.0821,
      "step": 118
    },
    {
      "epoch": 0.4231111111111111,
      "grad_norm": 0.7834073901176453,
      "learning_rate": 0.000194885598923284,
      "loss": 1.4193,
      "step": 119
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.5930349826812744,
      "learning_rate": 0.00019461641991924632,
      "loss": 1.1611,
      "step": 120
    },
    {
      "epoch": 0.43022222222222223,
      "grad_norm": 0.43651703000068665,
      "learning_rate": 0.00019434724091520861,
      "loss": 0.9157,
      "step": 121
    },
    {
      "epoch": 0.43377777777777776,
      "grad_norm": 0.8871474862098694,
      "learning_rate": 0.00019407806191117094,
      "loss": 0.7413,
      "step": 122
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.49216988682746887,
      "learning_rate": 0.00019380888290713327,
      "loss": 1.3435,
      "step": 123
    },
    {
      "epoch": 0.4408888888888889,
      "grad_norm": 1.3751505613327026,
      "learning_rate": 0.00019353970390309557,
      "loss": 1.6574,
      "step": 124
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.5222814679145813,
      "learning_rate": 0.0001932705248990579,
      "loss": 1.008,
      "step": 125
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.5151017904281616,
      "learning_rate": 0.0001930013458950202,
      "loss": 1.2464,
      "step": 126
    },
    {
      "epoch": 0.45155555555555554,
      "grad_norm": 0.5718678832054138,
      "learning_rate": 0.0001927321668909825,
      "loss": 0.9871,
      "step": 127
    },
    {
      "epoch": 0.45511111111111113,
      "grad_norm": 1.23574697971344,
      "learning_rate": 0.00019246298788694484,
      "loss": 2.0865,
      "step": 128
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.45002615451812744,
      "learning_rate": 0.00019219380888290714,
      "loss": 1.5408,
      "step": 129
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 0.7219682335853577,
      "learning_rate": 0.00019192462987886947,
      "loss": 1.0868,
      "step": 130
    },
    {
      "epoch": 0.4657777777777778,
      "grad_norm": 0.6695947051048279,
      "learning_rate": 0.00019165545087483177,
      "loss": 1.4775,
      "step": 131
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.6133909821510315,
      "learning_rate": 0.0001913862718707941,
      "loss": 0.7883,
      "step": 132
    },
    {
      "epoch": 0.4728888888888889,
      "grad_norm": 0.5814960598945618,
      "learning_rate": 0.00019111709286675642,
      "loss": 1.2102,
      "step": 133
    },
    {
      "epoch": 0.47644444444444445,
      "grad_norm": 0.5922862887382507,
      "learning_rate": 0.00019084791386271872,
      "loss": 0.997,
      "step": 134
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.5335346460342407,
      "learning_rate": 0.00019057873485868102,
      "loss": 0.7855,
      "step": 135
    },
    {
      "epoch": 0.48355555555555557,
      "grad_norm": 0.5321141481399536,
      "learning_rate": 0.00019030955585464334,
      "loss": 1.1751,
      "step": 136
    },
    {
      "epoch": 0.4871111111111111,
      "grad_norm": 0.5106782913208008,
      "learning_rate": 0.00019004037685060567,
      "loss": 1.1227,
      "step": 137
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.5629293918609619,
      "learning_rate": 0.000189771197846568,
      "loss": 1.3005,
      "step": 138
    },
    {
      "epoch": 0.49422222222222223,
      "grad_norm": 0.378921777009964,
      "learning_rate": 0.0001895020188425303,
      "loss": 1.0884,
      "step": 139
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 0.8850380182266235,
      "learning_rate": 0.0001892328398384926,
      "loss": 1.192,
      "step": 140
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.572763204574585,
      "learning_rate": 0.00018896366083445492,
      "loss": 0.6845,
      "step": 141
    },
    {
      "epoch": 0.5048888888888889,
      "grad_norm": 0.584597647190094,
      "learning_rate": 0.00018869448183041725,
      "loss": 0.9947,
      "step": 142
    },
    {
      "epoch": 0.5084444444444445,
      "grad_norm": 0.4791162610054016,
      "learning_rate": 0.00018842530282637955,
      "loss": 1.2347,
      "step": 143
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.6760646104812622,
      "learning_rate": 0.00018815612382234187,
      "loss": 1.2606,
      "step": 144
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 0.6713061332702637,
      "learning_rate": 0.00018788694481830417,
      "loss": 1.2867,
      "step": 145
    },
    {
      "epoch": 0.5191111111111111,
      "grad_norm": 2.5703744888305664,
      "learning_rate": 0.0001876177658142665,
      "loss": 0.4341,
      "step": 146
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.7214426398277283,
      "learning_rate": 0.00018734858681022882,
      "loss": 1.1408,
      "step": 147
    },
    {
      "epoch": 0.5262222222222223,
      "grad_norm": 0.5359913110733032,
      "learning_rate": 0.00018707940780619112,
      "loss": 1.1818,
      "step": 148
    },
    {
      "epoch": 0.5297777777777778,
      "grad_norm": 0.48528075218200684,
      "learning_rate": 0.00018681022880215345,
      "loss": 1.3345,
      "step": 149
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.6618868112564087,
      "learning_rate": 0.00018654104979811575,
      "loss": 1.4868,
      "step": 150
    },
    {
      "epoch": 0.5368888888888889,
      "grad_norm": 0.5592797994613647,
      "learning_rate": 0.00018627187079407807,
      "loss": 1.5118,
      "step": 151
    },
    {
      "epoch": 0.5404444444444444,
      "grad_norm": 0.7681078314781189,
      "learning_rate": 0.0001860026917900404,
      "loss": 1.184,
      "step": 152
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.7294600605964661,
      "learning_rate": 0.0001857335127860027,
      "loss": 1.1922,
      "step": 153
    },
    {
      "epoch": 0.5475555555555556,
      "grad_norm": 0.5436679124832153,
      "learning_rate": 0.00018546433378196502,
      "loss": 1.0991,
      "step": 154
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 0.5317585468292236,
      "learning_rate": 0.00018519515477792732,
      "loss": 1.0717,
      "step": 155
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 1.3958908319473267,
      "learning_rate": 0.00018492597577388965,
      "loss": 0.9477,
      "step": 156
    },
    {
      "epoch": 0.5582222222222222,
      "grad_norm": 0.4654548764228821,
      "learning_rate": 0.00018465679676985198,
      "loss": 1.5322,
      "step": 157
    },
    {
      "epoch": 0.5617777777777778,
      "grad_norm": 1.3589075803756714,
      "learning_rate": 0.00018438761776581427,
      "loss": 1.1833,
      "step": 158
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 1.261024832725525,
      "learning_rate": 0.0001841184387617766,
      "loss": 0.9546,
      "step": 159
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 0.3779989182949066,
      "learning_rate": 0.0001838492597577389,
      "loss": 1.0497,
      "step": 160
    },
    {
      "epoch": 0.5724444444444444,
      "grad_norm": 0.5307259559631348,
      "learning_rate": 0.0001835800807537012,
      "loss": 1.1049,
      "step": 161
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.461746484041214,
      "learning_rate": 0.00018331090174966355,
      "loss": 0.9394,
      "step": 162
    },
    {
      "epoch": 0.5795555555555556,
      "grad_norm": 0.3630843460559845,
      "learning_rate": 0.00018304172274562585,
      "loss": 1.1587,
      "step": 163
    },
    {
      "epoch": 0.5831111111111111,
      "grad_norm": 1.5686595439910889,
      "learning_rate": 0.00018277254374158818,
      "loss": 1.1538,
      "step": 164
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.6291707158088684,
      "learning_rate": 0.00018250336473755048,
      "loss": 1.2383,
      "step": 165
    },
    {
      "epoch": 0.5902222222222222,
      "grad_norm": 0.7741327881813049,
      "learning_rate": 0.00018223418573351277,
      "loss": 1.0396,
      "step": 166
    },
    {
      "epoch": 0.5937777777777777,
      "grad_norm": 0.5122392177581787,
      "learning_rate": 0.00018196500672947513,
      "loss": 0.7663,
      "step": 167
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.5217849612236023,
      "learning_rate": 0.00018169582772543743,
      "loss": 1.222,
      "step": 168
    },
    {
      "epoch": 0.6008888888888889,
      "grad_norm": 0.6501277089118958,
      "learning_rate": 0.00018142664872139973,
      "loss": 0.9019,
      "step": 169
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 0.4106917679309845,
      "learning_rate": 0.00018115746971736205,
      "loss": 1.2087,
      "step": 170
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.4611952006816864,
      "learning_rate": 0.00018088829071332435,
      "loss": 0.9025,
      "step": 171
    },
    {
      "epoch": 0.6115555555555555,
      "grad_norm": 2.2620315551757812,
      "learning_rate": 0.0001806191117092867,
      "loss": 1.055,
      "step": 172
    },
    {
      "epoch": 0.6151111111111112,
      "grad_norm": 0.5724611282348633,
      "learning_rate": 0.000180349932705249,
      "loss": 1.589,
      "step": 173
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.6973462104797363,
      "learning_rate": 0.0001800807537012113,
      "loss": 1.4342,
      "step": 174
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.6673855781555176,
      "learning_rate": 0.00017981157469717363,
      "loss": 1.3324,
      "step": 175
    },
    {
      "epoch": 0.6257777777777778,
      "grad_norm": 0.9670286178588867,
      "learning_rate": 0.00017954239569313593,
      "loss": 1.1528,
      "step": 176
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.7826244235038757,
      "learning_rate": 0.00017927321668909825,
      "loss": 1.3613,
      "step": 177
    },
    {
      "epoch": 0.6328888888888888,
      "grad_norm": 0.5583764314651489,
      "learning_rate": 0.00017900403768506058,
      "loss": 1.2596,
      "step": 178
    },
    {
      "epoch": 0.6364444444444445,
      "grad_norm": 1.4546701908111572,
      "learning_rate": 0.00017873485868102288,
      "loss": 1.1234,
      "step": 179
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.514494001865387,
      "learning_rate": 0.0001784656796769852,
      "loss": 0.9313,
      "step": 180
    },
    {
      "epoch": 0.6435555555555555,
      "grad_norm": 0.9494575262069702,
      "learning_rate": 0.0001781965006729475,
      "loss": 2.3181,
      "step": 181
    },
    {
      "epoch": 0.6471111111111111,
      "grad_norm": 0.6291656494140625,
      "learning_rate": 0.00017792732166890983,
      "loss": 0.9611,
      "step": 182
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 1.0900291204452515,
      "learning_rate": 0.00017765814266487216,
      "loss": 2.2243,
      "step": 183
    },
    {
      "epoch": 0.6542222222222223,
      "grad_norm": 1.7421339750289917,
      "learning_rate": 0.00017738896366083446,
      "loss": 1.5088,
      "step": 184
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 1.1131839752197266,
      "learning_rate": 0.00017711978465679678,
      "loss": 1.243,
      "step": 185
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.9183111786842346,
      "learning_rate": 0.00017685060565275908,
      "loss": 1.671,
      "step": 186
    },
    {
      "epoch": 0.6648888888888889,
      "grad_norm": 1.0032020807266235,
      "learning_rate": 0.0001765814266487214,
      "loss": 1.1944,
      "step": 187
    },
    {
      "epoch": 0.6684444444444444,
      "grad_norm": 0.5245067477226257,
      "learning_rate": 0.00017631224764468373,
      "loss": 1.1521,
      "step": 188
    },
    {
      "epoch": 0.672,
      "grad_norm": 1.6315789222717285,
      "learning_rate": 0.00017604306864064603,
      "loss": 0.8746,
      "step": 189
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 0.7185429930686951,
      "learning_rate": 0.00017577388963660836,
      "loss": 1.2791,
      "step": 190
    },
    {
      "epoch": 0.6791111111111111,
      "grad_norm": 0.5553684234619141,
      "learning_rate": 0.00017550471063257066,
      "loss": 1.2469,
      "step": 191
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.6155862212181091,
      "learning_rate": 0.00017523553162853298,
      "loss": 0.9643,
      "step": 192
    },
    {
      "epoch": 0.6862222222222222,
      "grad_norm": 0.5407004356384277,
      "learning_rate": 0.0001749663526244953,
      "loss": 1.1121,
      "step": 193
    },
    {
      "epoch": 0.6897777777777778,
      "grad_norm": 0.8713154792785645,
      "learning_rate": 0.0001746971736204576,
      "loss": 1.3437,
      "step": 194
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.35754144191741943,
      "learning_rate": 0.0001744279946164199,
      "loss": 1.3003,
      "step": 195
    },
    {
      "epoch": 0.6968888888888889,
      "grad_norm": 0.5295566916465759,
      "learning_rate": 0.00017415881561238226,
      "loss": 1.0948,
      "step": 196
    },
    {
      "epoch": 0.7004444444444444,
      "grad_norm": 0.5680763721466064,
      "learning_rate": 0.00017388963660834456,
      "loss": 0.873,
      "step": 197
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.5312590599060059,
      "learning_rate": 0.00017362045760430689,
      "loss": 1.3737,
      "step": 198
    },
    {
      "epoch": 0.7075555555555556,
      "grad_norm": 0.4810865521430969,
      "learning_rate": 0.00017335127860026918,
      "loss": 1.034,
      "step": 199
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.6006820201873779,
      "learning_rate": 0.00017308209959623148,
      "loss": 1.4625,
      "step": 200
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.5798536539077759,
      "learning_rate": 0.00017281292059219384,
      "loss": 0.9986,
      "step": 201
    },
    {
      "epoch": 0.7182222222222222,
      "grad_norm": 0.38229236006736755,
      "learning_rate": 0.00017254374158815614,
      "loss": 0.8592,
      "step": 202
    },
    {
      "epoch": 0.7217777777777777,
      "grad_norm": 0.5694853067398071,
      "learning_rate": 0.00017227456258411844,
      "loss": 1.5698,
      "step": 203
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.5129810571670532,
      "learning_rate": 0.00017200538358008076,
      "loss": 1.137,
      "step": 204
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 0.6529664397239685,
      "learning_rate": 0.00017173620457604306,
      "loss": 1.3738,
      "step": 205
    },
    {
      "epoch": 0.7324444444444445,
      "grad_norm": 0.6725929975509644,
      "learning_rate": 0.0001714670255720054,
      "loss": 0.9657,
      "step": 206
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.5176121592521667,
      "learning_rate": 0.0001711978465679677,
      "loss": 1.3465,
      "step": 207
    },
    {
      "epoch": 0.7395555555555555,
      "grad_norm": 0.6820777654647827,
      "learning_rate": 0.00017092866756393,
      "loss": 1.3023,
      "step": 208
    },
    {
      "epoch": 0.7431111111111111,
      "grad_norm": 0.6149845123291016,
      "learning_rate": 0.00017065948855989234,
      "loss": 0.9118,
      "step": 209
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.8230158686637878,
      "learning_rate": 0.00017039030955585464,
      "loss": 0.5452,
      "step": 210
    },
    {
      "epoch": 0.7502222222222222,
      "grad_norm": 0.8835077881813049,
      "learning_rate": 0.00017012113055181696,
      "loss": 1.7679,
      "step": 211
    },
    {
      "epoch": 0.7537777777777778,
      "grad_norm": 0.8833909034729004,
      "learning_rate": 0.0001698519515477793,
      "loss": 1.1987,
      "step": 212
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.566779613494873,
      "learning_rate": 0.0001695827725437416,
      "loss": 1.2695,
      "step": 213
    },
    {
      "epoch": 0.7608888888888888,
      "grad_norm": 0.5161460638046265,
      "learning_rate": 0.00016931359353970391,
      "loss": 1.1229,
      "step": 214
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 0.6026802062988281,
      "learning_rate": 0.0001690444145356662,
      "loss": 0.8391,
      "step": 215
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.41746753454208374,
      "learning_rate": 0.00016877523553162854,
      "loss": 0.8475,
      "step": 216
    },
    {
      "epoch": 0.7715555555555556,
      "grad_norm": 0.468995064496994,
      "learning_rate": 0.00016850605652759087,
      "loss": 1.1433,
      "step": 217
    },
    {
      "epoch": 0.7751111111111111,
      "grad_norm": 0.7184010148048401,
      "learning_rate": 0.00016823687752355316,
      "loss": 1.1225,
      "step": 218
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.7040696740150452,
      "learning_rate": 0.0001679676985195155,
      "loss": 0.8035,
      "step": 219
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 0.4247884452342987,
      "learning_rate": 0.0001676985195154778,
      "loss": 1.0806,
      "step": 220
    },
    {
      "epoch": 0.7857777777777778,
      "grad_norm": 0.48878809809684753,
      "learning_rate": 0.00016742934051144012,
      "loss": 1.2708,
      "step": 221
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.3975940942764282,
      "learning_rate": 0.00016716016150740244,
      "loss": 1.1185,
      "step": 222
    },
    {
      "epoch": 0.7928888888888889,
      "grad_norm": 0.47210660576820374,
      "learning_rate": 0.00016689098250336474,
      "loss": 1.106,
      "step": 223
    },
    {
      "epoch": 0.7964444444444444,
      "grad_norm": 0.5187931060791016,
      "learning_rate": 0.00016662180349932707,
      "loss": 1.0283,
      "step": 224
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.5324670076370239,
      "learning_rate": 0.00016635262449528937,
      "loss": 1.2222,
      "step": 225
    },
    {
      "epoch": 0.8035555555555556,
      "grad_norm": 0.5222822427749634,
      "learning_rate": 0.0001660834454912517,
      "loss": 1.2133,
      "step": 226
    },
    {
      "epoch": 0.8071111111111111,
      "grad_norm": 0.46047264337539673,
      "learning_rate": 0.00016581426648721402,
      "loss": 1.1059,
      "step": 227
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.644741952419281,
      "learning_rate": 0.00016554508748317632,
      "loss": 1.3312,
      "step": 228
    },
    {
      "epoch": 0.8142222222222222,
      "grad_norm": 1.013222575187683,
      "learning_rate": 0.00016527590847913864,
      "loss": 1.5379,
      "step": 229
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 0.7131029963493347,
      "learning_rate": 0.00016500672947510094,
      "loss": 0.794,
      "step": 230
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.6251517534255981,
      "learning_rate": 0.00016473755047106327,
      "loss": 1.009,
      "step": 231
    },
    {
      "epoch": 0.8248888888888889,
      "grad_norm": 0.5318983197212219,
      "learning_rate": 0.0001644683714670256,
      "loss": 1.1121,
      "step": 232
    },
    {
      "epoch": 0.8284444444444444,
      "grad_norm": 1.289717435836792,
      "learning_rate": 0.0001641991924629879,
      "loss": 1.2509,
      "step": 233
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.3509086072444916,
      "learning_rate": 0.0001639300134589502,
      "loss": 1.0111,
      "step": 234
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 0.6093185544013977,
      "learning_rate": 0.00016366083445491252,
      "loss": 1.5503,
      "step": 235
    },
    {
      "epoch": 0.8391111111111111,
      "grad_norm": 0.3081625998020172,
      "learning_rate": 0.00016339165545087484,
      "loss": 0.9779,
      "step": 236
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.5099732875823975,
      "learning_rate": 0.00016312247644683717,
      "loss": 1.2745,
      "step": 237
    },
    {
      "epoch": 0.8462222222222222,
      "grad_norm": 0.4560704529285431,
      "learning_rate": 0.00016285329744279947,
      "loss": 1.3095,
      "step": 238
    },
    {
      "epoch": 0.8497777777777777,
      "grad_norm": 0.6672989726066589,
      "learning_rate": 0.00016258411843876177,
      "loss": 1.3676,
      "step": 239
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.9427176713943481,
      "learning_rate": 0.0001623149394347241,
      "loss": 1.8367,
      "step": 240
    },
    {
      "epoch": 0.8568888888888889,
      "grad_norm": 0.45965632796287537,
      "learning_rate": 0.00016204576043068642,
      "loss": 1.3296,
      "step": 241
    },
    {
      "epoch": 0.8604444444444445,
      "grad_norm": 0.5767579674720764,
      "learning_rate": 0.00016177658142664872,
      "loss": 0.9071,
      "step": 242
    },
    {
      "epoch": 0.864,
      "grad_norm": 1.0995452404022217,
      "learning_rate": 0.00016150740242261105,
      "loss": 1.9467,
      "step": 243
    },
    {
      "epoch": 0.8675555555555555,
      "grad_norm": 1.997581124305725,
      "learning_rate": 0.00016123822341857335,
      "loss": 0.4737,
      "step": 244
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 0.5704809427261353,
      "learning_rate": 0.00016096904441453567,
      "loss": 1.2059,
      "step": 245
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.5709977746009827,
      "learning_rate": 0.000160699865410498,
      "loss": 1.0681,
      "step": 246
    },
    {
      "epoch": 0.8782222222222222,
      "grad_norm": 0.4789314866065979,
      "learning_rate": 0.0001604306864064603,
      "loss": 1.3911,
      "step": 247
    },
    {
      "epoch": 0.8817777777777778,
      "grad_norm": 0.6251077055931091,
      "learning_rate": 0.00016016150740242262,
      "loss": 0.8446,
      "step": 248
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.6198777556419373,
      "learning_rate": 0.00015989232839838492,
      "loss": 1.2123,
      "step": 249
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.48808401823043823,
      "learning_rate": 0.00015962314939434725,
      "loss": 1.3844,
      "step": 250
    },
    {
      "epoch": 0.8924444444444445,
      "grad_norm": 0.8783051371574402,
      "learning_rate": 0.00015935397039030957,
      "loss": 1.0221,
      "step": 251
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.4624428451061249,
      "learning_rate": 0.00015908479138627187,
      "loss": 1.0763,
      "step": 252
    },
    {
      "epoch": 0.8995555555555556,
      "grad_norm": 0.5417936444282532,
      "learning_rate": 0.0001588156123822342,
      "loss": 1.3057,
      "step": 253
    },
    {
      "epoch": 0.9031111111111111,
      "grad_norm": 0.43751218914985657,
      "learning_rate": 0.0001585464333781965,
      "loss": 1.1134,
      "step": 254
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.943323016166687,
      "learning_rate": 0.00015827725437415882,
      "loss": 1.1528,
      "step": 255
    },
    {
      "epoch": 0.9102222222222223,
      "grad_norm": 0.5172732472419739,
      "learning_rate": 0.00015800807537012115,
      "loss": 0.9659,
      "step": 256
    },
    {
      "epoch": 0.9137777777777778,
      "grad_norm": 1.233431100845337,
      "learning_rate": 0.00015773889636608345,
      "loss": 1.7117,
      "step": 257
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.39350539445877075,
      "learning_rate": 0.00015746971736204578,
      "loss": 0.9438,
      "step": 258
    },
    {
      "epoch": 0.9208888888888889,
      "grad_norm": 1.268738031387329,
      "learning_rate": 0.00015720053835800807,
      "loss": 1.5171,
      "step": 259
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 0.56369948387146,
      "learning_rate": 0.0001569313593539704,
      "loss": 1.484,
      "step": 260
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.4048101007938385,
      "learning_rate": 0.00015666218034993273,
      "loss": 0.875,
      "step": 261
    },
    {
      "epoch": 0.9315555555555556,
      "grad_norm": 0.5403534173965454,
      "learning_rate": 0.00015639300134589503,
      "loss": 0.9223,
      "step": 262
    },
    {
      "epoch": 0.9351111111111111,
      "grad_norm": 1.744361400604248,
      "learning_rate": 0.00015612382234185735,
      "loss": 0.9628,
      "step": 263
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.6678670644760132,
      "learning_rate": 0.00015585464333781965,
      "loss": 1.3679,
      "step": 264
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 0.3641666769981384,
      "learning_rate": 0.00015558546433378198,
      "loss": 1.505,
      "step": 265
    },
    {
      "epoch": 0.9457777777777778,
      "grad_norm": 0.5681832432746887,
      "learning_rate": 0.0001553162853297443,
      "loss": 1.3195,
      "step": 266
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.3981543779373169,
      "learning_rate": 0.0001550471063257066,
      "loss": 1.0068,
      "step": 267
    },
    {
      "epoch": 0.9528888888888889,
      "grad_norm": 0.5515474677085876,
      "learning_rate": 0.0001547779273216689,
      "loss": 1.4179,
      "step": 268
    },
    {
      "epoch": 0.9564444444444444,
      "grad_norm": 1.3329676389694214,
      "learning_rate": 0.00015450874831763123,
      "loss": 1.3393,
      "step": 269
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.341877818107605,
      "learning_rate": 0.00015423956931359355,
      "loss": 1.1206,
      "step": 270
    },
    {
      "epoch": 0.9635555555555556,
      "grad_norm": 0.44232168793678284,
      "learning_rate": 0.00015397039030955588,
      "loss": 1.2508,
      "step": 271
    },
    {
      "epoch": 0.9671111111111111,
      "grad_norm": 0.6992319226264954,
      "learning_rate": 0.00015370121130551818,
      "loss": 1.0084,
      "step": 272
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.5592799186706543,
      "learning_rate": 0.00015343203230148048,
      "loss": 1.4097,
      "step": 273
    },
    {
      "epoch": 0.9742222222222222,
      "grad_norm": 0.5160696506500244,
      "learning_rate": 0.0001531628532974428,
      "loss": 1.3497,
      "step": 274
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.6935634016990662,
      "learning_rate": 0.00015289367429340513,
      "loss": 1.6339,
      "step": 275
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.9417396783828735,
      "learning_rate": 0.00015262449528936743,
      "loss": 1.59,
      "step": 276
    },
    {
      "epoch": 0.9848888888888889,
      "grad_norm": 0.6722342371940613,
      "learning_rate": 0.00015235531628532976,
      "loss": 1.3673,
      "step": 277
    },
    {
      "epoch": 0.9884444444444445,
      "grad_norm": 2.3245015144348145,
      "learning_rate": 0.00015208613728129205,
      "loss": 0.6895,
      "step": 278
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.4311911463737488,
      "learning_rate": 0.00015181695827725438,
      "loss": 1.4836,
      "step": 279
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 0.3776307702064514,
      "learning_rate": 0.0001515477792732167,
      "loss": 1.4433,
      "step": 280
    },
    {
      "epoch": 0.9991111111111111,
      "grad_norm": 0.35465294122695923,
      "learning_rate": 0.000151278600269179,
      "loss": 1.2047,
      "step": 281
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.6660449504852295,
      "learning_rate": 0.00015100942126514133,
      "loss": 1.3034,
      "step": 282
    },
    {
      "epoch": 1.0035555555555555,
      "grad_norm": 0.491671621799469,
      "learning_rate": 0.00015074024226110363,
      "loss": 1.1289,
      "step": 283
    },
    {
      "epoch": 1.007111111111111,
      "grad_norm": 0.6420548558235168,
      "learning_rate": 0.00015047106325706596,
      "loss": 0.8111,
      "step": 284
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.348878413438797,
      "learning_rate": 0.00015020188425302828,
      "loss": 0.8367,
      "step": 285
    },
    {
      "epoch": 1.0142222222222221,
      "grad_norm": 0.49409574270248413,
      "learning_rate": 0.00014993270524899058,
      "loss": 0.6623,
      "step": 286
    },
    {
      "epoch": 1.0177777777777777,
      "grad_norm": 1.9392786026000977,
      "learning_rate": 0.0001496635262449529,
      "loss": 0.5144,
      "step": 287
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.7135207653045654,
      "learning_rate": 0.0001493943472409152,
      "loss": 1.6439,
      "step": 288
    },
    {
      "epoch": 1.024888888888889,
      "grad_norm": 0.4018026888370514,
      "learning_rate": 0.00014912516823687753,
      "loss": 0.8314,
      "step": 289
    },
    {
      "epoch": 1.0284444444444445,
      "grad_norm": 0.4371011257171631,
      "learning_rate": 0.00014885598923283986,
      "loss": 0.8353,
      "step": 290
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.5494061708450317,
      "learning_rate": 0.00014858681022880216,
      "loss": 1.2242,
      "step": 291
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 0.5114312767982483,
      "learning_rate": 0.00014831763122476448,
      "loss": 0.9304,
      "step": 292
    },
    {
      "epoch": 1.039111111111111,
      "grad_norm": 0.33620643615722656,
      "learning_rate": 0.00014804845222072678,
      "loss": 1.0793,
      "step": 293
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.4570811986923218,
      "learning_rate": 0.00014777927321668908,
      "loss": 0.9469,
      "step": 294
    },
    {
      "epoch": 1.0462222222222222,
      "grad_norm": 0.39089396595954895,
      "learning_rate": 0.00014751009421265144,
      "loss": 1.078,
      "step": 295
    },
    {
      "epoch": 1.0497777777777777,
      "grad_norm": 0.5925427079200745,
      "learning_rate": 0.00014724091520861373,
      "loss": 1.2811,
      "step": 296
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.686424732208252,
      "learning_rate": 0.00014697173620457606,
      "loss": 1.3824,
      "step": 297
    },
    {
      "epoch": 1.056888888888889,
      "grad_norm": 2.2481749057769775,
      "learning_rate": 0.00014670255720053836,
      "loss": 0.9625,
      "step": 298
    },
    {
      "epoch": 1.0604444444444445,
      "grad_norm": 0.4314707815647125,
      "learning_rate": 0.00014643337819650066,
      "loss": 1.4748,
      "step": 299
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.41334810853004456,
      "learning_rate": 0.000146164199192463,
      "loss": 1.1987,
      "step": 300
    },
    {
      "epoch": 1.0675555555555556,
      "grad_norm": 1.418563723564148,
      "learning_rate": 0.0001458950201884253,
      "loss": 1.2506,
      "step": 301
    },
    {
      "epoch": 1.0711111111111111,
      "grad_norm": 0.4225234389305115,
      "learning_rate": 0.0001456258411843876,
      "loss": 0.8938,
      "step": 302
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.6556747555732727,
      "learning_rate": 0.00014535666218034994,
      "loss": 0.8747,
      "step": 303
    },
    {
      "epoch": 1.0782222222222222,
      "grad_norm": 0.4171738624572754,
      "learning_rate": 0.00014508748317631224,
      "loss": 0.7903,
      "step": 304
    },
    {
      "epoch": 1.0817777777777777,
      "grad_norm": 0.407701313495636,
      "learning_rate": 0.0001448183041722746,
      "loss": 1.3107,
      "step": 305
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.8502035140991211,
      "learning_rate": 0.0001445491251682369,
      "loss": 0.9008,
      "step": 306
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 2.8406260013580322,
      "learning_rate": 0.0001442799461641992,
      "loss": 1.876,
      "step": 307
    },
    {
      "epoch": 1.0924444444444443,
      "grad_norm": 0.49091583490371704,
      "learning_rate": 0.0001440107671601615,
      "loss": 1.2037,
      "step": 308
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.7698044776916504,
      "learning_rate": 0.0001437415881561238,
      "loss": 0.6167,
      "step": 309
    },
    {
      "epoch": 1.0995555555555556,
      "grad_norm": 0.5304529070854187,
      "learning_rate": 0.00014347240915208614,
      "loss": 1.0344,
      "step": 310
    },
    {
      "epoch": 1.1031111111111112,
      "grad_norm": 0.3576546609401703,
      "learning_rate": 0.00014320323014804846,
      "loss": 1.1444,
      "step": 311
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.6831066608428955,
      "learning_rate": 0.00014293405114401076,
      "loss": 2.0732,
      "step": 312
    },
    {
      "epoch": 1.1102222222222222,
      "grad_norm": 0.7344862818717957,
      "learning_rate": 0.0001426648721399731,
      "loss": 2.1,
      "step": 313
    },
    {
      "epoch": 1.1137777777777778,
      "grad_norm": 0.6108401417732239,
      "learning_rate": 0.0001423956931359354,
      "loss": 1.2544,
      "step": 314
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.46840226650238037,
      "learning_rate": 0.00014212651413189771,
      "loss": 0.9068,
      "step": 315
    },
    {
      "epoch": 1.1208888888888888,
      "grad_norm": 1.0332311391830444,
      "learning_rate": 0.00014185733512786004,
      "loss": 1.3588,
      "step": 316
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 0.43495795130729675,
      "learning_rate": 0.00014158815612382234,
      "loss": 1.0726,
      "step": 317
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.6963450908660889,
      "learning_rate": 0.00014131897711978467,
      "loss": 0.8869,
      "step": 318
    },
    {
      "epoch": 1.1315555555555556,
      "grad_norm": 0.45892462134361267,
      "learning_rate": 0.000141049798115747,
      "loss": 1.1813,
      "step": 319
    },
    {
      "epoch": 1.1351111111111112,
      "grad_norm": 1.175833821296692,
      "learning_rate": 0.0001407806191117093,
      "loss": 1.2899,
      "step": 320
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.5530577301979065,
      "learning_rate": 0.00014051144010767162,
      "loss": 1.2218,
      "step": 321
    },
    {
      "epoch": 1.1422222222222222,
      "grad_norm": 0.33767277002334595,
      "learning_rate": 0.00014024226110363392,
      "loss": 0.9844,
      "step": 322
    },
    {
      "epoch": 1.1457777777777778,
      "grad_norm": 0.37571579217910767,
      "learning_rate": 0.00013997308209959624,
      "loss": 0.8213,
      "step": 323
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.43501412868499756,
      "learning_rate": 0.00013970390309555857,
      "loss": 0.9494,
      "step": 324
    },
    {
      "epoch": 1.1528888888888889,
      "grad_norm": 0.9383093118667603,
      "learning_rate": 0.00013943472409152087,
      "loss": 1.1826,
      "step": 325
    },
    {
      "epoch": 1.1564444444444444,
      "grad_norm": 1.0093683004379272,
      "learning_rate": 0.0001391655450874832,
      "loss": 1.7745,
      "step": 326
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.47069960832595825,
      "learning_rate": 0.0001388963660834455,
      "loss": 0.7098,
      "step": 327
    },
    {
      "epoch": 1.1635555555555555,
      "grad_norm": 0.35761910676956177,
      "learning_rate": 0.00013862718707940782,
      "loss": 1.4242,
      "step": 328
    },
    {
      "epoch": 1.1671111111111112,
      "grad_norm": 0.6805028319358826,
      "learning_rate": 0.00013835800807537014,
      "loss": 0.6437,
      "step": 329
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.9209582209587097,
      "learning_rate": 0.00013808882907133244,
      "loss": 1.0217,
      "step": 330
    },
    {
      "epoch": 1.1742222222222223,
      "grad_norm": 0.5888932347297668,
      "learning_rate": 0.00013781965006729477,
      "loss": 1.3067,
      "step": 331
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 1.4281781911849976,
      "learning_rate": 0.00013755047106325707,
      "loss": 0.0763,
      "step": 332
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 1.1801198720932007,
      "learning_rate": 0.00013728129205921937,
      "loss": 1.4572,
      "step": 333
    },
    {
      "epoch": 1.1848888888888889,
      "grad_norm": 0.528374433517456,
      "learning_rate": 0.00013701211305518172,
      "loss": 1.3542,
      "step": 334
    },
    {
      "epoch": 1.1884444444444444,
      "grad_norm": 0.6746883392333984,
      "learning_rate": 0.00013674293405114402,
      "loss": 0.8825,
      "step": 335
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.5219385623931885,
      "learning_rate": 0.00013647375504710635,
      "loss": 0.8417,
      "step": 336
    },
    {
      "epoch": 1.1955555555555555,
      "grad_norm": 0.7371846437454224,
      "learning_rate": 0.00013620457604306865,
      "loss": 1.4079,
      "step": 337
    },
    {
      "epoch": 1.199111111111111,
      "grad_norm": 1.5975574254989624,
      "learning_rate": 0.00013593539703903094,
      "loss": 1.6536,
      "step": 338
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.6501990556716919,
      "learning_rate": 0.0001356662180349933,
      "loss": 1.5771,
      "step": 339
    },
    {
      "epoch": 1.2062222222222223,
      "grad_norm": 0.4334208071231842,
      "learning_rate": 0.0001353970390309556,
      "loss": 0.8486,
      "step": 340
    },
    {
      "epoch": 1.2097777777777778,
      "grad_norm": 0.6557787656784058,
      "learning_rate": 0.0001351278600269179,
      "loss": 1.0548,
      "step": 341
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.6312916278839111,
      "learning_rate": 0.00013485868102288022,
      "loss": 0.7501,
      "step": 342
    },
    {
      "epoch": 1.216888888888889,
      "grad_norm": 1.1787288188934326,
      "learning_rate": 0.00013458950201884252,
      "loss": 1.3801,
      "step": 343
    },
    {
      "epoch": 1.2204444444444444,
      "grad_norm": 0.4858352839946747,
      "learning_rate": 0.00013432032301480487,
      "loss": 1.3563,
      "step": 344
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.6948398947715759,
      "learning_rate": 0.00013405114401076717,
      "loss": 1.2242,
      "step": 345
    },
    {
      "epoch": 1.2275555555555555,
      "grad_norm": 0.41810595989227295,
      "learning_rate": 0.00013378196500672947,
      "loss": 0.8599,
      "step": 346
    },
    {
      "epoch": 1.231111111111111,
      "grad_norm": 0.6559593081474304,
      "learning_rate": 0.0001335127860026918,
      "loss": 1.4023,
      "step": 347
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.4936801493167877,
      "learning_rate": 0.0001332436069986541,
      "loss": 1.2044,
      "step": 348
    },
    {
      "epoch": 1.2382222222222223,
      "grad_norm": 0.5381296277046204,
      "learning_rate": 0.00013297442799461642,
      "loss": 1.4087,
      "step": 349
    },
    {
      "epoch": 1.2417777777777779,
      "grad_norm": 0.5157427191734314,
      "learning_rate": 0.00013270524899057875,
      "loss": 0.8946,
      "step": 350
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 1.5028034448623657,
      "learning_rate": 0.00013243606998654105,
      "loss": 1.4573,
      "step": 351
    },
    {
      "epoch": 1.248888888888889,
      "grad_norm": 0.4232273995876312,
      "learning_rate": 0.00013216689098250337,
      "loss": 0.6886,
      "step": 352
    },
    {
      "epoch": 1.2524444444444445,
      "grad_norm": 0.602737545967102,
      "learning_rate": 0.00013189771197846567,
      "loss": 1.2523,
      "step": 353
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.4993952810764313,
      "learning_rate": 0.000131628532974428,
      "loss": 0.8682,
      "step": 354
    },
    {
      "epoch": 1.2595555555555555,
      "grad_norm": 0.5261483192443848,
      "learning_rate": 0.00013135935397039033,
      "loss": 1.0591,
      "step": 355
    },
    {
      "epoch": 1.263111111111111,
      "grad_norm": 0.4837905764579773,
      "learning_rate": 0.00013109017496635262,
      "loss": 1.1756,
      "step": 356
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.832447350025177,
      "learning_rate": 0.00013082099596231495,
      "loss": 1.0163,
      "step": 357
    },
    {
      "epoch": 1.2702222222222221,
      "grad_norm": 0.6625156998634338,
      "learning_rate": 0.00013055181695827725,
      "loss": 1.4777,
      "step": 358
    },
    {
      "epoch": 1.2737777777777777,
      "grad_norm": 0.7598236799240112,
      "learning_rate": 0.00013028263795423958,
      "loss": 1.0017,
      "step": 359
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.4000476896762848,
      "learning_rate": 0.0001300134589502019,
      "loss": 1.0208,
      "step": 360
    },
    {
      "epoch": 1.280888888888889,
      "grad_norm": 0.42712485790252686,
      "learning_rate": 0.0001297442799461642,
      "loss": 1.1063,
      "step": 361
    },
    {
      "epoch": 1.2844444444444445,
      "grad_norm": 1.524664282798767,
      "learning_rate": 0.00012947510094212653,
      "loss": 0.7795,
      "step": 362
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.5034049153327942,
      "learning_rate": 0.00012920592193808883,
      "loss": 1.4956,
      "step": 363
    },
    {
      "epoch": 1.2915555555555556,
      "grad_norm": 0.7318310737609863,
      "learning_rate": 0.00012893674293405115,
      "loss": 0.8895,
      "step": 364
    },
    {
      "epoch": 1.295111111111111,
      "grad_norm": 1.0917840003967285,
      "learning_rate": 0.00012866756393001348,
      "loss": 0.9866,
      "step": 365
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.49054598808288574,
      "learning_rate": 0.00012839838492597578,
      "loss": 0.9419,
      "step": 366
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 0.47545126080513,
      "learning_rate": 0.00012812920592193808,
      "loss": 1.3694,
      "step": 367
    },
    {
      "epoch": 1.3057777777777777,
      "grad_norm": 0.4937813878059387,
      "learning_rate": 0.0001278600269179004,
      "loss": 1.1736,
      "step": 368
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.49558064341545105,
      "learning_rate": 0.00012759084791386273,
      "loss": 1.0889,
      "step": 369
    },
    {
      "epoch": 1.3128888888888888,
      "grad_norm": 0.6193681359291077,
      "learning_rate": 0.00012732166890982505,
      "loss": 0.8664,
      "step": 370
    },
    {
      "epoch": 1.3164444444444445,
      "grad_norm": 0.5346587896347046,
      "learning_rate": 0.00012705248990578735,
      "loss": 1.0578,
      "step": 371
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.5411945581436157,
      "learning_rate": 0.00012678331090174965,
      "loss": 0.94,
      "step": 372
    },
    {
      "epoch": 1.3235555555555556,
      "grad_norm": 0.5450740456581116,
      "learning_rate": 0.00012651413189771198,
      "loss": 1.2285,
      "step": 373
    },
    {
      "epoch": 1.3271111111111111,
      "grad_norm": 0.5201631784439087,
      "learning_rate": 0.0001262449528936743,
      "loss": 1.2156,
      "step": 374
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.8746376633644104,
      "learning_rate": 0.0001259757738896366,
      "loss": 1.5062,
      "step": 375
    },
    {
      "epoch": 1.3342222222222222,
      "grad_norm": 0.5007808208465576,
      "learning_rate": 0.00012570659488559893,
      "loss": 1.2301,
      "step": 376
    },
    {
      "epoch": 1.3377777777777777,
      "grad_norm": 0.5830467939376831,
      "learning_rate": 0.00012543741588156123,
      "loss": 0.8392,
      "step": 377
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.42929258942604065,
      "learning_rate": 0.00012516823687752356,
      "loss": 1.2081,
      "step": 378
    },
    {
      "epoch": 1.3448888888888888,
      "grad_norm": 0.52093905210495,
      "learning_rate": 0.00012489905787348588,
      "loss": 1.0895,
      "step": 379
    },
    {
      "epoch": 1.3484444444444446,
      "grad_norm": 0.627741813659668,
      "learning_rate": 0.00012462987886944818,
      "loss": 1.5656,
      "step": 380
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.7872267961502075,
      "learning_rate": 0.0001243606998654105,
      "loss": 1.2289,
      "step": 381
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.40516775846481323,
      "learning_rate": 0.0001240915208613728,
      "loss": 1.2183,
      "step": 382
    },
    {
      "epoch": 1.3591111111111112,
      "grad_norm": 0.3053956925868988,
      "learning_rate": 0.00012382234185733513,
      "loss": 0.77,
      "step": 383
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.661889374256134,
      "learning_rate": 0.00012355316285329746,
      "loss": 1.1077,
      "step": 384
    },
    {
      "epoch": 1.3662222222222222,
      "grad_norm": 0.6418240070343018,
      "learning_rate": 0.00012328398384925976,
      "loss": 1.0933,
      "step": 385
    },
    {
      "epoch": 1.3697777777777778,
      "grad_norm": 0.4024575650691986,
      "learning_rate": 0.00012301480484522208,
      "loss": 0.8233,
      "step": 386
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.5325844883918762,
      "learning_rate": 0.00012274562584118438,
      "loss": 1.0286,
      "step": 387
    },
    {
      "epoch": 1.3768888888888888,
      "grad_norm": 0.6266524195671082,
      "learning_rate": 0.0001224764468371467,
      "loss": 1.0966,
      "step": 388
    },
    {
      "epoch": 1.3804444444444444,
      "grad_norm": 0.7550873756408691,
      "learning_rate": 0.00012220726783310903,
      "loss": 1.3189,
      "step": 389
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.44828981161117554,
      "learning_rate": 0.00012193808882907133,
      "loss": 0.9153,
      "step": 390
    },
    {
      "epoch": 1.3875555555555557,
      "grad_norm": 0.49068406224250793,
      "learning_rate": 0.00012166890982503365,
      "loss": 0.8217,
      "step": 391
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 0.46209636330604553,
      "learning_rate": 0.00012139973082099596,
      "loss": 0.8939,
      "step": 392
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.562278687953949,
      "learning_rate": 0.0001211305518169583,
      "loss": 1.1071,
      "step": 393
    },
    {
      "epoch": 1.3982222222222223,
      "grad_norm": 0.42748209834098816,
      "learning_rate": 0.0001208613728129206,
      "loss": 1.1919,
      "step": 394
    },
    {
      "epoch": 1.4017777777777778,
      "grad_norm": 0.40763670206069946,
      "learning_rate": 0.00012059219380888291,
      "loss": 1.3548,
      "step": 395
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.5826980471611023,
      "learning_rate": 0.00012032301480484522,
      "loss": 0.8941,
      "step": 396
    },
    {
      "epoch": 1.4088888888888889,
      "grad_norm": 0.4893500506877899,
      "learning_rate": 0.00012005383580080754,
      "loss": 1.0319,
      "step": 397
    },
    {
      "epoch": 1.4124444444444444,
      "grad_norm": 0.6868374347686768,
      "learning_rate": 0.00011978465679676986,
      "loss": 0.9584,
      "step": 398
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.5697224140167236,
      "learning_rate": 0.00011951547779273217,
      "loss": 1.5175,
      "step": 399
    },
    {
      "epoch": 1.4195555555555557,
      "grad_norm": 0.5339218378067017,
      "learning_rate": 0.00011924629878869449,
      "loss": 1.1168,
      "step": 400
    },
    {
      "epoch": 1.423111111111111,
      "grad_norm": 0.7209904789924622,
      "learning_rate": 0.0001189771197846568,
      "loss": 1.5668,
      "step": 401
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.680099368095398,
      "learning_rate": 0.00011870794078061911,
      "loss": 1.2445,
      "step": 402
    },
    {
      "epoch": 1.4302222222222223,
      "grad_norm": 0.5311406254768372,
      "learning_rate": 0.00011843876177658144,
      "loss": 0.9979,
      "step": 403
    },
    {
      "epoch": 1.4337777777777778,
      "grad_norm": 0.658854603767395,
      "learning_rate": 0.00011816958277254375,
      "loss": 1.2837,
      "step": 404
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 2.419548749923706,
      "learning_rate": 0.00011790040376850606,
      "loss": 0.1339,
      "step": 405
    },
    {
      "epoch": 1.4408888888888889,
      "grad_norm": 0.4946020245552063,
      "learning_rate": 0.00011763122476446838,
      "loss": 1.1551,
      "step": 406
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.5353121161460876,
      "learning_rate": 0.00011736204576043069,
      "loss": 1.064,
      "step": 407
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.645849347114563,
      "learning_rate": 0.00011709286675639301,
      "loss": 0.8859,
      "step": 408
    },
    {
      "epoch": 1.4515555555555555,
      "grad_norm": 0.4076879918575287,
      "learning_rate": 0.00011682368775235533,
      "loss": 0.8663,
      "step": 409
    },
    {
      "epoch": 1.455111111111111,
      "grad_norm": 0.4506739377975464,
      "learning_rate": 0.00011655450874831764,
      "loss": 1.1576,
      "step": 410
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.5075539350509644,
      "learning_rate": 0.00011628532974427995,
      "loss": 0.9667,
      "step": 411
    },
    {
      "epoch": 1.462222222222222,
      "grad_norm": 0.6042335033416748,
      "learning_rate": 0.00011601615074024226,
      "loss": 1.0946,
      "step": 412
    },
    {
      "epoch": 1.4657777777777778,
      "grad_norm": 0.9611848592758179,
      "learning_rate": 0.00011574697173620459,
      "loss": 1.5511,
      "step": 413
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 2.7561402320861816,
      "learning_rate": 0.0001154777927321669,
      "loss": 0.1693,
      "step": 414
    },
    {
      "epoch": 1.472888888888889,
      "grad_norm": 0.7777256965637207,
      "learning_rate": 0.00011520861372812922,
      "loss": 1.1326,
      "step": 415
    },
    {
      "epoch": 1.4764444444444444,
      "grad_norm": 0.636185348033905,
      "learning_rate": 0.00011493943472409153,
      "loss": 1.448,
      "step": 416
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.6775022149085999,
      "learning_rate": 0.00011467025572005383,
      "loss": 1.235,
      "step": 417
    },
    {
      "epoch": 1.4835555555555555,
      "grad_norm": 0.4335225522518158,
      "learning_rate": 0.00011440107671601617,
      "loss": 0.8417,
      "step": 418
    },
    {
      "epoch": 1.487111111111111,
      "grad_norm": 0.2955522835254669,
      "learning_rate": 0.00011413189771197848,
      "loss": 1.0075,
      "step": 419
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.669853150844574,
      "learning_rate": 0.00011386271870794079,
      "loss": 1.0987,
      "step": 420
    },
    {
      "epoch": 1.4942222222222221,
      "grad_norm": 0.5401464700698853,
      "learning_rate": 0.00011359353970390309,
      "loss": 0.9357,
      "step": 421
    },
    {
      "epoch": 1.4977777777777779,
      "grad_norm": 0.35327255725860596,
      "learning_rate": 0.0001133243606998654,
      "loss": 0.8894,
      "step": 422
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.6428256630897522,
      "learning_rate": 0.00011305518169582774,
      "loss": 0.9341,
      "step": 423
    },
    {
      "epoch": 1.504888888888889,
      "grad_norm": 0.8231083750724792,
      "learning_rate": 0.00011278600269179006,
      "loss": 1.0232,
      "step": 424
    },
    {
      "epoch": 1.5084444444444445,
      "grad_norm": 1.4006667137145996,
      "learning_rate": 0.00011251682368775235,
      "loss": 1.0929,
      "step": 425
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.6655312180519104,
      "learning_rate": 0.00011224764468371467,
      "loss": 1.5393,
      "step": 426
    },
    {
      "epoch": 1.5155555555555555,
      "grad_norm": 0.692173182964325,
      "learning_rate": 0.00011197846567967698,
      "loss": 1.0666,
      "step": 427
    },
    {
      "epoch": 1.519111111111111,
      "grad_norm": 0.45526015758514404,
      "learning_rate": 0.00011170928667563932,
      "loss": 1.4852,
      "step": 428
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.5109575390815735,
      "learning_rate": 0.00011144010767160162,
      "loss": 1.1658,
      "step": 429
    },
    {
      "epoch": 1.5262222222222221,
      "grad_norm": 0.5618743896484375,
      "learning_rate": 0.00011117092866756393,
      "loss": 1.3046,
      "step": 430
    },
    {
      "epoch": 1.529777777777778,
      "grad_norm": 0.45944514870643616,
      "learning_rate": 0.00011090174966352624,
      "loss": 1.355,
      "step": 431
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.5362595319747925,
      "learning_rate": 0.00011063257065948856,
      "loss": 0.9887,
      "step": 432
    },
    {
      "epoch": 1.536888888888889,
      "grad_norm": 0.4880378246307373,
      "learning_rate": 0.00011036339165545088,
      "loss": 1.3803,
      "step": 433
    },
    {
      "epoch": 1.5404444444444443,
      "grad_norm": 0.7276250123977661,
      "learning_rate": 0.0001100942126514132,
      "loss": 0.7118,
      "step": 434
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.8629196286201477,
      "learning_rate": 0.00010982503364737551,
      "loss": 1.7967,
      "step": 435
    },
    {
      "epoch": 1.5475555555555556,
      "grad_norm": 1.3917365074157715,
      "learning_rate": 0.00010955585464333782,
      "loss": 1.6964,
      "step": 436
    },
    {
      "epoch": 1.551111111111111,
      "grad_norm": 0.5979849696159363,
      "learning_rate": 0.00010928667563930013,
      "loss": 1.1895,
      "step": 437
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.5620782971382141,
      "learning_rate": 0.00010901749663526246,
      "loss": 1.0082,
      "step": 438
    },
    {
      "epoch": 1.5582222222222222,
      "grad_norm": 0.5043663382530212,
      "learning_rate": 0.00010874831763122477,
      "loss": 1.0071,
      "step": 439
    },
    {
      "epoch": 1.561777777777778,
      "grad_norm": 0.659468948841095,
      "learning_rate": 0.00010847913862718708,
      "loss": 1.0779,
      "step": 440
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.7350125908851624,
      "learning_rate": 0.0001082099596231494,
      "loss": 1.2754,
      "step": 441
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 0.874281644821167,
      "learning_rate": 0.00010794078061911172,
      "loss": 0.83,
      "step": 442
    },
    {
      "epoch": 1.5724444444444443,
      "grad_norm": 0.7123392820358276,
      "learning_rate": 0.00010767160161507404,
      "loss": 1.3468,
      "step": 443
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.439998596906662,
      "learning_rate": 0.00010740242261103635,
      "loss": 1.0144,
      "step": 444
    },
    {
      "epoch": 1.5795555555555556,
      "grad_norm": 0.40848302841186523,
      "learning_rate": 0.00010713324360699866,
      "loss": 1.091,
      "step": 445
    },
    {
      "epoch": 1.5831111111111111,
      "grad_norm": 0.7358989119529724,
      "learning_rate": 0.00010686406460296097,
      "loss": 0.726,
      "step": 446
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.48468920588493347,
      "learning_rate": 0.0001065948855989233,
      "loss": 1.2408,
      "step": 447
    },
    {
      "epoch": 1.5902222222222222,
      "grad_norm": 0.5322388410568237,
      "learning_rate": 0.00010632570659488561,
      "loss": 1.1177,
      "step": 448
    },
    {
      "epoch": 1.5937777777777777,
      "grad_norm": 0.47103622555732727,
      "learning_rate": 0.00010605652759084792,
      "loss": 1.2,
      "step": 449
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.4360021650791168,
      "learning_rate": 0.00010578734858681024,
      "loss": 0.9571,
      "step": 450
    },
    {
      "epoch": 1.600888888888889,
      "grad_norm": 0.4869726598262787,
      "learning_rate": 0.00010551816958277254,
      "loss": 1.113,
      "step": 451
    },
    {
      "epoch": 1.6044444444444443,
      "grad_norm": 0.6626381874084473,
      "learning_rate": 0.00010524899057873488,
      "loss": 1.0247,
      "step": 452
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.5871049761772156,
      "learning_rate": 0.00010497981157469719,
      "loss": 1.2926,
      "step": 453
    },
    {
      "epoch": 1.6115555555555554,
      "grad_norm": 0.5724563002586365,
      "learning_rate": 0.0001047106325706595,
      "loss": 0.9195,
      "step": 454
    },
    {
      "epoch": 1.6151111111111112,
      "grad_norm": 0.708459198474884,
      "learning_rate": 0.0001044414535666218,
      "loss": 1.152,
      "step": 455
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.6907461285591125,
      "learning_rate": 0.00010417227456258411,
      "loss": 0.9354,
      "step": 456
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.4663774073123932,
      "learning_rate": 0.00010390309555854645,
      "loss": 0.8643,
      "step": 457
    },
    {
      "epoch": 1.6257777777777778,
      "grad_norm": 1.5924190282821655,
      "learning_rate": 0.00010363391655450876,
      "loss": 1.1769,
      "step": 458
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 1.1578738689422607,
      "learning_rate": 0.00010336473755047106,
      "loss": 1.6854,
      "step": 459
    },
    {
      "epoch": 1.6328888888888888,
      "grad_norm": 0.5077686309814453,
      "learning_rate": 0.00010309555854643338,
      "loss": 1.0461,
      "step": 460
    },
    {
      "epoch": 1.6364444444444444,
      "grad_norm": 0.3927466571331024,
      "learning_rate": 0.00010282637954239569,
      "loss": 0.8703,
      "step": 461
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.6425483822822571,
      "learning_rate": 0.00010255720053835803,
      "loss": 1.7624,
      "step": 462
    },
    {
      "epoch": 1.6435555555555554,
      "grad_norm": 0.725844144821167,
      "learning_rate": 0.00010228802153432033,
      "loss": 1.0301,
      "step": 463
    },
    {
      "epoch": 1.6471111111111112,
      "grad_norm": 0.6518636345863342,
      "learning_rate": 0.00010201884253028264,
      "loss": 1.17,
      "step": 464
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.6274486780166626,
      "learning_rate": 0.00010174966352624495,
      "loss": 1.2543,
      "step": 465
    },
    {
      "epoch": 1.6542222222222223,
      "grad_norm": 0.510281503200531,
      "learning_rate": 0.00010148048452220727,
      "loss": 1.377,
      "step": 466
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 1.129225730895996,
      "learning_rate": 0.00010121130551816959,
      "loss": 1.2855,
      "step": 467
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.7240411639213562,
      "learning_rate": 0.0001009421265141319,
      "loss": 1.3352,
      "step": 468
    },
    {
      "epoch": 1.6648888888888889,
      "grad_norm": 0.6668725609779358,
      "learning_rate": 0.00010067294751009422,
      "loss": 1.1478,
      "step": 469
    },
    {
      "epoch": 1.6684444444444444,
      "grad_norm": 0.7390429377555847,
      "learning_rate": 0.00010040376850605653,
      "loss": 1.2207,
      "step": 470
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.7465084195137024,
      "learning_rate": 0.00010013458950201884,
      "loss": 0.8591,
      "step": 471
    },
    {
      "epoch": 1.6755555555555555,
      "grad_norm": 0.8764381408691406,
      "learning_rate": 9.986541049798115e-05,
      "loss": 1.729,
      "step": 472
    },
    {
      "epoch": 1.6791111111111112,
      "grad_norm": 0.46826648712158203,
      "learning_rate": 9.959623149394348e-05,
      "loss": 0.9387,
      "step": 473
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.6442382335662842,
      "learning_rate": 9.932705248990579e-05,
      "loss": 1.3558,
      "step": 474
    },
    {
      "epoch": 1.6862222222222223,
      "grad_norm": 1.217589020729065,
      "learning_rate": 9.90578734858681e-05,
      "loss": 1.5086,
      "step": 475
    },
    {
      "epoch": 1.6897777777777778,
      "grad_norm": 0.4249441623687744,
      "learning_rate": 9.878869448183042e-05,
      "loss": 0.914,
      "step": 476
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.5283467173576355,
      "learning_rate": 9.851951547779273e-05,
      "loss": 1.0403,
      "step": 477
    },
    {
      "epoch": 1.696888888888889,
      "grad_norm": 0.5963265895843506,
      "learning_rate": 9.825033647375506e-05,
      "loss": 0.8081,
      "step": 478
    },
    {
      "epoch": 1.7004444444444444,
      "grad_norm": 0.6121907830238342,
      "learning_rate": 9.798115746971737e-05,
      "loss": 0.7648,
      "step": 479
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.6706607341766357,
      "learning_rate": 9.771197846567968e-05,
      "loss": 0.9652,
      "step": 480
    },
    {
      "epoch": 1.7075555555555555,
      "grad_norm": 0.7840828895568848,
      "learning_rate": 9.7442799461642e-05,
      "loss": 1.7135,
      "step": 481
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.7045674920082092,
      "learning_rate": 9.717362045760431e-05,
      "loss": 1.2658,
      "step": 482
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.5571746230125427,
      "learning_rate": 9.690444145356663e-05,
      "loss": 0.8469,
      "step": 483
    },
    {
      "epoch": 1.7182222222222223,
      "grad_norm": 1.092525601387024,
      "learning_rate": 9.663526244952895e-05,
      "loss": 1.4951,
      "step": 484
    },
    {
      "epoch": 1.7217777777777776,
      "grad_norm": 1.4281729459762573,
      "learning_rate": 9.636608344549124e-05,
      "loss": 0.9034,
      "step": 485
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.48763972520828247,
      "learning_rate": 9.609690444145357e-05,
      "loss": 1.174,
      "step": 486
    },
    {
      "epoch": 1.728888888888889,
      "grad_norm": 0.7100622057914734,
      "learning_rate": 9.582772543741588e-05,
      "loss": 1.2208,
      "step": 487
    },
    {
      "epoch": 1.7324444444444445,
      "grad_norm": 0.8657350540161133,
      "learning_rate": 9.555854643337821e-05,
      "loss": 1.8077,
      "step": 488
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.5231872200965881,
      "learning_rate": 9.528936742934051e-05,
      "loss": 1.0886,
      "step": 489
    },
    {
      "epoch": 1.7395555555555555,
      "grad_norm": 0.6128417253494263,
      "learning_rate": 9.502018842530283e-05,
      "loss": 1.0034,
      "step": 490
    },
    {
      "epoch": 1.743111111111111,
      "grad_norm": 0.7048588991165161,
      "learning_rate": 9.475100942126515e-05,
      "loss": 1.3726,
      "step": 491
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.544094443321228,
      "learning_rate": 9.448183041722746e-05,
      "loss": 1.1508,
      "step": 492
    },
    {
      "epoch": 1.7502222222222223,
      "grad_norm": 1.5538989305496216,
      "learning_rate": 9.421265141318977e-05,
      "loss": 0.9497,
      "step": 493
    },
    {
      "epoch": 1.7537777777777777,
      "grad_norm": 0.4652959108352661,
      "learning_rate": 9.394347240915209e-05,
      "loss": 1.0842,
      "step": 494
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.4682917594909668,
      "learning_rate": 9.367429340511441e-05,
      "loss": 1.1866,
      "step": 495
    },
    {
      "epoch": 1.7608888888888887,
      "grad_norm": 0.6098275780677795,
      "learning_rate": 9.340511440107672e-05,
      "loss": 1.4466,
      "step": 496
    },
    {
      "epoch": 1.7644444444444445,
      "grad_norm": 0.8003350496292114,
      "learning_rate": 9.313593539703904e-05,
      "loss": 0.6746,
      "step": 497
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.5836489796638489,
      "learning_rate": 9.286675639300135e-05,
      "loss": 0.6901,
      "step": 498
    },
    {
      "epoch": 1.7715555555555556,
      "grad_norm": 0.7085847854614258,
      "learning_rate": 9.259757738896366e-05,
      "loss": 1.2825,
      "step": 499
    },
    {
      "epoch": 1.775111111111111,
      "grad_norm": 0.4334103763103485,
      "learning_rate": 9.232839838492599e-05,
      "loss": 1.1795,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 843,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.171489386785997e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
