{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.775111111111111,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035555555555555557,
      "grad_norm": 3.417015552520752,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.7456,
      "step": 1
    },
    {
      "epoch": 0.0071111111111111115,
      "grad_norm": 2.2623541355133057,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.4412,
      "step": 2
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 2.405350685119629,
      "learning_rate": 6e-06,
      "loss": 1.0993,
      "step": 3
    },
    {
      "epoch": 0.014222222222222223,
      "grad_norm": 1.7570509910583496,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.3481,
      "step": 4
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 1.6395419836044312,
      "learning_rate": 1e-05,
      "loss": 1.2886,
      "step": 5
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 1.9863721132278442,
      "learning_rate": 1.2e-05,
      "loss": 1.0441,
      "step": 6
    },
    {
      "epoch": 0.024888888888888887,
      "grad_norm": 1.8783313035964966,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.249,
      "step": 7
    },
    {
      "epoch": 0.028444444444444446,
      "grad_norm": 1.9590867757797241,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.2137,
      "step": 8
    },
    {
      "epoch": 0.032,
      "grad_norm": 2.084606647491455,
      "learning_rate": 1.8e-05,
      "loss": 1.0822,
      "step": 9
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 1.5971049070358276,
      "learning_rate": 2e-05,
      "loss": 0.9709,
      "step": 10
    },
    {
      "epoch": 0.03911111111111111,
      "grad_norm": 1.7119561433792114,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.0564,
      "step": 11
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 2.5340511798858643,
      "learning_rate": 2.4e-05,
      "loss": 1.2541,
      "step": 12
    },
    {
      "epoch": 0.04622222222222222,
      "grad_norm": 3.271831750869751,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.2915,
      "step": 13
    },
    {
      "epoch": 0.049777777777777775,
      "grad_norm": 2.574676036834717,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.035,
      "step": 14
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 2.294796943664551,
      "learning_rate": 3e-05,
      "loss": 1.0513,
      "step": 15
    },
    {
      "epoch": 0.05688888888888889,
      "grad_norm": 2.257603883743286,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.8795,
      "step": 16
    },
    {
      "epoch": 0.060444444444444446,
      "grad_norm": 2.513715982437134,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.9061,
      "step": 17
    },
    {
      "epoch": 0.064,
      "grad_norm": 1.420697569847107,
      "learning_rate": 3.6e-05,
      "loss": 0.6173,
      "step": 18
    },
    {
      "epoch": 0.06755555555555555,
      "grad_norm": 2.624630928039551,
      "learning_rate": 3.8e-05,
      "loss": 1.1697,
      "step": 19
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 3.4680824279785156,
      "learning_rate": 4e-05,
      "loss": 0.8967,
      "step": 20
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 2.6797943115234375,
      "learning_rate": 4.2e-05,
      "loss": 0.695,
      "step": 21
    },
    {
      "epoch": 0.07822222222222222,
      "grad_norm": 2.454733371734619,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.7614,
      "step": 22
    },
    {
      "epoch": 0.08177777777777778,
      "grad_norm": 2.687948226928711,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.8142,
      "step": 23
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 2.669346809387207,
      "learning_rate": 4.8e-05,
      "loss": 0.7365,
      "step": 24
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 2.0461840629577637,
      "learning_rate": 5e-05,
      "loss": 0.5769,
      "step": 25
    },
    {
      "epoch": 0.09244444444444444,
      "grad_norm": 3.0744266510009766,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.6968,
      "step": 26
    },
    {
      "epoch": 0.096,
      "grad_norm": 2.6707687377929688,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 0.6693,
      "step": 27
    },
    {
      "epoch": 0.09955555555555555,
      "grad_norm": 3.8194727897644043,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.4376,
      "step": 28
    },
    {
      "epoch": 0.10311111111111111,
      "grad_norm": 2.7937817573547363,
      "learning_rate": 5.8e-05,
      "loss": 0.7816,
      "step": 29
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 3.1054444313049316,
      "learning_rate": 6e-05,
      "loss": 0.4605,
      "step": 30
    },
    {
      "epoch": 0.11022222222222222,
      "grad_norm": 2.3572492599487305,
      "learning_rate": 6.2e-05,
      "loss": 0.3864,
      "step": 31
    },
    {
      "epoch": 0.11377777777777778,
      "grad_norm": 2.8888144493103027,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.6771,
      "step": 32
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 2.4622697830200195,
      "learning_rate": 6.6e-05,
      "loss": 0.4159,
      "step": 33
    },
    {
      "epoch": 0.12088888888888889,
      "grad_norm": 1.3983370065689087,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.4291,
      "step": 34
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 3.5808324813842773,
      "learning_rate": 7e-05,
      "loss": 0.4108,
      "step": 35
    },
    {
      "epoch": 0.128,
      "grad_norm": 3.5400760173797607,
      "learning_rate": 7.2e-05,
      "loss": 0.4954,
      "step": 36
    },
    {
      "epoch": 0.13155555555555556,
      "grad_norm": 3.8553736209869385,
      "learning_rate": 7.4e-05,
      "loss": 0.5452,
      "step": 37
    },
    {
      "epoch": 0.1351111111111111,
      "grad_norm": 4.149404525756836,
      "learning_rate": 7.6e-05,
      "loss": 0.3787,
      "step": 38
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 2.64218807220459,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.3895,
      "step": 39
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 2.675976276397705,
      "learning_rate": 8e-05,
      "loss": 0.2474,
      "step": 40
    },
    {
      "epoch": 0.14577777777777778,
      "grad_norm": 2.768864870071411,
      "learning_rate": 8.2e-05,
      "loss": 0.385,
      "step": 41
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 1.4825807809829712,
      "learning_rate": 8.4e-05,
      "loss": 0.3394,
      "step": 42
    },
    {
      "epoch": 0.15288888888888888,
      "grad_norm": 3.253200054168701,
      "learning_rate": 8.6e-05,
      "loss": 0.3079,
      "step": 43
    },
    {
      "epoch": 0.15644444444444444,
      "grad_norm": 2.2768654823303223,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.4342,
      "step": 44
    },
    {
      "epoch": 0.16,
      "grad_norm": 3.1827032566070557,
      "learning_rate": 9e-05,
      "loss": 0.4667,
      "step": 45
    },
    {
      "epoch": 0.16355555555555557,
      "grad_norm": 1.919540286064148,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.2874,
      "step": 46
    },
    {
      "epoch": 0.1671111111111111,
      "grad_norm": 3.010023832321167,
      "learning_rate": 9.4e-05,
      "loss": 0.3881,
      "step": 47
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 2.585278272628784,
      "learning_rate": 9.6e-05,
      "loss": 0.3444,
      "step": 48
    },
    {
      "epoch": 0.17422222222222222,
      "grad_norm": 2.067357301712036,
      "learning_rate": 9.8e-05,
      "loss": 0.3481,
      "step": 49
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 2.086641550064087,
      "learning_rate": 0.0001,
      "loss": 0.3352,
      "step": 50
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 1.858405351638794,
      "learning_rate": 0.00010200000000000001,
      "loss": 0.2444,
      "step": 51
    },
    {
      "epoch": 0.18488888888888888,
      "grad_norm": 2.0978174209594727,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.2815,
      "step": 52
    },
    {
      "epoch": 0.18844444444444444,
      "grad_norm": 2.331804037094116,
      "learning_rate": 0.00010600000000000002,
      "loss": 0.2591,
      "step": 53
    },
    {
      "epoch": 0.192,
      "grad_norm": 2.1340763568878174,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.2995,
      "step": 54
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 1.2830572128295898,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.1995,
      "step": 55
    },
    {
      "epoch": 0.1991111111111111,
      "grad_norm": 2.966412305831909,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.4168,
      "step": 56
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 2.0543718338012695,
      "learning_rate": 0.00011399999999999999,
      "loss": 0.3519,
      "step": 57
    },
    {
      "epoch": 0.20622222222222222,
      "grad_norm": 2.7467586994171143,
      "learning_rate": 0.000116,
      "loss": 0.3311,
      "step": 58
    },
    {
      "epoch": 0.20977777777777779,
      "grad_norm": 2.949787139892578,
      "learning_rate": 0.000118,
      "loss": 0.5769,
      "step": 59
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 2.724496603012085,
      "learning_rate": 0.00012,
      "loss": 0.3724,
      "step": 60
    },
    {
      "epoch": 0.21688888888888888,
      "grad_norm": 1.9476864337921143,
      "learning_rate": 0.000122,
      "loss": 0.2985,
      "step": 61
    },
    {
      "epoch": 0.22044444444444444,
      "grad_norm": 1.966278076171875,
      "learning_rate": 0.000124,
      "loss": 0.3657,
      "step": 62
    },
    {
      "epoch": 0.224,
      "grad_norm": 3.014005661010742,
      "learning_rate": 0.000126,
      "loss": 0.3509,
      "step": 63
    },
    {
      "epoch": 0.22755555555555557,
      "grad_norm": 2.775313138961792,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.3619,
      "step": 64
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 1.750035047531128,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.1748,
      "step": 65
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 1.2672014236450195,
      "learning_rate": 0.000132,
      "loss": 0.1959,
      "step": 66
    },
    {
      "epoch": 0.23822222222222222,
      "grad_norm": 1.5879456996917725,
      "learning_rate": 0.000134,
      "loss": 0.2962,
      "step": 67
    },
    {
      "epoch": 0.24177777777777779,
      "grad_norm": 2.333885669708252,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.3883,
      "step": 68
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 2.0753962993621826,
      "learning_rate": 0.000138,
      "loss": 0.3446,
      "step": 69
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 2.7087621688842773,
      "learning_rate": 0.00014,
      "loss": 0.4443,
      "step": 70
    },
    {
      "epoch": 0.25244444444444447,
      "grad_norm": 2.7104108333587646,
      "learning_rate": 0.000142,
      "loss": 0.3386,
      "step": 71
    },
    {
      "epoch": 0.256,
      "grad_norm": 1.6935445070266724,
      "learning_rate": 0.000144,
      "loss": 0.2315,
      "step": 72
    },
    {
      "epoch": 0.25955555555555554,
      "grad_norm": 4.171173572540283,
      "learning_rate": 0.000146,
      "loss": 0.346,
      "step": 73
    },
    {
      "epoch": 0.26311111111111113,
      "grad_norm": 2.462888240814209,
      "learning_rate": 0.000148,
      "loss": 0.4549,
      "step": 74
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.148110866546631,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.3579,
      "step": 75
    },
    {
      "epoch": 0.2702222222222222,
      "grad_norm": 3.4548888206481934,
      "learning_rate": 0.000152,
      "loss": 0.651,
      "step": 76
    },
    {
      "epoch": 0.2737777777777778,
      "grad_norm": 1.7865114212036133,
      "learning_rate": 0.000154,
      "loss": 0.2341,
      "step": 77
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 2.6402270793914795,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.2925,
      "step": 78
    },
    {
      "epoch": 0.2808888888888889,
      "grad_norm": 1.8999165296554565,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.2642,
      "step": 79
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 1.4973293542861938,
      "learning_rate": 0.00016,
      "loss": 0.2485,
      "step": 80
    },
    {
      "epoch": 0.288,
      "grad_norm": 1.8843357563018799,
      "learning_rate": 0.000162,
      "loss": 0.242,
      "step": 81
    },
    {
      "epoch": 0.29155555555555557,
      "grad_norm": 1.9240669012069702,
      "learning_rate": 0.000164,
      "loss": 0.2369,
      "step": 82
    },
    {
      "epoch": 0.2951111111111111,
      "grad_norm": 1.6475318670272827,
      "learning_rate": 0.000166,
      "loss": 0.2531,
      "step": 83
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 1.9739848375320435,
      "learning_rate": 0.000168,
      "loss": 0.2853,
      "step": 84
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 3.3147940635681152,
      "learning_rate": 0.00017,
      "loss": 0.3842,
      "step": 85
    },
    {
      "epoch": 0.30577777777777776,
      "grad_norm": 2.817629337310791,
      "learning_rate": 0.000172,
      "loss": 0.2843,
      "step": 86
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 1.3192296028137207,
      "learning_rate": 0.000174,
      "loss": 0.2014,
      "step": 87
    },
    {
      "epoch": 0.3128888888888889,
      "grad_norm": 2.1299753189086914,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.2885,
      "step": 88
    },
    {
      "epoch": 0.3164444444444444,
      "grad_norm": 2.9361979961395264,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.35,
      "step": 89
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.9590184688568115,
      "learning_rate": 0.00018,
      "loss": 0.3733,
      "step": 90
    },
    {
      "epoch": 0.32355555555555554,
      "grad_norm": 4.02506160736084,
      "learning_rate": 0.000182,
      "loss": 0.3398,
      "step": 91
    },
    {
      "epoch": 0.32711111111111113,
      "grad_norm": 3.105518102645874,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.3365,
      "step": 92
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 2.257131814956665,
      "learning_rate": 0.00018600000000000002,
      "loss": 0.289,
      "step": 93
    },
    {
      "epoch": 0.3342222222222222,
      "grad_norm": 2.4724838733673096,
      "learning_rate": 0.000188,
      "loss": 0.3551,
      "step": 94
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": NaN,
      "learning_rate": 0.000188,
      "loss": 0.2554,
      "step": 95
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 3.408928871154785,
      "learning_rate": 0.00019,
      "loss": 0.3003,
      "step": 96
    },
    {
      "epoch": 0.3448888888888889,
      "grad_norm": 2.685626268386841,
      "learning_rate": 0.000192,
      "loss": 0.2019,
      "step": 97
    },
    {
      "epoch": 0.34844444444444445,
      "grad_norm": 1.7403990030288696,
      "learning_rate": 0.000194,
      "loss": 0.2584,
      "step": 98
    },
    {
      "epoch": 0.352,
      "grad_norm": 1.917219877243042,
      "learning_rate": 0.000196,
      "loss": 0.3454,
      "step": 99
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 2.2796175479888916,
      "learning_rate": 0.00019800000000000002,
      "loss": 0.3692,
      "step": 100
    },
    {
      "epoch": 0.3591111111111111,
      "grad_norm": 3.7576513290405273,
      "learning_rate": 0.0002,
      "loss": 0.3233,
      "step": 101
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 2.7825419902801514,
      "learning_rate": 0.0001997308209959623,
      "loss": 0.324,
      "step": 102
    },
    {
      "epoch": 0.3662222222222222,
      "grad_norm": 1.9354801177978516,
      "learning_rate": 0.00019946164199192463,
      "loss": 0.2521,
      "step": 103
    },
    {
      "epoch": 0.36977777777777776,
      "grad_norm": 4.425552845001221,
      "learning_rate": 0.00019919246298788696,
      "loss": 0.3493,
      "step": 104
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.1518288850784302,
      "learning_rate": 0.0001989232839838493,
      "loss": 0.1605,
      "step": 105
    },
    {
      "epoch": 0.3768888888888889,
      "grad_norm": 1.723423957824707,
      "learning_rate": 0.00019865410497981159,
      "loss": 0.2076,
      "step": 106
    },
    {
      "epoch": 0.3804444444444444,
      "grad_norm": 2.26922607421875,
      "learning_rate": 0.00019838492597577389,
      "loss": 0.2288,
      "step": 107
    },
    {
      "epoch": 0.384,
      "grad_norm": 2.1743366718292236,
      "learning_rate": 0.0001981157469717362,
      "loss": 0.2549,
      "step": 108
    },
    {
      "epoch": 0.38755555555555554,
      "grad_norm": 2.402208089828491,
      "learning_rate": 0.00019784656796769854,
      "loss": 0.3581,
      "step": 109
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 2.5873332023620605,
      "learning_rate": 0.00019757738896366084,
      "loss": 0.4092,
      "step": 110
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 2.9529976844787598,
      "learning_rate": 0.00019730820995962316,
      "loss": 0.3054,
      "step": 111
    },
    {
      "epoch": 0.3982222222222222,
      "grad_norm": 1.8943376541137695,
      "learning_rate": 0.00019703903095558546,
      "loss": 0.2481,
      "step": 112
    },
    {
      "epoch": 0.4017777777777778,
      "grad_norm": 3.0948398113250732,
      "learning_rate": 0.0001967698519515478,
      "loss": 0.2246,
      "step": 113
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 2.4627091884613037,
      "learning_rate": 0.00019650067294751011,
      "loss": 0.2093,
      "step": 114
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 2.5407731533050537,
      "learning_rate": 0.0001962314939434724,
      "loss": 0.2969,
      "step": 115
    },
    {
      "epoch": 0.41244444444444445,
      "grad_norm": 2.0906620025634766,
      "learning_rate": 0.00019596231493943474,
      "loss": 0.1868,
      "step": 116
    },
    {
      "epoch": 0.416,
      "grad_norm": 2.966822624206543,
      "learning_rate": 0.00019569313593539704,
      "loss": 0.4307,
      "step": 117
    },
    {
      "epoch": 0.41955555555555557,
      "grad_norm": 1.9838496446609497,
      "learning_rate": 0.00019542395693135936,
      "loss": 0.1731,
      "step": 118
    },
    {
      "epoch": 0.4231111111111111,
      "grad_norm": 1.5668261051177979,
      "learning_rate": 0.0001951547779273217,
      "loss": 0.194,
      "step": 119
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 2.1374614238739014,
      "learning_rate": 0.000194885598923284,
      "loss": 0.3963,
      "step": 120
    },
    {
      "epoch": 0.43022222222222223,
      "grad_norm": 2.929614305496216,
      "learning_rate": 0.00019461641991924632,
      "loss": 0.232,
      "step": 121
    },
    {
      "epoch": 0.43377777777777776,
      "grad_norm": 2.1057510375976562,
      "learning_rate": 0.00019434724091520861,
      "loss": 0.1338,
      "step": 122
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 2.0190625190734863,
      "learning_rate": 0.00019407806191117094,
      "loss": 0.2558,
      "step": 123
    },
    {
      "epoch": 0.4408888888888889,
      "grad_norm": 2.6725053787231445,
      "learning_rate": 0.00019380888290713327,
      "loss": 0.3304,
      "step": 124
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.899870753288269,
      "learning_rate": 0.00019353970390309557,
      "loss": 0.1209,
      "step": 125
    },
    {
      "epoch": 0.448,
      "grad_norm": 1.9068149328231812,
      "learning_rate": 0.0001932705248990579,
      "loss": 0.2642,
      "step": 126
    },
    {
      "epoch": 0.45155555555555554,
      "grad_norm": 2.0267372131347656,
      "learning_rate": 0.0001930013458950202,
      "loss": 0.2833,
      "step": 127
    },
    {
      "epoch": 0.45511111111111113,
      "grad_norm": 2.4475369453430176,
      "learning_rate": 0.0001927321668909825,
      "loss": 0.3487,
      "step": 128
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 2.543243408203125,
      "learning_rate": 0.00019246298788694484,
      "loss": 0.2796,
      "step": 129
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 2.0898385047912598,
      "learning_rate": 0.00019219380888290714,
      "loss": 0.2959,
      "step": 130
    },
    {
      "epoch": 0.4657777777777778,
      "grad_norm": 2.2002415657043457,
      "learning_rate": 0.00019192462987886947,
      "loss": 0.2248,
      "step": 131
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 2.2708232402801514,
      "learning_rate": 0.00019165545087483177,
      "loss": 0.0763,
      "step": 132
    },
    {
      "epoch": 0.4728888888888889,
      "grad_norm": 2.3125011920928955,
      "learning_rate": 0.0001913862718707941,
      "loss": 0.3869,
      "step": 133
    },
    {
      "epoch": 0.47644444444444445,
      "grad_norm": 2.071300506591797,
      "learning_rate": 0.00019111709286675642,
      "loss": 0.1788,
      "step": 134
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.0568366050720215,
      "learning_rate": 0.00019084791386271872,
      "loss": 0.2375,
      "step": 135
    },
    {
      "epoch": 0.48355555555555557,
      "grad_norm": 3.717430830001831,
      "learning_rate": 0.00019057873485868102,
      "loss": 0.3842,
      "step": 136
    },
    {
      "epoch": 0.4871111111111111,
      "grad_norm": 4.826358318328857,
      "learning_rate": 0.00019030955585464334,
      "loss": 0.2193,
      "step": 137
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 2.633531332015991,
      "learning_rate": 0.00019004037685060567,
      "loss": 0.2146,
      "step": 138
    },
    {
      "epoch": 0.49422222222222223,
      "grad_norm": 2.0889182090759277,
      "learning_rate": 0.000189771197846568,
      "loss": 0.2804,
      "step": 139
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 2.4884886741638184,
      "learning_rate": 0.0001895020188425303,
      "loss": 0.3409,
      "step": 140
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 1.782486081123352,
      "learning_rate": 0.0001892328398384926,
      "loss": 0.2001,
      "step": 141
    },
    {
      "epoch": 0.5048888888888889,
      "grad_norm": 2.5042226314544678,
      "learning_rate": 0.00018896366083445492,
      "loss": 0.2012,
      "step": 142
    },
    {
      "epoch": 0.5084444444444445,
      "grad_norm": 2.069610595703125,
      "learning_rate": 0.00018869448183041725,
      "loss": 0.3522,
      "step": 143
    },
    {
      "epoch": 0.512,
      "grad_norm": 1.2324728965759277,
      "learning_rate": 0.00018842530282637955,
      "loss": 0.1014,
      "step": 144
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 1.5794532299041748,
      "learning_rate": 0.00018815612382234187,
      "loss": 0.3061,
      "step": 145
    },
    {
      "epoch": 0.5191111111111111,
      "grad_norm": 3.908510684967041,
      "learning_rate": 0.00018788694481830417,
      "loss": 0.3286,
      "step": 146
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 2.5637147426605225,
      "learning_rate": 0.0001876177658142665,
      "loss": 0.2174,
      "step": 147
    },
    {
      "epoch": 0.5262222222222223,
      "grad_norm": 2.3713529109954834,
      "learning_rate": 0.00018734858681022882,
      "loss": 0.2852,
      "step": 148
    },
    {
      "epoch": 0.5297777777777778,
      "grad_norm": 1.4986106157302856,
      "learning_rate": 0.00018707940780619112,
      "loss": 0.2175,
      "step": 149
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.3081369400024414,
      "learning_rate": 0.00018681022880215345,
      "loss": 0.3525,
      "step": 150
    },
    {
      "epoch": 0.5368888888888889,
      "grad_norm": 1.1330920457839966,
      "learning_rate": 0.00018654104979811575,
      "loss": 0.2305,
      "step": 151
    },
    {
      "epoch": 0.5404444444444444,
      "grad_norm": 1.8200173377990723,
      "learning_rate": 0.00018627187079407807,
      "loss": 0.1414,
      "step": 152
    },
    {
      "epoch": 0.544,
      "grad_norm": 2.4780478477478027,
      "learning_rate": 0.0001860026917900404,
      "loss": 0.3989,
      "step": 153
    },
    {
      "epoch": 0.5475555555555556,
      "grad_norm": 1.3920825719833374,
      "learning_rate": 0.0001857335127860027,
      "loss": 0.2132,
      "step": 154
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 1.3217829465866089,
      "learning_rate": 0.00018546433378196502,
      "loss": 0.2708,
      "step": 155
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 1.7324903011322021,
      "learning_rate": 0.00018519515477792732,
      "loss": 0.1974,
      "step": 156
    },
    {
      "epoch": 0.5582222222222222,
      "grad_norm": 1.5910848379135132,
      "learning_rate": 0.00018492597577388965,
      "loss": 0.2796,
      "step": 157
    },
    {
      "epoch": 0.5617777777777778,
      "grad_norm": 1.476298451423645,
      "learning_rate": 0.00018465679676985198,
      "loss": 0.1841,
      "step": 158
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.8249367475509644,
      "learning_rate": 0.00018438761776581427,
      "loss": 0.0779,
      "step": 159
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 1.2336952686309814,
      "learning_rate": 0.0001841184387617766,
      "loss": 0.2491,
      "step": 160
    },
    {
      "epoch": 0.5724444444444444,
      "grad_norm": 1.4487377405166626,
      "learning_rate": 0.0001838492597577389,
      "loss": 0.2115,
      "step": 161
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.867075800895691,
      "learning_rate": 0.0001835800807537012,
      "loss": 0.2581,
      "step": 162
    },
    {
      "epoch": 0.5795555555555556,
      "grad_norm": 2.0589351654052734,
      "learning_rate": 0.00018331090174966355,
      "loss": 0.3636,
      "step": 163
    },
    {
      "epoch": 0.5831111111111111,
      "grad_norm": 1.223888635635376,
      "learning_rate": 0.00018304172274562585,
      "loss": 0.1632,
      "step": 164
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.8868670463562012,
      "learning_rate": 0.00018277254374158818,
      "loss": 0.1154,
      "step": 165
    },
    {
      "epoch": 0.5902222222222222,
      "grad_norm": 1.0037362575531006,
      "learning_rate": 0.00018250336473755048,
      "loss": 0.0679,
      "step": 166
    },
    {
      "epoch": 0.5937777777777777,
      "grad_norm": 2.382289409637451,
      "learning_rate": 0.00018223418573351277,
      "loss": 0.3042,
      "step": 167
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 4.162321090698242,
      "learning_rate": 0.00018196500672947513,
      "loss": 0.2308,
      "step": 168
    },
    {
      "epoch": 0.6008888888888889,
      "grad_norm": 3.119981527328491,
      "learning_rate": 0.00018169582772543743,
      "loss": 0.1923,
      "step": 169
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 3.682765483856201,
      "learning_rate": 0.00018142664872139973,
      "loss": 0.3643,
      "step": 170
    },
    {
      "epoch": 0.608,
      "grad_norm": 2.835350513458252,
      "learning_rate": 0.00018115746971736205,
      "loss": 0.2394,
      "step": 171
    },
    {
      "epoch": 0.6115555555555555,
      "grad_norm": 2.471010446548462,
      "learning_rate": 0.00018088829071332435,
      "loss": 0.2621,
      "step": 172
    },
    {
      "epoch": 0.6151111111111112,
      "grad_norm": 1.4412662982940674,
      "learning_rate": 0.0001806191117092867,
      "loss": 0.2666,
      "step": 173
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 1.7593368291854858,
      "learning_rate": 0.000180349932705249,
      "loss": 0.3347,
      "step": 174
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 2.4260776042938232,
      "learning_rate": 0.0001800807537012113,
      "loss": 0.1722,
      "step": 175
    },
    {
      "epoch": 0.6257777777777778,
      "grad_norm": 4.9795308113098145,
      "learning_rate": 0.00017981157469717363,
      "loss": 0.3212,
      "step": 176
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 1.2852580547332764,
      "learning_rate": 0.00017954239569313593,
      "loss": 0.1718,
      "step": 177
    },
    {
      "epoch": 0.6328888888888888,
      "grad_norm": 1.905150055885315,
      "learning_rate": 0.00017927321668909825,
      "loss": 0.2978,
      "step": 178
    },
    {
      "epoch": 0.6364444444444445,
      "grad_norm": 2.2048795223236084,
      "learning_rate": 0.00017900403768506058,
      "loss": 0.5193,
      "step": 179
    },
    {
      "epoch": 0.64,
      "grad_norm": 2.9970881938934326,
      "learning_rate": 0.00017873485868102288,
      "loss": 0.1977,
      "step": 180
    },
    {
      "epoch": 0.6435555555555555,
      "grad_norm": 2.091374635696411,
      "learning_rate": 0.0001784656796769852,
      "loss": 0.5853,
      "step": 181
    },
    {
      "epoch": 0.6471111111111111,
      "grad_norm": 1.4068173170089722,
      "learning_rate": 0.0001781965006729475,
      "loss": 0.1666,
      "step": 182
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 1.2437806129455566,
      "learning_rate": 0.00017792732166890983,
      "loss": 0.1435,
      "step": 183
    },
    {
      "epoch": 0.6542222222222223,
      "grad_norm": 2.0126190185546875,
      "learning_rate": 0.00017765814266487216,
      "loss": 0.2322,
      "step": 184
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 1.98829185962677,
      "learning_rate": 0.00017738896366083446,
      "loss": 0.2324,
      "step": 185
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 2.6868340969085693,
      "learning_rate": 0.00017711978465679678,
      "loss": 0.2478,
      "step": 186
    },
    {
      "epoch": 0.6648888888888889,
      "grad_norm": 1.6848866939544678,
      "learning_rate": 0.00017685060565275908,
      "loss": 0.2682,
      "step": 187
    },
    {
      "epoch": 0.6684444444444444,
      "grad_norm": 2.379966974258423,
      "learning_rate": 0.0001765814266487214,
      "loss": 0.2035,
      "step": 188
    },
    {
      "epoch": 0.672,
      "grad_norm": 1.8276042938232422,
      "learning_rate": 0.00017631224764468373,
      "loss": 0.3099,
      "step": 189
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 1.8102283477783203,
      "learning_rate": 0.00017604306864064603,
      "loss": 0.2193,
      "step": 190
    },
    {
      "epoch": 0.6791111111111111,
      "grad_norm": 1.907231092453003,
      "learning_rate": 0.00017577388963660836,
      "loss": 0.263,
      "step": 191
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 1.8648825883865356,
      "learning_rate": 0.00017550471063257066,
      "loss": 0.1936,
      "step": 192
    },
    {
      "epoch": 0.6862222222222222,
      "grad_norm": 1.2324389219284058,
      "learning_rate": 0.00017523553162853298,
      "loss": 0.1676,
      "step": 193
    },
    {
      "epoch": 0.6897777777777778,
      "grad_norm": 1.618326187133789,
      "learning_rate": 0.0001749663526244953,
      "loss": 0.236,
      "step": 194
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 1.5929958820343018,
      "learning_rate": 0.0001746971736204576,
      "loss": 0.293,
      "step": 195
    },
    {
      "epoch": 0.6968888888888889,
      "grad_norm": 2.528024196624756,
      "learning_rate": 0.0001744279946164199,
      "loss": 0.3251,
      "step": 196
    },
    {
      "epoch": 0.7004444444444444,
      "grad_norm": 1.9959334135055542,
      "learning_rate": 0.00017415881561238226,
      "loss": 0.2855,
      "step": 197
    },
    {
      "epoch": 0.704,
      "grad_norm": 1.680546760559082,
      "learning_rate": 0.00017388963660834456,
      "loss": 0.1529,
      "step": 198
    },
    {
      "epoch": 0.7075555555555556,
      "grad_norm": 2.4446918964385986,
      "learning_rate": 0.00017362045760430689,
      "loss": 0.2621,
      "step": 199
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.0808333158493042,
      "learning_rate": 0.00017335127860026918,
      "loss": 0.1761,
      "step": 200
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 1.3534152507781982,
      "learning_rate": 0.00017308209959623148,
      "loss": 0.1856,
      "step": 201
    },
    {
      "epoch": 0.7182222222222222,
      "grad_norm": 1.4481234550476074,
      "learning_rate": 0.00017281292059219384,
      "loss": 0.0823,
      "step": 202
    },
    {
      "epoch": 0.7217777777777777,
      "grad_norm": 1.3327528238296509,
      "learning_rate": 0.00017254374158815614,
      "loss": 0.2145,
      "step": 203
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 1.8004844188690186,
      "learning_rate": 0.00017227456258411844,
      "loss": 0.2157,
      "step": 204
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 1.2512668371200562,
      "learning_rate": 0.00017200538358008076,
      "loss": 0.2153,
      "step": 205
    },
    {
      "epoch": 0.7324444444444445,
      "grad_norm": 1.6947021484375,
      "learning_rate": 0.00017173620457604306,
      "loss": 0.1922,
      "step": 206
    },
    {
      "epoch": 0.736,
      "grad_norm": 1.1330373287200928,
      "learning_rate": 0.0001714670255720054,
      "loss": 0.1398,
      "step": 207
    },
    {
      "epoch": 0.7395555555555555,
      "grad_norm": 1.4576880931854248,
      "learning_rate": 0.0001711978465679677,
      "loss": 0.2029,
      "step": 208
    },
    {
      "epoch": 0.7431111111111111,
      "grad_norm": 2.296070098876953,
      "learning_rate": 0.00017092866756393,
      "loss": 0.2607,
      "step": 209
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 1.3249839544296265,
      "learning_rate": 0.00017065948855989234,
      "loss": 0.1416,
      "step": 210
    },
    {
      "epoch": 0.7502222222222222,
      "grad_norm": 1.1995031833648682,
      "learning_rate": 0.00017039030955585464,
      "loss": 0.183,
      "step": 211
    },
    {
      "epoch": 0.7537777777777778,
      "grad_norm": 1.486039161682129,
      "learning_rate": 0.00017012113055181696,
      "loss": 0.2243,
      "step": 212
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 1.8418980836868286,
      "learning_rate": 0.0001698519515477793,
      "loss": 0.2853,
      "step": 213
    },
    {
      "epoch": 0.7608888888888888,
      "grad_norm": 2.1599745750427246,
      "learning_rate": 0.0001695827725437416,
      "loss": 0.2419,
      "step": 214
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 1.7430497407913208,
      "learning_rate": 0.00016931359353970391,
      "loss": 0.1959,
      "step": 215
    },
    {
      "epoch": 0.768,
      "grad_norm": 1.4764467477798462,
      "learning_rate": 0.0001690444145356662,
      "loss": 0.159,
      "step": 216
    },
    {
      "epoch": 0.7715555555555556,
      "grad_norm": 2.072406530380249,
      "learning_rate": 0.00016877523553162854,
      "loss": 0.1852,
      "step": 217
    },
    {
      "epoch": 0.7751111111111111,
      "grad_norm": 2.86185359954834,
      "learning_rate": 0.00016850605652759087,
      "loss": 0.4518,
      "step": 218
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.9642516374588013,
      "learning_rate": 0.00016823687752355316,
      "loss": 0.1079,
      "step": 219
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 2.3428187370300293,
      "learning_rate": 0.0001679676985195155,
      "loss": 0.3006,
      "step": 220
    },
    {
      "epoch": 0.7857777777777778,
      "grad_norm": 1.9015491008758545,
      "learning_rate": 0.0001676985195154778,
      "loss": 0.2311,
      "step": 221
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 1.191836953163147,
      "learning_rate": 0.00016742934051144012,
      "loss": 0.2152,
      "step": 222
    },
    {
      "epoch": 0.7928888888888889,
      "grad_norm": 2.4765751361846924,
      "learning_rate": 0.00016716016150740244,
      "loss": 0.2068,
      "step": 223
    },
    {
      "epoch": 0.7964444444444444,
      "grad_norm": 1.5483460426330566,
      "learning_rate": 0.00016689098250336474,
      "loss": 0.2081,
      "step": 224
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.9021528363227844,
      "learning_rate": 0.00016662180349932707,
      "loss": 0.1493,
      "step": 225
    },
    {
      "epoch": 0.8035555555555556,
      "grad_norm": 1.4927772283554077,
      "learning_rate": 0.00016635262449528937,
      "loss": 0.2473,
      "step": 226
    },
    {
      "epoch": 0.8071111111111111,
      "grad_norm": 1.1591830253601074,
      "learning_rate": 0.0001660834454912517,
      "loss": 0.2376,
      "step": 227
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 1.164390206336975,
      "learning_rate": 0.00016581426648721402,
      "loss": 0.199,
      "step": 228
    },
    {
      "epoch": 0.8142222222222222,
      "grad_norm": 1.0501039028167725,
      "learning_rate": 0.00016554508748317632,
      "loss": 0.1698,
      "step": 229
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 1.0654584169387817,
      "learning_rate": 0.00016527590847913864,
      "loss": 0.1303,
      "step": 230
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 2.2311227321624756,
      "learning_rate": 0.00016500672947510094,
      "loss": 0.3039,
      "step": 231
    },
    {
      "epoch": 0.8248888888888889,
      "grad_norm": 2.269216299057007,
      "learning_rate": 0.00016473755047106327,
      "loss": 0.378,
      "step": 232
    },
    {
      "epoch": 0.8284444444444444,
      "grad_norm": 1.8481101989746094,
      "learning_rate": 0.0001644683714670256,
      "loss": 0.2731,
      "step": 233
    },
    {
      "epoch": 0.832,
      "grad_norm": 2.20630145072937,
      "learning_rate": 0.0001641991924629879,
      "loss": 0.2966,
      "step": 234
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 2.0484907627105713,
      "learning_rate": 0.0001639300134589502,
      "loss": 0.3392,
      "step": 235
    },
    {
      "epoch": 0.8391111111111111,
      "grad_norm": 4.147709846496582,
      "learning_rate": 0.00016366083445491252,
      "loss": 0.3866,
      "step": 236
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 2.426947593688965,
      "learning_rate": 0.00016339165545087484,
      "loss": 0.2747,
      "step": 237
    },
    {
      "epoch": 0.8462222222222222,
      "grad_norm": 1.9145969152450562,
      "learning_rate": 0.00016312247644683717,
      "loss": 0.3169,
      "step": 238
    },
    {
      "epoch": 0.8497777777777777,
      "grad_norm": 0.9128560423851013,
      "learning_rate": 0.00016285329744279947,
      "loss": 0.1246,
      "step": 239
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.9424158334732056,
      "learning_rate": 0.00016258411843876177,
      "loss": 0.1258,
      "step": 240
    },
    {
      "epoch": 0.8568888888888889,
      "grad_norm": 1.200623869895935,
      "learning_rate": 0.0001623149394347241,
      "loss": 0.139,
      "step": 241
    },
    {
      "epoch": 0.8604444444444445,
      "grad_norm": 1.628354549407959,
      "learning_rate": 0.00016204576043068642,
      "loss": 0.2455,
      "step": 242
    },
    {
      "epoch": 0.864,
      "grad_norm": 1.2771495580673218,
      "learning_rate": 0.00016177658142664872,
      "loss": 0.261,
      "step": 243
    },
    {
      "epoch": 0.8675555555555555,
      "grad_norm": 2.3363494873046875,
      "learning_rate": 0.00016150740242261105,
      "loss": 0.2937,
      "step": 244
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 1.1929019689559937,
      "learning_rate": 0.00016123822341857335,
      "loss": 0.2873,
      "step": 245
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 1.126509189605713,
      "learning_rate": 0.00016096904441453567,
      "loss": 0.2059,
      "step": 246
    },
    {
      "epoch": 0.8782222222222222,
      "grad_norm": 0.8706187009811401,
      "learning_rate": 0.000160699865410498,
      "loss": 0.1908,
      "step": 247
    },
    {
      "epoch": 0.8817777777777778,
      "grad_norm": 2.1332030296325684,
      "learning_rate": 0.0001604306864064603,
      "loss": 0.2325,
      "step": 248
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 1.9623475074768066,
      "learning_rate": 0.00016016150740242262,
      "loss": 0.2742,
      "step": 249
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.5290945768356323,
      "learning_rate": 0.00015989232839838492,
      "loss": 0.1715,
      "step": 250
    },
    {
      "epoch": 0.8924444444444445,
      "grad_norm": 0.8689821362495422,
      "learning_rate": 0.00015962314939434725,
      "loss": 0.1007,
      "step": 251
    },
    {
      "epoch": 0.896,
      "grad_norm": 1.910539984703064,
      "learning_rate": 0.00015935397039030957,
      "loss": 0.2753,
      "step": 252
    },
    {
      "epoch": 0.8995555555555556,
      "grad_norm": 1.6747769117355347,
      "learning_rate": 0.00015908479138627187,
      "loss": 0.3602,
      "step": 253
    },
    {
      "epoch": 0.9031111111111111,
      "grad_norm": 2.0330708026885986,
      "learning_rate": 0.0001588156123822342,
      "loss": 0.2916,
      "step": 254
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 1.6225824356079102,
      "learning_rate": 0.0001585464333781965,
      "loss": 0.2618,
      "step": 255
    },
    {
      "epoch": 0.9102222222222223,
      "grad_norm": 0.9529979825019836,
      "learning_rate": 0.00015827725437415882,
      "loss": 0.1258,
      "step": 256
    },
    {
      "epoch": 0.9137777777777778,
      "grad_norm": 1.34640634059906,
      "learning_rate": 0.00015800807537012115,
      "loss": 0.231,
      "step": 257
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 1.2468105554580688,
      "learning_rate": 0.00015773889636608345,
      "loss": 0.1594,
      "step": 258
    },
    {
      "epoch": 0.9208888888888889,
      "grad_norm": 1.0532536506652832,
      "learning_rate": 0.00015746971736204578,
      "loss": 0.0967,
      "step": 259
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 2.949993371963501,
      "learning_rate": 0.00015720053835800807,
      "loss": 0.4647,
      "step": 260
    },
    {
      "epoch": 0.928,
      "grad_norm": 1.7001549005508423,
      "learning_rate": 0.0001569313593539704,
      "loss": 0.2612,
      "step": 261
    },
    {
      "epoch": 0.9315555555555556,
      "grad_norm": 1.3044205904006958,
      "learning_rate": 0.00015666218034993273,
      "loss": 0.181,
      "step": 262
    },
    {
      "epoch": 0.9351111111111111,
      "grad_norm": 1.9322819709777832,
      "learning_rate": 0.00015639300134589503,
      "loss": 0.3161,
      "step": 263
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 1.4254404306411743,
      "learning_rate": 0.00015612382234185735,
      "loss": 0.1846,
      "step": 264
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 1.1254563331604004,
      "learning_rate": 0.00015585464333781965,
      "loss": 0.1886,
      "step": 265
    },
    {
      "epoch": 0.9457777777777778,
      "grad_norm": 1.1051491498947144,
      "learning_rate": 0.00015558546433378198,
      "loss": 0.1768,
      "step": 266
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 1.5113861560821533,
      "learning_rate": 0.0001553162853297443,
      "loss": 0.2549,
      "step": 267
    },
    {
      "epoch": 0.9528888888888889,
      "grad_norm": 1.8936904668807983,
      "learning_rate": 0.0001550471063257066,
      "loss": 0.3379,
      "step": 268
    },
    {
      "epoch": 0.9564444444444444,
      "grad_norm": 0.9338057637214661,
      "learning_rate": 0.0001547779273216689,
      "loss": 0.1094,
      "step": 269
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.6018155813217163,
      "learning_rate": 0.00015450874831763123,
      "loss": 0.1757,
      "step": 270
    },
    {
      "epoch": 0.9635555555555556,
      "grad_norm": 2.1283836364746094,
      "learning_rate": 0.00015423956931359355,
      "loss": 0.2269,
      "step": 271
    },
    {
      "epoch": 0.9671111111111111,
      "grad_norm": 2.097332715988159,
      "learning_rate": 0.00015397039030955588,
      "loss": 0.2735,
      "step": 272
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 1.9954752922058105,
      "learning_rate": 0.00015370121130551818,
      "loss": 0.3097,
      "step": 273
    },
    {
      "epoch": 0.9742222222222222,
      "grad_norm": 2.2187533378601074,
      "learning_rate": 0.00015343203230148048,
      "loss": 0.2678,
      "step": 274
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.06121826171875,
      "learning_rate": 0.0001531628532974428,
      "loss": 0.1606,
      "step": 275
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 1.46017324924469,
      "learning_rate": 0.00015289367429340513,
      "loss": 0.2079,
      "step": 276
    },
    {
      "epoch": 0.9848888888888889,
      "grad_norm": 1.1075791120529175,
      "learning_rate": 0.00015262449528936743,
      "loss": 0.1655,
      "step": 277
    },
    {
      "epoch": 0.9884444444444445,
      "grad_norm": 1.8120288848876953,
      "learning_rate": 0.00015235531628532976,
      "loss": 0.1545,
      "step": 278
    },
    {
      "epoch": 0.992,
      "grad_norm": 2.390756130218506,
      "learning_rate": 0.00015208613728129205,
      "loss": 0.3616,
      "step": 279
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 2.1333436965942383,
      "learning_rate": 0.00015181695827725438,
      "loss": 0.457,
      "step": 280
    },
    {
      "epoch": 0.9991111111111111,
      "grad_norm": 1.1114246845245361,
      "learning_rate": 0.0001515477792732167,
      "loss": 0.1664,
      "step": 281
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.9711302518844604,
      "learning_rate": 0.000151278600269179,
      "loss": 0.1547,
      "step": 282
    },
    {
      "epoch": 1.0035555555555555,
      "grad_norm": 3.3716697692871094,
      "learning_rate": 0.00015100942126514133,
      "loss": 0.3885,
      "step": 283
    },
    {
      "epoch": 1.007111111111111,
      "grad_norm": 1.1015137434005737,
      "learning_rate": 0.00015074024226110363,
      "loss": 0.1535,
      "step": 284
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 1.1631591320037842,
      "learning_rate": 0.00015047106325706596,
      "loss": 0.183,
      "step": 285
    },
    {
      "epoch": 1.0142222222222221,
      "grad_norm": 0.8793090581893921,
      "learning_rate": 0.00015020188425302828,
      "loss": 0.0922,
      "step": 286
    },
    {
      "epoch": 1.0177777777777777,
      "grad_norm": 1.801939845085144,
      "learning_rate": 0.00014993270524899058,
      "loss": 0.1508,
      "step": 287
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 1.9405453205108643,
      "learning_rate": 0.0001496635262449529,
      "loss": 0.2897,
      "step": 288
    },
    {
      "epoch": 1.024888888888889,
      "grad_norm": 1.1193333864212036,
      "learning_rate": 0.0001493943472409152,
      "loss": 0.1557,
      "step": 289
    },
    {
      "epoch": 1.0284444444444445,
      "grad_norm": 0.8549714684486389,
      "learning_rate": 0.00014912516823687753,
      "loss": 0.0846,
      "step": 290
    },
    {
      "epoch": 1.032,
      "grad_norm": 1.2124375104904175,
      "learning_rate": 0.00014885598923283986,
      "loss": 0.1999,
      "step": 291
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 0.9481258392333984,
      "learning_rate": 0.00014858681022880216,
      "loss": 0.1328,
      "step": 292
    },
    {
      "epoch": 1.039111111111111,
      "grad_norm": 1.044527530670166,
      "learning_rate": 0.00014831763122476448,
      "loss": 0.1445,
      "step": 293
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.9684349298477173,
      "learning_rate": 0.00014804845222072678,
      "loss": 0.0502,
      "step": 294
    },
    {
      "epoch": 1.0462222222222222,
      "grad_norm": 2.46555757522583,
      "learning_rate": 0.00014777927321668908,
      "loss": 0.2329,
      "step": 295
    },
    {
      "epoch": 1.0497777777777777,
      "grad_norm": 1.6031893491744995,
      "learning_rate": 0.00014751009421265144,
      "loss": 0.116,
      "step": 296
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 1.2798947095870972,
      "learning_rate": 0.00014724091520861373,
      "loss": 0.1029,
      "step": 297
    },
    {
      "epoch": 1.056888888888889,
      "grad_norm": 1.616414189338684,
      "learning_rate": 0.00014697173620457606,
      "loss": 0.2111,
      "step": 298
    },
    {
      "epoch": 1.0604444444444445,
      "grad_norm": 1.058650016784668,
      "learning_rate": 0.00014670255720053836,
      "loss": 0.093,
      "step": 299
    },
    {
      "epoch": 1.064,
      "grad_norm": 1.2231093645095825,
      "learning_rate": 0.00014643337819650066,
      "loss": 0.1618,
      "step": 300
    },
    {
      "epoch": 1.0675555555555556,
      "grad_norm": 0.7709832787513733,
      "learning_rate": 0.000146164199192463,
      "loss": 0.0685,
      "step": 301
    },
    {
      "epoch": 1.0711111111111111,
      "grad_norm": 0.7199633121490479,
      "learning_rate": 0.0001458950201884253,
      "loss": 0.0699,
      "step": 302
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 1.5841765403747559,
      "learning_rate": 0.0001456258411843876,
      "loss": 0.1503,
      "step": 303
    },
    {
      "epoch": 1.0782222222222222,
      "grad_norm": 1.3483582735061646,
      "learning_rate": 0.00014535666218034994,
      "loss": 0.1812,
      "step": 304
    },
    {
      "epoch": 1.0817777777777777,
      "grad_norm": 1.0037134885787964,
      "learning_rate": 0.00014508748317631224,
      "loss": 0.0815,
      "step": 305
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 1.6594412326812744,
      "learning_rate": 0.0001448183041722746,
      "loss": 0.271,
      "step": 306
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.8589193820953369,
      "learning_rate": 0.0001445491251682369,
      "loss": 0.1438,
      "step": 307
    },
    {
      "epoch": 1.0924444444444443,
      "grad_norm": 2.0304477214813232,
      "learning_rate": 0.0001442799461641992,
      "loss": 0.1891,
      "step": 308
    },
    {
      "epoch": 1.096,
      "grad_norm": 1.148711919784546,
      "learning_rate": 0.0001440107671601615,
      "loss": 0.1279,
      "step": 309
    },
    {
      "epoch": 1.0995555555555556,
      "grad_norm": 1.8858845233917236,
      "learning_rate": 0.0001437415881561238,
      "loss": 0.197,
      "step": 310
    },
    {
      "epoch": 1.1031111111111112,
      "grad_norm": 1.6745566129684448,
      "learning_rate": 0.00014347240915208614,
      "loss": 0.1527,
      "step": 311
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 1.5631842613220215,
      "learning_rate": 0.00014320323014804846,
      "loss": 0.283,
      "step": 312
    },
    {
      "epoch": 1.1102222222222222,
      "grad_norm": 1.47993004322052,
      "learning_rate": 0.00014293405114401076,
      "loss": 0.1774,
      "step": 313
    },
    {
      "epoch": 1.1137777777777778,
      "grad_norm": 2.3405487537384033,
      "learning_rate": 0.0001426648721399731,
      "loss": 0.2671,
      "step": 314
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 1.3646390438079834,
      "learning_rate": 0.0001423956931359354,
      "loss": 0.2025,
      "step": 315
    },
    {
      "epoch": 1.1208888888888888,
      "grad_norm": 1.3259129524230957,
      "learning_rate": 0.00014212651413189771,
      "loss": 0.1483,
      "step": 316
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 3.0947630405426025,
      "learning_rate": 0.00014185733512786004,
      "loss": 0.2617,
      "step": 317
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 1.6526049375534058,
      "learning_rate": 0.00014158815612382234,
      "loss": 0.1936,
      "step": 318
    },
    {
      "epoch": 1.1315555555555556,
      "grad_norm": 1.0764763355255127,
      "learning_rate": 0.00014131897711978467,
      "loss": 0.1483,
      "step": 319
    },
    {
      "epoch": 1.1351111111111112,
      "grad_norm": 1.662041425704956,
      "learning_rate": 0.000141049798115747,
      "loss": 0.1703,
      "step": 320
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.6927356719970703,
      "learning_rate": 0.0001407806191117093,
      "loss": 0.062,
      "step": 321
    },
    {
      "epoch": 1.1422222222222222,
      "grad_norm": 1.347030520439148,
      "learning_rate": 0.00014051144010767162,
      "loss": 0.1069,
      "step": 322
    },
    {
      "epoch": 1.1457777777777778,
      "grad_norm": 1.226531982421875,
      "learning_rate": 0.00014024226110363392,
      "loss": 0.1057,
      "step": 323
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 2.153378963470459,
      "learning_rate": 0.00013997308209959624,
      "loss": 0.256,
      "step": 324
    },
    {
      "epoch": 1.1528888888888889,
      "grad_norm": 0.8056048154830933,
      "learning_rate": 0.00013970390309555857,
      "loss": 0.1065,
      "step": 325
    },
    {
      "epoch": 1.1564444444444444,
      "grad_norm": 1.3128670454025269,
      "learning_rate": 0.00013943472409152087,
      "loss": 0.1289,
      "step": 326
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.7728413343429565,
      "learning_rate": 0.0001391655450874832,
      "loss": 0.1944,
      "step": 327
    },
    {
      "epoch": 1.1635555555555555,
      "grad_norm": 1.1779669523239136,
      "learning_rate": 0.0001388963660834455,
      "loss": 0.1051,
      "step": 328
    },
    {
      "epoch": 1.1671111111111112,
      "grad_norm": 1.0403189659118652,
      "learning_rate": 0.00013862718707940782,
      "loss": 0.1234,
      "step": 329
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.8155553936958313,
      "learning_rate": 0.00013835800807537014,
      "loss": 0.0823,
      "step": 330
    },
    {
      "epoch": 1.1742222222222223,
      "grad_norm": 2.03707218170166,
      "learning_rate": 0.00013808882907133244,
      "loss": 0.308,
      "step": 331
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.8309100866317749,
      "learning_rate": 0.00013781965006729477,
      "loss": 0.0762,
      "step": 332
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 1.4044522047042847,
      "learning_rate": 0.00013755047106325707,
      "loss": 0.1735,
      "step": 333
    },
    {
      "epoch": 1.1848888888888889,
      "grad_norm": 0.9011748433113098,
      "learning_rate": 0.00013728129205921937,
      "loss": 0.1513,
      "step": 334
    },
    {
      "epoch": 1.1884444444444444,
      "grad_norm": 1.2139878273010254,
      "learning_rate": 0.00013701211305518172,
      "loss": 0.1971,
      "step": 335
    },
    {
      "epoch": 1.192,
      "grad_norm": 1.2557393312454224,
      "learning_rate": 0.00013674293405114402,
      "loss": 0.1466,
      "step": 336
    },
    {
      "epoch": 1.1955555555555555,
      "grad_norm": 2.107708215713501,
      "learning_rate": 0.00013647375504710635,
      "loss": 0.2283,
      "step": 337
    },
    {
      "epoch": 1.199111111111111,
      "grad_norm": 1.1470390558242798,
      "learning_rate": 0.00013620457604306865,
      "loss": 0.145,
      "step": 338
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 1.7137854099273682,
      "learning_rate": 0.00013593539703903094,
      "loss": 0.2404,
      "step": 339
    },
    {
      "epoch": 1.2062222222222223,
      "grad_norm": 1.7908140420913696,
      "learning_rate": 0.0001356662180349933,
      "loss": 0.2248,
      "step": 340
    },
    {
      "epoch": 1.2097777777777778,
      "grad_norm": 1.4163470268249512,
      "learning_rate": 0.0001353970390309556,
      "loss": 0.1936,
      "step": 341
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 2.3024520874023438,
      "learning_rate": 0.0001351278600269179,
      "loss": 0.1514,
      "step": 342
    },
    {
      "epoch": 1.216888888888889,
      "grad_norm": 1.6218980550765991,
      "learning_rate": 0.00013485868102288022,
      "loss": 0.1782,
      "step": 343
    },
    {
      "epoch": 1.2204444444444444,
      "grad_norm": 1.2429587841033936,
      "learning_rate": 0.00013458950201884252,
      "loss": 0.2113,
      "step": 344
    },
    {
      "epoch": 1.224,
      "grad_norm": 1.0917668342590332,
      "learning_rate": 0.00013432032301480487,
      "loss": 0.1832,
      "step": 345
    },
    {
      "epoch": 1.2275555555555555,
      "grad_norm": 1.9010086059570312,
      "learning_rate": 0.00013405114401076717,
      "loss": 0.2061,
      "step": 346
    },
    {
      "epoch": 1.231111111111111,
      "grad_norm": 1.3065053224563599,
      "learning_rate": 0.00013378196500672947,
      "loss": 0.1334,
      "step": 347
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 1.7331147193908691,
      "learning_rate": 0.0001335127860026918,
      "loss": 0.149,
      "step": 348
    },
    {
      "epoch": 1.2382222222222223,
      "grad_norm": 0.8300129175186157,
      "learning_rate": 0.0001332436069986541,
      "loss": 0.1152,
      "step": 349
    },
    {
      "epoch": 1.2417777777777779,
      "grad_norm": 0.9100944399833679,
      "learning_rate": 0.00013297442799461642,
      "loss": 0.084,
      "step": 350
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 1.0331075191497803,
      "learning_rate": 0.00013270524899057875,
      "loss": 0.0743,
      "step": 351
    },
    {
      "epoch": 1.248888888888889,
      "grad_norm": 1.6657640933990479,
      "learning_rate": 0.00013243606998654105,
      "loss": 0.1399,
      "step": 352
    },
    {
      "epoch": 1.2524444444444445,
      "grad_norm": 1.9556974172592163,
      "learning_rate": 0.00013216689098250337,
      "loss": 0.2214,
      "step": 353
    },
    {
      "epoch": 1.256,
      "grad_norm": 1.6011335849761963,
      "learning_rate": 0.00013189771197846567,
      "loss": 0.1764,
      "step": 354
    },
    {
      "epoch": 1.2595555555555555,
      "grad_norm": 1.8209842443466187,
      "learning_rate": 0.000131628532974428,
      "loss": 0.2599,
      "step": 355
    },
    {
      "epoch": 1.263111111111111,
      "grad_norm": 0.9177243709564209,
      "learning_rate": 0.00013135935397039033,
      "loss": 0.0989,
      "step": 356
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 2.2565267086029053,
      "learning_rate": 0.00013109017496635262,
      "loss": 0.2732,
      "step": 357
    },
    {
      "epoch": 1.2702222222222221,
      "grad_norm": 1.3750512599945068,
      "learning_rate": 0.00013082099596231495,
      "loss": 0.1637,
      "step": 358
    },
    {
      "epoch": 1.2737777777777777,
      "grad_norm": 1.6841477155685425,
      "learning_rate": 0.00013055181695827725,
      "loss": 0.2141,
      "step": 359
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 1.1940034627914429,
      "learning_rate": 0.00013028263795423958,
      "loss": 0.0772,
      "step": 360
    },
    {
      "epoch": 1.280888888888889,
      "grad_norm": 1.0710903406143188,
      "learning_rate": 0.0001300134589502019,
      "loss": 0.1686,
      "step": 361
    },
    {
      "epoch": 1.2844444444444445,
      "grad_norm": 1.8805502653121948,
      "learning_rate": 0.0001297442799461642,
      "loss": 0.1956,
      "step": 362
    },
    {
      "epoch": 1.288,
      "grad_norm": 2.040799856185913,
      "learning_rate": 0.00012947510094212653,
      "loss": 0.251,
      "step": 363
    },
    {
      "epoch": 1.2915555555555556,
      "grad_norm": 2.046177387237549,
      "learning_rate": 0.00012920592193808883,
      "loss": 0.1642,
      "step": 364
    },
    {
      "epoch": 1.295111111111111,
      "grad_norm": 3.614863395690918,
      "learning_rate": 0.00012893674293405115,
      "loss": 0.2975,
      "step": 365
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 1.1160032749176025,
      "learning_rate": 0.00012866756393001348,
      "loss": 0.1004,
      "step": 366
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 2.0393385887145996,
      "learning_rate": 0.00012839838492597578,
      "loss": 0.0843,
      "step": 367
    },
    {
      "epoch": 1.3057777777777777,
      "grad_norm": 1.4062014818191528,
      "learning_rate": 0.00012812920592193808,
      "loss": 0.0913,
      "step": 368
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 1.2522838115692139,
      "learning_rate": 0.0001278600269179004,
      "loss": 0.2333,
      "step": 369
    },
    {
      "epoch": 1.3128888888888888,
      "grad_norm": 2.1729235649108887,
      "learning_rate": 0.00012759084791386273,
      "loss": 0.0944,
      "step": 370
    },
    {
      "epoch": 1.3164444444444445,
      "grad_norm": 1.9654595851898193,
      "learning_rate": 0.00012732166890982505,
      "loss": 0.2534,
      "step": 371
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.4727864265441895,
      "learning_rate": 0.00012705248990578735,
      "loss": 0.2373,
      "step": 372
    },
    {
      "epoch": 1.3235555555555556,
      "grad_norm": 1.1611695289611816,
      "learning_rate": 0.00012678331090174965,
      "loss": 0.105,
      "step": 373
    },
    {
      "epoch": 1.3271111111111111,
      "grad_norm": 1.8822758197784424,
      "learning_rate": 0.00012651413189771198,
      "loss": 0.2042,
      "step": 374
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.8200066685676575,
      "learning_rate": 0.0001262449528936743,
      "loss": 0.1408,
      "step": 375
    },
    {
      "epoch": 1.3342222222222222,
      "grad_norm": 1.007015585899353,
      "learning_rate": 0.0001259757738896366,
      "loss": 0.0918,
      "step": 376
    },
    {
      "epoch": 1.3377777777777777,
      "grad_norm": 1.3979414701461792,
      "learning_rate": 0.00012570659488559893,
      "loss": 0.1886,
      "step": 377
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 1.5646698474884033,
      "learning_rate": 0.00012543741588156123,
      "loss": 0.1205,
      "step": 378
    },
    {
      "epoch": 1.3448888888888888,
      "grad_norm": 0.9475061893463135,
      "learning_rate": 0.00012516823687752356,
      "loss": 0.0634,
      "step": 379
    },
    {
      "epoch": 1.3484444444444446,
      "grad_norm": 2.2085065841674805,
      "learning_rate": 0.00012489905787348588,
      "loss": 0.2643,
      "step": 380
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 1.0029385089874268,
      "learning_rate": 0.00012462987886944818,
      "loss": 0.1019,
      "step": 381
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 1.6136457920074463,
      "learning_rate": 0.0001243606998654105,
      "loss": 0.2349,
      "step": 382
    },
    {
      "epoch": 1.3591111111111112,
      "grad_norm": 1.5356446504592896,
      "learning_rate": 0.0001240915208613728,
      "loss": 0.1577,
      "step": 383
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 1.5236259698867798,
      "learning_rate": 0.00012382234185733513,
      "loss": 0.184,
      "step": 384
    },
    {
      "epoch": 1.3662222222222222,
      "grad_norm": 2.3564229011535645,
      "learning_rate": 0.00012355316285329746,
      "loss": 0.1207,
      "step": 385
    },
    {
      "epoch": 1.3697777777777778,
      "grad_norm": 1.024688959121704,
      "learning_rate": 0.00012328398384925976,
      "loss": 0.0959,
      "step": 386
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 1.2558876276016235,
      "learning_rate": 0.00012301480484522208,
      "loss": 0.0968,
      "step": 387
    },
    {
      "epoch": 1.3768888888888888,
      "grad_norm": 0.901168704032898,
      "learning_rate": 0.00012274562584118438,
      "loss": 0.0507,
      "step": 388
    },
    {
      "epoch": 1.3804444444444444,
      "grad_norm": 1.2287055253982544,
      "learning_rate": 0.0001224764468371467,
      "loss": 0.1114,
      "step": 389
    },
    {
      "epoch": 1.384,
      "grad_norm": 1.7820336818695068,
      "learning_rate": 0.00012220726783310903,
      "loss": 0.1066,
      "step": 390
    },
    {
      "epoch": 1.3875555555555557,
      "grad_norm": 0.8306026458740234,
      "learning_rate": 0.00012193808882907133,
      "loss": 0.0688,
      "step": 391
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 2.032001256942749,
      "learning_rate": 0.00012166890982503365,
      "loss": 0.2484,
      "step": 392
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 2.016287088394165,
      "learning_rate": 0.00012139973082099596,
      "loss": 0.209,
      "step": 393
    },
    {
      "epoch": 1.3982222222222223,
      "grad_norm": 1.697540283203125,
      "learning_rate": 0.0001211305518169583,
      "loss": 0.2769,
      "step": 394
    },
    {
      "epoch": 1.4017777777777778,
      "grad_norm": 1.461546540260315,
      "learning_rate": 0.0001208613728129206,
      "loss": 0.1021,
      "step": 395
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 1.3945800065994263,
      "learning_rate": 0.00012059219380888291,
      "loss": 0.2549,
      "step": 396
    },
    {
      "epoch": 1.4088888888888889,
      "grad_norm": 0.9186742901802063,
      "learning_rate": 0.00012032301480484522,
      "loss": 0.0695,
      "step": 397
    },
    {
      "epoch": 1.4124444444444444,
      "grad_norm": 1.602012276649475,
      "learning_rate": 0.00012005383580080754,
      "loss": 0.1626,
      "step": 398
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.7302123308181763,
      "learning_rate": 0.00011978465679676986,
      "loss": 0.0424,
      "step": 399
    },
    {
      "epoch": 1.4195555555555557,
      "grad_norm": 1.199017882347107,
      "learning_rate": 0.00011951547779273217,
      "loss": 0.1269,
      "step": 400
    },
    {
      "epoch": 1.423111111111111,
      "grad_norm": 0.8213843703269958,
      "learning_rate": 0.00011924629878869449,
      "loss": 0.0801,
      "step": 401
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 2.7975287437438965,
      "learning_rate": 0.0001189771197846568,
      "loss": 0.2994,
      "step": 402
    },
    {
      "epoch": 1.4302222222222223,
      "grad_norm": 1.5594102144241333,
      "learning_rate": 0.00011870794078061911,
      "loss": 0.1473,
      "step": 403
    },
    {
      "epoch": 1.4337777777777778,
      "grad_norm": 1.2735631465911865,
      "learning_rate": 0.00011843876177658144,
      "loss": 0.2128,
      "step": 404
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 1.0970335006713867,
      "learning_rate": 0.00011816958277254375,
      "loss": 0.1284,
      "step": 405
    },
    {
      "epoch": 1.4408888888888889,
      "grad_norm": 1.2890454530715942,
      "learning_rate": 0.00011790040376850606,
      "loss": 0.1057,
      "step": 406
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.9056270718574524,
      "learning_rate": 0.00011763122476446838,
      "loss": 0.0779,
      "step": 407
    },
    {
      "epoch": 1.448,
      "grad_norm": 1.5866323709487915,
      "learning_rate": 0.00011736204576043069,
      "loss": 0.2397,
      "step": 408
    },
    {
      "epoch": 1.4515555555555555,
      "grad_norm": 0.7520661354064941,
      "learning_rate": 0.00011709286675639301,
      "loss": 0.0737,
      "step": 409
    },
    {
      "epoch": 1.455111111111111,
      "grad_norm": 1.1492524147033691,
      "learning_rate": 0.00011682368775235533,
      "loss": 0.1973,
      "step": 410
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 1.0486136674880981,
      "learning_rate": 0.00011655450874831764,
      "loss": 0.0851,
      "step": 411
    },
    {
      "epoch": 1.462222222222222,
      "grad_norm": 1.432553768157959,
      "learning_rate": 0.00011628532974427995,
      "loss": 0.1175,
      "step": 412
    },
    {
      "epoch": 1.4657777777777778,
      "grad_norm": 2.152362585067749,
      "learning_rate": 0.00011601615074024226,
      "loss": 0.2063,
      "step": 413
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 1.3977584838867188,
      "learning_rate": 0.00011574697173620459,
      "loss": 0.1536,
      "step": 414
    },
    {
      "epoch": 1.472888888888889,
      "grad_norm": 2.736027717590332,
      "learning_rate": 0.0001154777927321669,
      "loss": 0.2868,
      "step": 415
    },
    {
      "epoch": 1.4764444444444444,
      "grad_norm": 1.0270427465438843,
      "learning_rate": 0.00011520861372812922,
      "loss": 0.1214,
      "step": 416
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.2671003341674805,
      "learning_rate": 0.00011493943472409153,
      "loss": 0.1611,
      "step": 417
    },
    {
      "epoch": 1.4835555555555555,
      "grad_norm": 2.512117862701416,
      "learning_rate": 0.00011467025572005383,
      "loss": 0.1459,
      "step": 418
    },
    {
      "epoch": 1.487111111111111,
      "grad_norm": 1.521756649017334,
      "learning_rate": 0.00011440107671601617,
      "loss": 0.0845,
      "step": 419
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.8336865901947021,
      "learning_rate": 0.00011413189771197848,
      "loss": 0.1283,
      "step": 420
    },
    {
      "epoch": 1.4942222222222221,
      "grad_norm": 1.8346341848373413,
      "learning_rate": 0.00011386271870794079,
      "loss": 0.1633,
      "step": 421
    },
    {
      "epoch": 1.4977777777777779,
      "grad_norm": 1.9720593690872192,
      "learning_rate": 0.00011359353970390309,
      "loss": 0.165,
      "step": 422
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 1.344612956047058,
      "learning_rate": 0.0001133243606998654,
      "loss": 0.0633,
      "step": 423
    },
    {
      "epoch": 1.504888888888889,
      "grad_norm": 1.7976975440979004,
      "learning_rate": 0.00011305518169582774,
      "loss": 0.1991,
      "step": 424
    },
    {
      "epoch": 1.5084444444444445,
      "grad_norm": 1.2733609676361084,
      "learning_rate": 0.00011278600269179006,
      "loss": 0.1127,
      "step": 425
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.1763142347335815,
      "learning_rate": 0.00011251682368775235,
      "loss": 0.1427,
      "step": 426
    },
    {
      "epoch": 1.5155555555555555,
      "grad_norm": 0.9423421621322632,
      "learning_rate": 0.00011224764468371467,
      "loss": 0.0949,
      "step": 427
    },
    {
      "epoch": 1.519111111111111,
      "grad_norm": 1.335646390914917,
      "learning_rate": 0.00011197846567967698,
      "loss": 0.2437,
      "step": 428
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 1.1872601509094238,
      "learning_rate": 0.00011170928667563932,
      "loss": 0.0936,
      "step": 429
    },
    {
      "epoch": 1.5262222222222221,
      "grad_norm": 1.1522389650344849,
      "learning_rate": 0.00011144010767160162,
      "loss": 0.224,
      "step": 430
    },
    {
      "epoch": 1.529777777777778,
      "grad_norm": 1.495286226272583,
      "learning_rate": 0.00011117092866756393,
      "loss": 0.2013,
      "step": 431
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 2.4908437728881836,
      "learning_rate": 0.00011090174966352624,
      "loss": 0.2681,
      "step": 432
    },
    {
      "epoch": 1.536888888888889,
      "grad_norm": 2.1335995197296143,
      "learning_rate": 0.00011063257065948856,
      "loss": 0.1558,
      "step": 433
    },
    {
      "epoch": 1.5404444444444443,
      "grad_norm": 1.115425944328308,
      "learning_rate": 0.00011036339165545088,
      "loss": 0.1201,
      "step": 434
    },
    {
      "epoch": 1.544,
      "grad_norm": 2.0194993019104004,
      "learning_rate": 0.0001100942126514132,
      "loss": 0.2211,
      "step": 435
    },
    {
      "epoch": 1.5475555555555556,
      "grad_norm": 1.2186092138290405,
      "learning_rate": 0.00010982503364737551,
      "loss": 0.1378,
      "step": 436
    },
    {
      "epoch": 1.551111111111111,
      "grad_norm": 1.467484712600708,
      "learning_rate": 0.00010955585464333782,
      "loss": 0.1918,
      "step": 437
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 1.3642059564590454,
      "learning_rate": 0.00010928667563930013,
      "loss": 0.187,
      "step": 438
    },
    {
      "epoch": 1.5582222222222222,
      "grad_norm": 1.114992618560791,
      "learning_rate": 0.00010901749663526246,
      "loss": 0.1346,
      "step": 439
    },
    {
      "epoch": 1.561777777777778,
      "grad_norm": 2.3477349281311035,
      "learning_rate": 0.00010874831763122477,
      "loss": 0.2359,
      "step": 440
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 1.3658705949783325,
      "learning_rate": 0.00010847913862718708,
      "loss": 0.2508,
      "step": 441
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 0.8107307553291321,
      "learning_rate": 0.0001082099596231494,
      "loss": 0.1645,
      "step": 442
    },
    {
      "epoch": 1.5724444444444443,
      "grad_norm": 1.0572888851165771,
      "learning_rate": 0.00010794078061911172,
      "loss": 0.1516,
      "step": 443
    },
    {
      "epoch": 1.576,
      "grad_norm": 1.7000030279159546,
      "learning_rate": 0.00010767160161507404,
      "loss": 0.0892,
      "step": 444
    },
    {
      "epoch": 1.5795555555555556,
      "grad_norm": 1.3925490379333496,
      "learning_rate": 0.00010740242261103635,
      "loss": 0.0801,
      "step": 445
    },
    {
      "epoch": 1.5831111111111111,
      "grad_norm": 1.5953987836837769,
      "learning_rate": 0.00010713324360699866,
      "loss": 0.2426,
      "step": 446
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 1.6280221939086914,
      "learning_rate": 0.00010686406460296097,
      "loss": 0.2417,
      "step": 447
    },
    {
      "epoch": 1.5902222222222222,
      "grad_norm": 1.411734938621521,
      "learning_rate": 0.0001065948855989233,
      "loss": 0.239,
      "step": 448
    },
    {
      "epoch": 1.5937777777777777,
      "grad_norm": 3.3871796131134033,
      "learning_rate": 0.00010632570659488561,
      "loss": 0.2686,
      "step": 449
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 1.0268148183822632,
      "learning_rate": 0.00010605652759084792,
      "loss": 0.0579,
      "step": 450
    },
    {
      "epoch": 1.600888888888889,
      "grad_norm": 1.6151080131530762,
      "learning_rate": 0.00010578734858681024,
      "loss": 0.1441,
      "step": 451
    },
    {
      "epoch": 1.6044444444444443,
      "grad_norm": 0.9226762652397156,
      "learning_rate": 0.00010551816958277254,
      "loss": 0.1235,
      "step": 452
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.8754933476448059,
      "learning_rate": 0.00010524899057873488,
      "loss": 0.0985,
      "step": 453
    },
    {
      "epoch": 1.6115555555555554,
      "grad_norm": 1.6619751453399658,
      "learning_rate": 0.00010497981157469719,
      "loss": 0.1533,
      "step": 454
    },
    {
      "epoch": 1.6151111111111112,
      "grad_norm": 1.2760047912597656,
      "learning_rate": 0.0001047106325706595,
      "loss": 0.1627,
      "step": 455
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 1.2620010375976562,
      "learning_rate": 0.0001044414535666218,
      "loss": 0.1538,
      "step": 456
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 2.3732666969299316,
      "learning_rate": 0.00010417227456258411,
      "loss": 0.1458,
      "step": 457
    },
    {
      "epoch": 1.6257777777777778,
      "grad_norm": 1.4666600227355957,
      "learning_rate": 0.00010390309555854645,
      "loss": 0.1511,
      "step": 458
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 2.702733278274536,
      "learning_rate": 0.00010363391655450876,
      "loss": 0.2866,
      "step": 459
    },
    {
      "epoch": 1.6328888888888888,
      "grad_norm": 2.148590087890625,
      "learning_rate": 0.00010336473755047106,
      "loss": 0.2997,
      "step": 460
    },
    {
      "epoch": 1.6364444444444444,
      "grad_norm": 0.5221681594848633,
      "learning_rate": 0.00010309555854643338,
      "loss": 0.0423,
      "step": 461
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.6492117643356323,
      "learning_rate": 0.00010282637954239569,
      "loss": 0.2253,
      "step": 462
    },
    {
      "epoch": 1.6435555555555554,
      "grad_norm": 1.4046692848205566,
      "learning_rate": 0.00010255720053835803,
      "loss": 0.151,
      "step": 463
    },
    {
      "epoch": 1.6471111111111112,
      "grad_norm": 1.6105624437332153,
      "learning_rate": 0.00010228802153432033,
      "loss": 0.1168,
      "step": 464
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 2.041365146636963,
      "learning_rate": 0.00010201884253028264,
      "loss": 0.2385,
      "step": 465
    },
    {
      "epoch": 1.6542222222222223,
      "grad_norm": 1.8676501512527466,
      "learning_rate": 0.00010174966352624495,
      "loss": 0.2294,
      "step": 466
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 1.0306425094604492,
      "learning_rate": 0.00010148048452220727,
      "loss": 0.1308,
      "step": 467
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 1.2227333784103394,
      "learning_rate": 0.00010121130551816959,
      "loss": 0.2938,
      "step": 468
    },
    {
      "epoch": 1.6648888888888889,
      "grad_norm": 2.1581907272338867,
      "learning_rate": 0.0001009421265141319,
      "loss": 0.1996,
      "step": 469
    },
    {
      "epoch": 1.6684444444444444,
      "grad_norm": 2.2457327842712402,
      "learning_rate": 0.00010067294751009422,
      "loss": 0.2035,
      "step": 470
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 1.8724266290664673,
      "learning_rate": 0.00010040376850605653,
      "loss": 0.1394,
      "step": 471
    },
    {
      "epoch": 1.6755555555555555,
      "grad_norm": 1.1751654148101807,
      "learning_rate": 0.00010013458950201884,
      "loss": 0.1307,
      "step": 472
    },
    {
      "epoch": 1.6791111111111112,
      "grad_norm": 1.2832000255584717,
      "learning_rate": 9.986541049798115e-05,
      "loss": 0.1153,
      "step": 473
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 1.8296787738800049,
      "learning_rate": 9.959623149394348e-05,
      "loss": 0.1547,
      "step": 474
    },
    {
      "epoch": 1.6862222222222223,
      "grad_norm": 2.109354019165039,
      "learning_rate": 9.932705248990579e-05,
      "loss": 0.3113,
      "step": 475
    },
    {
      "epoch": 1.6897777777777778,
      "grad_norm": 1.7209941148757935,
      "learning_rate": 9.90578734858681e-05,
      "loss": 0.1609,
      "step": 476
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 1.4978065490722656,
      "learning_rate": 9.878869448183042e-05,
      "loss": 0.1424,
      "step": 477
    },
    {
      "epoch": 1.696888888888889,
      "grad_norm": 0.6257785558700562,
      "learning_rate": 9.851951547779273e-05,
      "loss": 0.0827,
      "step": 478
    },
    {
      "epoch": 1.7004444444444444,
      "grad_norm": 1.503553032875061,
      "learning_rate": 9.825033647375506e-05,
      "loss": 0.1653,
      "step": 479
    },
    {
      "epoch": 1.704,
      "grad_norm": 1.138076901435852,
      "learning_rate": 9.798115746971737e-05,
      "loss": 0.1066,
      "step": 480
    },
    {
      "epoch": 1.7075555555555555,
      "grad_norm": 4.74118185043335,
      "learning_rate": 9.771197846567968e-05,
      "loss": 0.1837,
      "step": 481
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 1.0864936113357544,
      "learning_rate": 9.7442799461642e-05,
      "loss": 0.0815,
      "step": 482
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.9334877133369446,
      "learning_rate": 9.717362045760431e-05,
      "loss": 0.0745,
      "step": 483
    },
    {
      "epoch": 1.7182222222222223,
      "grad_norm": 0.9587839841842651,
      "learning_rate": 9.690444145356663e-05,
      "loss": 0.1094,
      "step": 484
    },
    {
      "epoch": 1.7217777777777776,
      "grad_norm": 1.417466640472412,
      "learning_rate": 9.663526244952895e-05,
      "loss": 0.1841,
      "step": 485
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 1.542955756187439,
      "learning_rate": 9.636608344549124e-05,
      "loss": 0.1395,
      "step": 486
    },
    {
      "epoch": 1.728888888888889,
      "grad_norm": 1.0905280113220215,
      "learning_rate": 9.609690444145357e-05,
      "loss": 0.1623,
      "step": 487
    },
    {
      "epoch": 1.7324444444444445,
      "grad_norm": 1.4304945468902588,
      "learning_rate": 9.582772543741588e-05,
      "loss": 0.1681,
      "step": 488
    },
    {
      "epoch": 1.736,
      "grad_norm": 1.8521716594696045,
      "learning_rate": 9.555854643337821e-05,
      "loss": 0.1793,
      "step": 489
    },
    {
      "epoch": 1.7395555555555555,
      "grad_norm": 1.5729695558547974,
      "learning_rate": 9.528936742934051e-05,
      "loss": 0.1052,
      "step": 490
    },
    {
      "epoch": 1.743111111111111,
      "grad_norm": 2.580075979232788,
      "learning_rate": 9.502018842530283e-05,
      "loss": 0.3423,
      "step": 491
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 2.033419132232666,
      "learning_rate": 9.475100942126515e-05,
      "loss": 0.2046,
      "step": 492
    },
    {
      "epoch": 1.7502222222222223,
      "grad_norm": 3.1218652725219727,
      "learning_rate": 9.448183041722746e-05,
      "loss": 0.2492,
      "step": 493
    },
    {
      "epoch": 1.7537777777777777,
      "grad_norm": 0.9641081094741821,
      "learning_rate": 9.421265141318977e-05,
      "loss": 0.0395,
      "step": 494
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 1.9109647274017334,
      "learning_rate": 9.394347240915209e-05,
      "loss": 0.2447,
      "step": 495
    },
    {
      "epoch": 1.7608888888888887,
      "grad_norm": 1.57454252243042,
      "learning_rate": 9.367429340511441e-05,
      "loss": 0.2261,
      "step": 496
    },
    {
      "epoch": 1.7644444444444445,
      "grad_norm": 0.9481619000434875,
      "learning_rate": 9.340511440107672e-05,
      "loss": 0.0636,
      "step": 497
    },
    {
      "epoch": 1.768,
      "grad_norm": 4.4122819900512695,
      "learning_rate": 9.313593539703904e-05,
      "loss": 0.3003,
      "step": 498
    },
    {
      "epoch": 1.7715555555555556,
      "grad_norm": 3.0347654819488525,
      "learning_rate": 9.286675639300135e-05,
      "loss": 0.1278,
      "step": 499
    },
    {
      "epoch": 1.775111111111111,
      "grad_norm": 3.0616188049316406,
      "learning_rate": 9.259757738896366e-05,
      "loss": 0.327,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 843,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.16507420519895e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
