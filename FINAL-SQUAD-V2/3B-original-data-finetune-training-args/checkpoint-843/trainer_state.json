{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.992,
  "eval_steps": 500,
  "global_step": 843,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035555555555555557,
      "grad_norm": 3.417015552520752,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.7456,
      "step": 1
    },
    {
      "epoch": 0.0071111111111111115,
      "grad_norm": 2.2623541355133057,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.4412,
      "step": 2
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 2.405350685119629,
      "learning_rate": 6e-06,
      "loss": 1.0993,
      "step": 3
    },
    {
      "epoch": 0.014222222222222223,
      "grad_norm": 1.7570509910583496,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.3481,
      "step": 4
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 1.6395419836044312,
      "learning_rate": 1e-05,
      "loss": 1.2886,
      "step": 5
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 1.9863721132278442,
      "learning_rate": 1.2e-05,
      "loss": 1.0441,
      "step": 6
    },
    {
      "epoch": 0.024888888888888887,
      "grad_norm": 1.8783313035964966,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.249,
      "step": 7
    },
    {
      "epoch": 0.028444444444444446,
      "grad_norm": 1.9590867757797241,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.2137,
      "step": 8
    },
    {
      "epoch": 0.032,
      "grad_norm": 2.084606647491455,
      "learning_rate": 1.8e-05,
      "loss": 1.0822,
      "step": 9
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 1.5971049070358276,
      "learning_rate": 2e-05,
      "loss": 0.9709,
      "step": 10
    },
    {
      "epoch": 0.03911111111111111,
      "grad_norm": 1.7119561433792114,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.0564,
      "step": 11
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 2.5340511798858643,
      "learning_rate": 2.4e-05,
      "loss": 1.2541,
      "step": 12
    },
    {
      "epoch": 0.04622222222222222,
      "grad_norm": 3.271831750869751,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.2915,
      "step": 13
    },
    {
      "epoch": 0.049777777777777775,
      "grad_norm": 2.574676036834717,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.035,
      "step": 14
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 2.294796943664551,
      "learning_rate": 3e-05,
      "loss": 1.0513,
      "step": 15
    },
    {
      "epoch": 0.05688888888888889,
      "grad_norm": 2.257603883743286,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.8795,
      "step": 16
    },
    {
      "epoch": 0.060444444444444446,
      "grad_norm": 2.513715982437134,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.9061,
      "step": 17
    },
    {
      "epoch": 0.064,
      "grad_norm": 1.420697569847107,
      "learning_rate": 3.6e-05,
      "loss": 0.6173,
      "step": 18
    },
    {
      "epoch": 0.06755555555555555,
      "grad_norm": 2.624630928039551,
      "learning_rate": 3.8e-05,
      "loss": 1.1697,
      "step": 19
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 3.4680824279785156,
      "learning_rate": 4e-05,
      "loss": 0.8967,
      "step": 20
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 2.6797943115234375,
      "learning_rate": 4.2e-05,
      "loss": 0.695,
      "step": 21
    },
    {
      "epoch": 0.07822222222222222,
      "grad_norm": 2.454733371734619,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.7614,
      "step": 22
    },
    {
      "epoch": 0.08177777777777778,
      "grad_norm": 2.687948226928711,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.8142,
      "step": 23
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 2.669346809387207,
      "learning_rate": 4.8e-05,
      "loss": 0.7365,
      "step": 24
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 2.0461840629577637,
      "learning_rate": 5e-05,
      "loss": 0.5769,
      "step": 25
    },
    {
      "epoch": 0.09244444444444444,
      "grad_norm": 3.0744266510009766,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.6968,
      "step": 26
    },
    {
      "epoch": 0.096,
      "grad_norm": 2.6707687377929688,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 0.6693,
      "step": 27
    },
    {
      "epoch": 0.09955555555555555,
      "grad_norm": 3.8194727897644043,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.4376,
      "step": 28
    },
    {
      "epoch": 0.10311111111111111,
      "grad_norm": 2.7937817573547363,
      "learning_rate": 5.8e-05,
      "loss": 0.7816,
      "step": 29
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 3.1054444313049316,
      "learning_rate": 6e-05,
      "loss": 0.4605,
      "step": 30
    },
    {
      "epoch": 0.11022222222222222,
      "grad_norm": 2.3572492599487305,
      "learning_rate": 6.2e-05,
      "loss": 0.3864,
      "step": 31
    },
    {
      "epoch": 0.11377777777777778,
      "grad_norm": 2.8888144493103027,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.6771,
      "step": 32
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 2.4622697830200195,
      "learning_rate": 6.6e-05,
      "loss": 0.4159,
      "step": 33
    },
    {
      "epoch": 0.12088888888888889,
      "grad_norm": 1.3983370065689087,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.4291,
      "step": 34
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 3.5808324813842773,
      "learning_rate": 7e-05,
      "loss": 0.4108,
      "step": 35
    },
    {
      "epoch": 0.128,
      "grad_norm": 3.5400760173797607,
      "learning_rate": 7.2e-05,
      "loss": 0.4954,
      "step": 36
    },
    {
      "epoch": 0.13155555555555556,
      "grad_norm": 3.8553736209869385,
      "learning_rate": 7.4e-05,
      "loss": 0.5452,
      "step": 37
    },
    {
      "epoch": 0.1351111111111111,
      "grad_norm": 4.149404525756836,
      "learning_rate": 7.6e-05,
      "loss": 0.3787,
      "step": 38
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 2.64218807220459,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.3895,
      "step": 39
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 2.675976276397705,
      "learning_rate": 8e-05,
      "loss": 0.2474,
      "step": 40
    },
    {
      "epoch": 0.14577777777777778,
      "grad_norm": 2.768864870071411,
      "learning_rate": 8.2e-05,
      "loss": 0.385,
      "step": 41
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 1.4825807809829712,
      "learning_rate": 8.4e-05,
      "loss": 0.3394,
      "step": 42
    },
    {
      "epoch": 0.15288888888888888,
      "grad_norm": 3.253200054168701,
      "learning_rate": 8.6e-05,
      "loss": 0.3079,
      "step": 43
    },
    {
      "epoch": 0.15644444444444444,
      "grad_norm": 2.2768654823303223,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.4342,
      "step": 44
    },
    {
      "epoch": 0.16,
      "grad_norm": 3.1827032566070557,
      "learning_rate": 9e-05,
      "loss": 0.4667,
      "step": 45
    },
    {
      "epoch": 0.16355555555555557,
      "grad_norm": 1.919540286064148,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.2874,
      "step": 46
    },
    {
      "epoch": 0.1671111111111111,
      "grad_norm": 3.010023832321167,
      "learning_rate": 9.4e-05,
      "loss": 0.3881,
      "step": 47
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 2.585278272628784,
      "learning_rate": 9.6e-05,
      "loss": 0.3444,
      "step": 48
    },
    {
      "epoch": 0.17422222222222222,
      "grad_norm": 2.067357301712036,
      "learning_rate": 9.8e-05,
      "loss": 0.3481,
      "step": 49
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 2.086641550064087,
      "learning_rate": 0.0001,
      "loss": 0.3352,
      "step": 50
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 1.858405351638794,
      "learning_rate": 0.00010200000000000001,
      "loss": 0.2444,
      "step": 51
    },
    {
      "epoch": 0.18488888888888888,
      "grad_norm": 2.0978174209594727,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.2815,
      "step": 52
    },
    {
      "epoch": 0.18844444444444444,
      "grad_norm": 2.331804037094116,
      "learning_rate": 0.00010600000000000002,
      "loss": 0.2591,
      "step": 53
    },
    {
      "epoch": 0.192,
      "grad_norm": 2.1340763568878174,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.2995,
      "step": 54
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 1.2830572128295898,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.1995,
      "step": 55
    },
    {
      "epoch": 0.1991111111111111,
      "grad_norm": 2.966412305831909,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.4168,
      "step": 56
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 2.0543718338012695,
      "learning_rate": 0.00011399999999999999,
      "loss": 0.3519,
      "step": 57
    },
    {
      "epoch": 0.20622222222222222,
      "grad_norm": 2.7467586994171143,
      "learning_rate": 0.000116,
      "loss": 0.3311,
      "step": 58
    },
    {
      "epoch": 0.20977777777777779,
      "grad_norm": 2.949787139892578,
      "learning_rate": 0.000118,
      "loss": 0.5769,
      "step": 59
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 2.724496603012085,
      "learning_rate": 0.00012,
      "loss": 0.3724,
      "step": 60
    },
    {
      "epoch": 0.21688888888888888,
      "grad_norm": 1.9476864337921143,
      "learning_rate": 0.000122,
      "loss": 0.2985,
      "step": 61
    },
    {
      "epoch": 0.22044444444444444,
      "grad_norm": 1.966278076171875,
      "learning_rate": 0.000124,
      "loss": 0.3657,
      "step": 62
    },
    {
      "epoch": 0.224,
      "grad_norm": 3.014005661010742,
      "learning_rate": 0.000126,
      "loss": 0.3509,
      "step": 63
    },
    {
      "epoch": 0.22755555555555557,
      "grad_norm": 2.775313138961792,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.3619,
      "step": 64
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 1.750035047531128,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.1748,
      "step": 65
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 1.2672014236450195,
      "learning_rate": 0.000132,
      "loss": 0.1959,
      "step": 66
    },
    {
      "epoch": 0.23822222222222222,
      "grad_norm": 1.5879456996917725,
      "learning_rate": 0.000134,
      "loss": 0.2962,
      "step": 67
    },
    {
      "epoch": 0.24177777777777779,
      "grad_norm": 2.333885669708252,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.3883,
      "step": 68
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 2.0753962993621826,
      "learning_rate": 0.000138,
      "loss": 0.3446,
      "step": 69
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 2.7087621688842773,
      "learning_rate": 0.00014,
      "loss": 0.4443,
      "step": 70
    },
    {
      "epoch": 0.25244444444444447,
      "grad_norm": 2.7104108333587646,
      "learning_rate": 0.000142,
      "loss": 0.3386,
      "step": 71
    },
    {
      "epoch": 0.256,
      "grad_norm": 1.6935445070266724,
      "learning_rate": 0.000144,
      "loss": 0.2315,
      "step": 72
    },
    {
      "epoch": 0.25955555555555554,
      "grad_norm": 4.171173572540283,
      "learning_rate": 0.000146,
      "loss": 0.346,
      "step": 73
    },
    {
      "epoch": 0.26311111111111113,
      "grad_norm": 2.462888240814209,
      "learning_rate": 0.000148,
      "loss": 0.4549,
      "step": 74
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.148110866546631,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.3579,
      "step": 75
    },
    {
      "epoch": 0.2702222222222222,
      "grad_norm": 3.4548888206481934,
      "learning_rate": 0.000152,
      "loss": 0.651,
      "step": 76
    },
    {
      "epoch": 0.2737777777777778,
      "grad_norm": 1.7865114212036133,
      "learning_rate": 0.000154,
      "loss": 0.2341,
      "step": 77
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 2.6402270793914795,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.2925,
      "step": 78
    },
    {
      "epoch": 0.2808888888888889,
      "grad_norm": 1.8999165296554565,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.2642,
      "step": 79
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 1.4973293542861938,
      "learning_rate": 0.00016,
      "loss": 0.2485,
      "step": 80
    },
    {
      "epoch": 0.288,
      "grad_norm": 1.8843357563018799,
      "learning_rate": 0.000162,
      "loss": 0.242,
      "step": 81
    },
    {
      "epoch": 0.29155555555555557,
      "grad_norm": 1.9240669012069702,
      "learning_rate": 0.000164,
      "loss": 0.2369,
      "step": 82
    },
    {
      "epoch": 0.2951111111111111,
      "grad_norm": 1.6475318670272827,
      "learning_rate": 0.000166,
      "loss": 0.2531,
      "step": 83
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 1.9739848375320435,
      "learning_rate": 0.000168,
      "loss": 0.2853,
      "step": 84
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 3.3147940635681152,
      "learning_rate": 0.00017,
      "loss": 0.3842,
      "step": 85
    },
    {
      "epoch": 0.30577777777777776,
      "grad_norm": 2.817629337310791,
      "learning_rate": 0.000172,
      "loss": 0.2843,
      "step": 86
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 1.3192296028137207,
      "learning_rate": 0.000174,
      "loss": 0.2014,
      "step": 87
    },
    {
      "epoch": 0.3128888888888889,
      "grad_norm": 2.1299753189086914,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.2885,
      "step": 88
    },
    {
      "epoch": 0.3164444444444444,
      "grad_norm": 2.9361979961395264,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.35,
      "step": 89
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.9590184688568115,
      "learning_rate": 0.00018,
      "loss": 0.3733,
      "step": 90
    },
    {
      "epoch": 0.32355555555555554,
      "grad_norm": 4.02506160736084,
      "learning_rate": 0.000182,
      "loss": 0.3398,
      "step": 91
    },
    {
      "epoch": 0.32711111111111113,
      "grad_norm": 3.105518102645874,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.3365,
      "step": 92
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 2.257131814956665,
      "learning_rate": 0.00018600000000000002,
      "loss": 0.289,
      "step": 93
    },
    {
      "epoch": 0.3342222222222222,
      "grad_norm": 2.4724838733673096,
      "learning_rate": 0.000188,
      "loss": 0.3551,
      "step": 94
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": NaN,
      "learning_rate": 0.000188,
      "loss": 0.2554,
      "step": 95
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 3.408928871154785,
      "learning_rate": 0.00019,
      "loss": 0.3003,
      "step": 96
    },
    {
      "epoch": 0.3448888888888889,
      "grad_norm": 2.685626268386841,
      "learning_rate": 0.000192,
      "loss": 0.2019,
      "step": 97
    },
    {
      "epoch": 0.34844444444444445,
      "grad_norm": 1.7403990030288696,
      "learning_rate": 0.000194,
      "loss": 0.2584,
      "step": 98
    },
    {
      "epoch": 0.352,
      "grad_norm": 1.917219877243042,
      "learning_rate": 0.000196,
      "loss": 0.3454,
      "step": 99
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 2.2796175479888916,
      "learning_rate": 0.00019800000000000002,
      "loss": 0.3692,
      "step": 100
    },
    {
      "epoch": 0.3591111111111111,
      "grad_norm": 3.7576513290405273,
      "learning_rate": 0.0002,
      "loss": 0.3233,
      "step": 101
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 2.7825419902801514,
      "learning_rate": 0.0001997308209959623,
      "loss": 0.324,
      "step": 102
    },
    {
      "epoch": 0.3662222222222222,
      "grad_norm": 1.9354801177978516,
      "learning_rate": 0.00019946164199192463,
      "loss": 0.2521,
      "step": 103
    },
    {
      "epoch": 0.36977777777777776,
      "grad_norm": 4.425552845001221,
      "learning_rate": 0.00019919246298788696,
      "loss": 0.3493,
      "step": 104
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.1518288850784302,
      "learning_rate": 0.0001989232839838493,
      "loss": 0.1605,
      "step": 105
    },
    {
      "epoch": 0.3768888888888889,
      "grad_norm": 1.723423957824707,
      "learning_rate": 0.00019865410497981159,
      "loss": 0.2076,
      "step": 106
    },
    {
      "epoch": 0.3804444444444444,
      "grad_norm": 2.26922607421875,
      "learning_rate": 0.00019838492597577389,
      "loss": 0.2288,
      "step": 107
    },
    {
      "epoch": 0.384,
      "grad_norm": 2.1743366718292236,
      "learning_rate": 0.0001981157469717362,
      "loss": 0.2549,
      "step": 108
    },
    {
      "epoch": 0.38755555555555554,
      "grad_norm": 2.402208089828491,
      "learning_rate": 0.00019784656796769854,
      "loss": 0.3581,
      "step": 109
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 2.5873332023620605,
      "learning_rate": 0.00019757738896366084,
      "loss": 0.4092,
      "step": 110
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 2.9529976844787598,
      "learning_rate": 0.00019730820995962316,
      "loss": 0.3054,
      "step": 111
    },
    {
      "epoch": 0.3982222222222222,
      "grad_norm": 1.8943376541137695,
      "learning_rate": 0.00019703903095558546,
      "loss": 0.2481,
      "step": 112
    },
    {
      "epoch": 0.4017777777777778,
      "grad_norm": 3.0948398113250732,
      "learning_rate": 0.0001967698519515478,
      "loss": 0.2246,
      "step": 113
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 2.4627091884613037,
      "learning_rate": 0.00019650067294751011,
      "loss": 0.2093,
      "step": 114
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 2.5407731533050537,
      "learning_rate": 0.0001962314939434724,
      "loss": 0.2969,
      "step": 115
    },
    {
      "epoch": 0.41244444444444445,
      "grad_norm": 2.0906620025634766,
      "learning_rate": 0.00019596231493943474,
      "loss": 0.1868,
      "step": 116
    },
    {
      "epoch": 0.416,
      "grad_norm": 2.966822624206543,
      "learning_rate": 0.00019569313593539704,
      "loss": 0.4307,
      "step": 117
    },
    {
      "epoch": 0.41955555555555557,
      "grad_norm": 1.9838496446609497,
      "learning_rate": 0.00019542395693135936,
      "loss": 0.1731,
      "step": 118
    },
    {
      "epoch": 0.4231111111111111,
      "grad_norm": 1.5668261051177979,
      "learning_rate": 0.0001951547779273217,
      "loss": 0.194,
      "step": 119
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 2.1374614238739014,
      "learning_rate": 0.000194885598923284,
      "loss": 0.3963,
      "step": 120
    },
    {
      "epoch": 0.43022222222222223,
      "grad_norm": 2.929614305496216,
      "learning_rate": 0.00019461641991924632,
      "loss": 0.232,
      "step": 121
    },
    {
      "epoch": 0.43377777777777776,
      "grad_norm": 2.1057510375976562,
      "learning_rate": 0.00019434724091520861,
      "loss": 0.1338,
      "step": 122
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 2.0190625190734863,
      "learning_rate": 0.00019407806191117094,
      "loss": 0.2558,
      "step": 123
    },
    {
      "epoch": 0.4408888888888889,
      "grad_norm": 2.6725053787231445,
      "learning_rate": 0.00019380888290713327,
      "loss": 0.3304,
      "step": 124
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.899870753288269,
      "learning_rate": 0.00019353970390309557,
      "loss": 0.1209,
      "step": 125
    },
    {
      "epoch": 0.448,
      "grad_norm": 1.9068149328231812,
      "learning_rate": 0.0001932705248990579,
      "loss": 0.2642,
      "step": 126
    },
    {
      "epoch": 0.45155555555555554,
      "grad_norm": 2.0267372131347656,
      "learning_rate": 0.0001930013458950202,
      "loss": 0.2833,
      "step": 127
    },
    {
      "epoch": 0.45511111111111113,
      "grad_norm": 2.4475369453430176,
      "learning_rate": 0.0001927321668909825,
      "loss": 0.3487,
      "step": 128
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 2.543243408203125,
      "learning_rate": 0.00019246298788694484,
      "loss": 0.2796,
      "step": 129
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 2.0898385047912598,
      "learning_rate": 0.00019219380888290714,
      "loss": 0.2959,
      "step": 130
    },
    {
      "epoch": 0.4657777777777778,
      "grad_norm": 2.2002415657043457,
      "learning_rate": 0.00019192462987886947,
      "loss": 0.2248,
      "step": 131
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 2.2708232402801514,
      "learning_rate": 0.00019165545087483177,
      "loss": 0.0763,
      "step": 132
    },
    {
      "epoch": 0.4728888888888889,
      "grad_norm": 2.3125011920928955,
      "learning_rate": 0.0001913862718707941,
      "loss": 0.3869,
      "step": 133
    },
    {
      "epoch": 0.47644444444444445,
      "grad_norm": 2.071300506591797,
      "learning_rate": 0.00019111709286675642,
      "loss": 0.1788,
      "step": 134
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.0568366050720215,
      "learning_rate": 0.00019084791386271872,
      "loss": 0.2375,
      "step": 135
    },
    {
      "epoch": 0.48355555555555557,
      "grad_norm": 3.717430830001831,
      "learning_rate": 0.00019057873485868102,
      "loss": 0.3842,
      "step": 136
    },
    {
      "epoch": 0.4871111111111111,
      "grad_norm": 4.826358318328857,
      "learning_rate": 0.00019030955585464334,
      "loss": 0.2193,
      "step": 137
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 2.633531332015991,
      "learning_rate": 0.00019004037685060567,
      "loss": 0.2146,
      "step": 138
    },
    {
      "epoch": 0.49422222222222223,
      "grad_norm": 2.0889182090759277,
      "learning_rate": 0.000189771197846568,
      "loss": 0.2804,
      "step": 139
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 2.4884886741638184,
      "learning_rate": 0.0001895020188425303,
      "loss": 0.3409,
      "step": 140
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 1.782486081123352,
      "learning_rate": 0.0001892328398384926,
      "loss": 0.2001,
      "step": 141
    },
    {
      "epoch": 0.5048888888888889,
      "grad_norm": 2.5042226314544678,
      "learning_rate": 0.00018896366083445492,
      "loss": 0.2012,
      "step": 142
    },
    {
      "epoch": 0.5084444444444445,
      "grad_norm": 2.069610595703125,
      "learning_rate": 0.00018869448183041725,
      "loss": 0.3522,
      "step": 143
    },
    {
      "epoch": 0.512,
      "grad_norm": 1.2324728965759277,
      "learning_rate": 0.00018842530282637955,
      "loss": 0.1014,
      "step": 144
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 1.5794532299041748,
      "learning_rate": 0.00018815612382234187,
      "loss": 0.3061,
      "step": 145
    },
    {
      "epoch": 0.5191111111111111,
      "grad_norm": 3.908510684967041,
      "learning_rate": 0.00018788694481830417,
      "loss": 0.3286,
      "step": 146
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 2.5637147426605225,
      "learning_rate": 0.0001876177658142665,
      "loss": 0.2174,
      "step": 147
    },
    {
      "epoch": 0.5262222222222223,
      "grad_norm": 2.3713529109954834,
      "learning_rate": 0.00018734858681022882,
      "loss": 0.2852,
      "step": 148
    },
    {
      "epoch": 0.5297777777777778,
      "grad_norm": 1.4986106157302856,
      "learning_rate": 0.00018707940780619112,
      "loss": 0.2175,
      "step": 149
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.3081369400024414,
      "learning_rate": 0.00018681022880215345,
      "loss": 0.3525,
      "step": 150
    },
    {
      "epoch": 0.5368888888888889,
      "grad_norm": 1.1330920457839966,
      "learning_rate": 0.00018654104979811575,
      "loss": 0.2305,
      "step": 151
    },
    {
      "epoch": 0.5404444444444444,
      "grad_norm": 1.8200173377990723,
      "learning_rate": 0.00018627187079407807,
      "loss": 0.1414,
      "step": 152
    },
    {
      "epoch": 0.544,
      "grad_norm": 2.4780478477478027,
      "learning_rate": 0.0001860026917900404,
      "loss": 0.3989,
      "step": 153
    },
    {
      "epoch": 0.5475555555555556,
      "grad_norm": 1.3920825719833374,
      "learning_rate": 0.0001857335127860027,
      "loss": 0.2132,
      "step": 154
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 1.3217829465866089,
      "learning_rate": 0.00018546433378196502,
      "loss": 0.2708,
      "step": 155
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 1.7324903011322021,
      "learning_rate": 0.00018519515477792732,
      "loss": 0.1974,
      "step": 156
    },
    {
      "epoch": 0.5582222222222222,
      "grad_norm": 1.5910848379135132,
      "learning_rate": 0.00018492597577388965,
      "loss": 0.2796,
      "step": 157
    },
    {
      "epoch": 0.5617777777777778,
      "grad_norm": 1.476298451423645,
      "learning_rate": 0.00018465679676985198,
      "loss": 0.1841,
      "step": 158
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.8249367475509644,
      "learning_rate": 0.00018438761776581427,
      "loss": 0.0779,
      "step": 159
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 1.2336952686309814,
      "learning_rate": 0.0001841184387617766,
      "loss": 0.2491,
      "step": 160
    },
    {
      "epoch": 0.5724444444444444,
      "grad_norm": 1.4487377405166626,
      "learning_rate": 0.0001838492597577389,
      "loss": 0.2115,
      "step": 161
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.867075800895691,
      "learning_rate": 0.0001835800807537012,
      "loss": 0.2581,
      "step": 162
    },
    {
      "epoch": 0.5795555555555556,
      "grad_norm": 2.0589351654052734,
      "learning_rate": 0.00018331090174966355,
      "loss": 0.3636,
      "step": 163
    },
    {
      "epoch": 0.5831111111111111,
      "grad_norm": 1.223888635635376,
      "learning_rate": 0.00018304172274562585,
      "loss": 0.1632,
      "step": 164
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.8868670463562012,
      "learning_rate": 0.00018277254374158818,
      "loss": 0.1154,
      "step": 165
    },
    {
      "epoch": 0.5902222222222222,
      "grad_norm": 1.0037362575531006,
      "learning_rate": 0.00018250336473755048,
      "loss": 0.0679,
      "step": 166
    },
    {
      "epoch": 0.5937777777777777,
      "grad_norm": 2.382289409637451,
      "learning_rate": 0.00018223418573351277,
      "loss": 0.3042,
      "step": 167
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 4.162321090698242,
      "learning_rate": 0.00018196500672947513,
      "loss": 0.2308,
      "step": 168
    },
    {
      "epoch": 0.6008888888888889,
      "grad_norm": 3.119981527328491,
      "learning_rate": 0.00018169582772543743,
      "loss": 0.1923,
      "step": 169
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 3.682765483856201,
      "learning_rate": 0.00018142664872139973,
      "loss": 0.3643,
      "step": 170
    },
    {
      "epoch": 0.608,
      "grad_norm": 2.835350513458252,
      "learning_rate": 0.00018115746971736205,
      "loss": 0.2394,
      "step": 171
    },
    {
      "epoch": 0.6115555555555555,
      "grad_norm": 2.471010446548462,
      "learning_rate": 0.00018088829071332435,
      "loss": 0.2621,
      "step": 172
    },
    {
      "epoch": 0.6151111111111112,
      "grad_norm": 1.4412662982940674,
      "learning_rate": 0.0001806191117092867,
      "loss": 0.2666,
      "step": 173
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 1.7593368291854858,
      "learning_rate": 0.000180349932705249,
      "loss": 0.3347,
      "step": 174
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 2.4260776042938232,
      "learning_rate": 0.0001800807537012113,
      "loss": 0.1722,
      "step": 175
    },
    {
      "epoch": 0.6257777777777778,
      "grad_norm": 4.9795308113098145,
      "learning_rate": 0.00017981157469717363,
      "loss": 0.3212,
      "step": 176
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 1.2852580547332764,
      "learning_rate": 0.00017954239569313593,
      "loss": 0.1718,
      "step": 177
    },
    {
      "epoch": 0.6328888888888888,
      "grad_norm": 1.905150055885315,
      "learning_rate": 0.00017927321668909825,
      "loss": 0.2978,
      "step": 178
    },
    {
      "epoch": 0.6364444444444445,
      "grad_norm": 2.2048795223236084,
      "learning_rate": 0.00017900403768506058,
      "loss": 0.5193,
      "step": 179
    },
    {
      "epoch": 0.64,
      "grad_norm": 2.9970881938934326,
      "learning_rate": 0.00017873485868102288,
      "loss": 0.1977,
      "step": 180
    },
    {
      "epoch": 0.6435555555555555,
      "grad_norm": 2.091374635696411,
      "learning_rate": 0.0001784656796769852,
      "loss": 0.5853,
      "step": 181
    },
    {
      "epoch": 0.6471111111111111,
      "grad_norm": 1.4068173170089722,
      "learning_rate": 0.0001781965006729475,
      "loss": 0.1666,
      "step": 182
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 1.2437806129455566,
      "learning_rate": 0.00017792732166890983,
      "loss": 0.1435,
      "step": 183
    },
    {
      "epoch": 0.6542222222222223,
      "grad_norm": 2.0126190185546875,
      "learning_rate": 0.00017765814266487216,
      "loss": 0.2322,
      "step": 184
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 1.98829185962677,
      "learning_rate": 0.00017738896366083446,
      "loss": 0.2324,
      "step": 185
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 2.6868340969085693,
      "learning_rate": 0.00017711978465679678,
      "loss": 0.2478,
      "step": 186
    },
    {
      "epoch": 0.6648888888888889,
      "grad_norm": 1.6848866939544678,
      "learning_rate": 0.00017685060565275908,
      "loss": 0.2682,
      "step": 187
    },
    {
      "epoch": 0.6684444444444444,
      "grad_norm": 2.379966974258423,
      "learning_rate": 0.0001765814266487214,
      "loss": 0.2035,
      "step": 188
    },
    {
      "epoch": 0.672,
      "grad_norm": 1.8276042938232422,
      "learning_rate": 0.00017631224764468373,
      "loss": 0.3099,
      "step": 189
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 1.8102283477783203,
      "learning_rate": 0.00017604306864064603,
      "loss": 0.2193,
      "step": 190
    },
    {
      "epoch": 0.6791111111111111,
      "grad_norm": 1.907231092453003,
      "learning_rate": 0.00017577388963660836,
      "loss": 0.263,
      "step": 191
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 1.8648825883865356,
      "learning_rate": 0.00017550471063257066,
      "loss": 0.1936,
      "step": 192
    },
    {
      "epoch": 0.6862222222222222,
      "grad_norm": 1.2324389219284058,
      "learning_rate": 0.00017523553162853298,
      "loss": 0.1676,
      "step": 193
    },
    {
      "epoch": 0.6897777777777778,
      "grad_norm": 1.618326187133789,
      "learning_rate": 0.0001749663526244953,
      "loss": 0.236,
      "step": 194
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 1.5929958820343018,
      "learning_rate": 0.0001746971736204576,
      "loss": 0.293,
      "step": 195
    },
    {
      "epoch": 0.6968888888888889,
      "grad_norm": 2.528024196624756,
      "learning_rate": 0.0001744279946164199,
      "loss": 0.3251,
      "step": 196
    },
    {
      "epoch": 0.7004444444444444,
      "grad_norm": 1.9959334135055542,
      "learning_rate": 0.00017415881561238226,
      "loss": 0.2855,
      "step": 197
    },
    {
      "epoch": 0.704,
      "grad_norm": 1.680546760559082,
      "learning_rate": 0.00017388963660834456,
      "loss": 0.1529,
      "step": 198
    },
    {
      "epoch": 0.7075555555555556,
      "grad_norm": 2.4446918964385986,
      "learning_rate": 0.00017362045760430689,
      "loss": 0.2621,
      "step": 199
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.0808333158493042,
      "learning_rate": 0.00017335127860026918,
      "loss": 0.1761,
      "step": 200
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 1.3534152507781982,
      "learning_rate": 0.00017308209959623148,
      "loss": 0.1856,
      "step": 201
    },
    {
      "epoch": 0.7182222222222222,
      "grad_norm": 1.4481234550476074,
      "learning_rate": 0.00017281292059219384,
      "loss": 0.0823,
      "step": 202
    },
    {
      "epoch": 0.7217777777777777,
      "grad_norm": 1.3327528238296509,
      "learning_rate": 0.00017254374158815614,
      "loss": 0.2145,
      "step": 203
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 1.8004844188690186,
      "learning_rate": 0.00017227456258411844,
      "loss": 0.2157,
      "step": 204
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 1.2512668371200562,
      "learning_rate": 0.00017200538358008076,
      "loss": 0.2153,
      "step": 205
    },
    {
      "epoch": 0.7324444444444445,
      "grad_norm": 1.6947021484375,
      "learning_rate": 0.00017173620457604306,
      "loss": 0.1922,
      "step": 206
    },
    {
      "epoch": 0.736,
      "grad_norm": 1.1330373287200928,
      "learning_rate": 0.0001714670255720054,
      "loss": 0.1398,
      "step": 207
    },
    {
      "epoch": 0.7395555555555555,
      "grad_norm": 1.4576880931854248,
      "learning_rate": 0.0001711978465679677,
      "loss": 0.2029,
      "step": 208
    },
    {
      "epoch": 0.7431111111111111,
      "grad_norm": 2.296070098876953,
      "learning_rate": 0.00017092866756393,
      "loss": 0.2607,
      "step": 209
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 1.3249839544296265,
      "learning_rate": 0.00017065948855989234,
      "loss": 0.1416,
      "step": 210
    },
    {
      "epoch": 0.7502222222222222,
      "grad_norm": 1.1995031833648682,
      "learning_rate": 0.00017039030955585464,
      "loss": 0.183,
      "step": 211
    },
    {
      "epoch": 0.7537777777777778,
      "grad_norm": 1.486039161682129,
      "learning_rate": 0.00017012113055181696,
      "loss": 0.2243,
      "step": 212
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 1.8418980836868286,
      "learning_rate": 0.0001698519515477793,
      "loss": 0.2853,
      "step": 213
    },
    {
      "epoch": 0.7608888888888888,
      "grad_norm": 2.1599745750427246,
      "learning_rate": 0.0001695827725437416,
      "loss": 0.2419,
      "step": 214
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 1.7430497407913208,
      "learning_rate": 0.00016931359353970391,
      "loss": 0.1959,
      "step": 215
    },
    {
      "epoch": 0.768,
      "grad_norm": 1.4764467477798462,
      "learning_rate": 0.0001690444145356662,
      "loss": 0.159,
      "step": 216
    },
    {
      "epoch": 0.7715555555555556,
      "grad_norm": 2.072406530380249,
      "learning_rate": 0.00016877523553162854,
      "loss": 0.1852,
      "step": 217
    },
    {
      "epoch": 0.7751111111111111,
      "grad_norm": 2.86185359954834,
      "learning_rate": 0.00016850605652759087,
      "loss": 0.4518,
      "step": 218
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.9642516374588013,
      "learning_rate": 0.00016823687752355316,
      "loss": 0.1079,
      "step": 219
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 2.3428187370300293,
      "learning_rate": 0.0001679676985195155,
      "loss": 0.3006,
      "step": 220
    },
    {
      "epoch": 0.7857777777777778,
      "grad_norm": 1.9015491008758545,
      "learning_rate": 0.0001676985195154778,
      "loss": 0.2311,
      "step": 221
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 1.191836953163147,
      "learning_rate": 0.00016742934051144012,
      "loss": 0.2152,
      "step": 222
    },
    {
      "epoch": 0.7928888888888889,
      "grad_norm": 2.4765751361846924,
      "learning_rate": 0.00016716016150740244,
      "loss": 0.2068,
      "step": 223
    },
    {
      "epoch": 0.7964444444444444,
      "grad_norm": 1.5483460426330566,
      "learning_rate": 0.00016689098250336474,
      "loss": 0.2081,
      "step": 224
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.9021528363227844,
      "learning_rate": 0.00016662180349932707,
      "loss": 0.1493,
      "step": 225
    },
    {
      "epoch": 0.8035555555555556,
      "grad_norm": 1.4927772283554077,
      "learning_rate": 0.00016635262449528937,
      "loss": 0.2473,
      "step": 226
    },
    {
      "epoch": 0.8071111111111111,
      "grad_norm": 1.1591830253601074,
      "learning_rate": 0.0001660834454912517,
      "loss": 0.2376,
      "step": 227
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 1.164390206336975,
      "learning_rate": 0.00016581426648721402,
      "loss": 0.199,
      "step": 228
    },
    {
      "epoch": 0.8142222222222222,
      "grad_norm": 1.0501039028167725,
      "learning_rate": 0.00016554508748317632,
      "loss": 0.1698,
      "step": 229
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 1.0654584169387817,
      "learning_rate": 0.00016527590847913864,
      "loss": 0.1303,
      "step": 230
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 2.2311227321624756,
      "learning_rate": 0.00016500672947510094,
      "loss": 0.3039,
      "step": 231
    },
    {
      "epoch": 0.8248888888888889,
      "grad_norm": 2.269216299057007,
      "learning_rate": 0.00016473755047106327,
      "loss": 0.378,
      "step": 232
    },
    {
      "epoch": 0.8284444444444444,
      "grad_norm": 1.8481101989746094,
      "learning_rate": 0.0001644683714670256,
      "loss": 0.2731,
      "step": 233
    },
    {
      "epoch": 0.832,
      "grad_norm": 2.20630145072937,
      "learning_rate": 0.0001641991924629879,
      "loss": 0.2966,
      "step": 234
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 2.0484907627105713,
      "learning_rate": 0.0001639300134589502,
      "loss": 0.3392,
      "step": 235
    },
    {
      "epoch": 0.8391111111111111,
      "grad_norm": 4.147709846496582,
      "learning_rate": 0.00016366083445491252,
      "loss": 0.3866,
      "step": 236
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 2.426947593688965,
      "learning_rate": 0.00016339165545087484,
      "loss": 0.2747,
      "step": 237
    },
    {
      "epoch": 0.8462222222222222,
      "grad_norm": 1.9145969152450562,
      "learning_rate": 0.00016312247644683717,
      "loss": 0.3169,
      "step": 238
    },
    {
      "epoch": 0.8497777777777777,
      "grad_norm": 0.9128560423851013,
      "learning_rate": 0.00016285329744279947,
      "loss": 0.1246,
      "step": 239
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.9424158334732056,
      "learning_rate": 0.00016258411843876177,
      "loss": 0.1258,
      "step": 240
    },
    {
      "epoch": 0.8568888888888889,
      "grad_norm": 1.200623869895935,
      "learning_rate": 0.0001623149394347241,
      "loss": 0.139,
      "step": 241
    },
    {
      "epoch": 0.8604444444444445,
      "grad_norm": 1.628354549407959,
      "learning_rate": 0.00016204576043068642,
      "loss": 0.2455,
      "step": 242
    },
    {
      "epoch": 0.864,
      "grad_norm": 1.2771495580673218,
      "learning_rate": 0.00016177658142664872,
      "loss": 0.261,
      "step": 243
    },
    {
      "epoch": 0.8675555555555555,
      "grad_norm": 2.3363494873046875,
      "learning_rate": 0.00016150740242261105,
      "loss": 0.2937,
      "step": 244
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 1.1929019689559937,
      "learning_rate": 0.00016123822341857335,
      "loss": 0.2873,
      "step": 245
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 1.126509189605713,
      "learning_rate": 0.00016096904441453567,
      "loss": 0.2059,
      "step": 246
    },
    {
      "epoch": 0.8782222222222222,
      "grad_norm": 0.8706187009811401,
      "learning_rate": 0.000160699865410498,
      "loss": 0.1908,
      "step": 247
    },
    {
      "epoch": 0.8817777777777778,
      "grad_norm": 2.1332030296325684,
      "learning_rate": 0.0001604306864064603,
      "loss": 0.2325,
      "step": 248
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 1.9623475074768066,
      "learning_rate": 0.00016016150740242262,
      "loss": 0.2742,
      "step": 249
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.5290945768356323,
      "learning_rate": 0.00015989232839838492,
      "loss": 0.1715,
      "step": 250
    },
    {
      "epoch": 0.8924444444444445,
      "grad_norm": 0.8689821362495422,
      "learning_rate": 0.00015962314939434725,
      "loss": 0.1007,
      "step": 251
    },
    {
      "epoch": 0.896,
      "grad_norm": 1.910539984703064,
      "learning_rate": 0.00015935397039030957,
      "loss": 0.2753,
      "step": 252
    },
    {
      "epoch": 0.8995555555555556,
      "grad_norm": 1.6747769117355347,
      "learning_rate": 0.00015908479138627187,
      "loss": 0.3602,
      "step": 253
    },
    {
      "epoch": 0.9031111111111111,
      "grad_norm": 2.0330708026885986,
      "learning_rate": 0.0001588156123822342,
      "loss": 0.2916,
      "step": 254
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 1.6225824356079102,
      "learning_rate": 0.0001585464333781965,
      "loss": 0.2618,
      "step": 255
    },
    {
      "epoch": 0.9102222222222223,
      "grad_norm": 0.9529979825019836,
      "learning_rate": 0.00015827725437415882,
      "loss": 0.1258,
      "step": 256
    },
    {
      "epoch": 0.9137777777777778,
      "grad_norm": 1.34640634059906,
      "learning_rate": 0.00015800807537012115,
      "loss": 0.231,
      "step": 257
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 1.2468105554580688,
      "learning_rate": 0.00015773889636608345,
      "loss": 0.1594,
      "step": 258
    },
    {
      "epoch": 0.9208888888888889,
      "grad_norm": 1.0532536506652832,
      "learning_rate": 0.00015746971736204578,
      "loss": 0.0967,
      "step": 259
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 2.949993371963501,
      "learning_rate": 0.00015720053835800807,
      "loss": 0.4647,
      "step": 260
    },
    {
      "epoch": 0.928,
      "grad_norm": 1.7001549005508423,
      "learning_rate": 0.0001569313593539704,
      "loss": 0.2612,
      "step": 261
    },
    {
      "epoch": 0.9315555555555556,
      "grad_norm": 1.3044205904006958,
      "learning_rate": 0.00015666218034993273,
      "loss": 0.181,
      "step": 262
    },
    {
      "epoch": 0.9351111111111111,
      "grad_norm": 1.9322819709777832,
      "learning_rate": 0.00015639300134589503,
      "loss": 0.3161,
      "step": 263
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 1.4254404306411743,
      "learning_rate": 0.00015612382234185735,
      "loss": 0.1846,
      "step": 264
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 1.1254563331604004,
      "learning_rate": 0.00015585464333781965,
      "loss": 0.1886,
      "step": 265
    },
    {
      "epoch": 0.9457777777777778,
      "grad_norm": 1.1051491498947144,
      "learning_rate": 0.00015558546433378198,
      "loss": 0.1768,
      "step": 266
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 1.5113861560821533,
      "learning_rate": 0.0001553162853297443,
      "loss": 0.2549,
      "step": 267
    },
    {
      "epoch": 0.9528888888888889,
      "grad_norm": 1.8936904668807983,
      "learning_rate": 0.0001550471063257066,
      "loss": 0.3379,
      "step": 268
    },
    {
      "epoch": 0.9564444444444444,
      "grad_norm": 0.9338057637214661,
      "learning_rate": 0.0001547779273216689,
      "loss": 0.1094,
      "step": 269
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.6018155813217163,
      "learning_rate": 0.00015450874831763123,
      "loss": 0.1757,
      "step": 270
    },
    {
      "epoch": 0.9635555555555556,
      "grad_norm": 2.1283836364746094,
      "learning_rate": 0.00015423956931359355,
      "loss": 0.2269,
      "step": 271
    },
    {
      "epoch": 0.9671111111111111,
      "grad_norm": 2.097332715988159,
      "learning_rate": 0.00015397039030955588,
      "loss": 0.2735,
      "step": 272
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 1.9954752922058105,
      "learning_rate": 0.00015370121130551818,
      "loss": 0.3097,
      "step": 273
    },
    {
      "epoch": 0.9742222222222222,
      "grad_norm": 2.2187533378601074,
      "learning_rate": 0.00015343203230148048,
      "loss": 0.2678,
      "step": 274
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.06121826171875,
      "learning_rate": 0.0001531628532974428,
      "loss": 0.1606,
      "step": 275
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 1.46017324924469,
      "learning_rate": 0.00015289367429340513,
      "loss": 0.2079,
      "step": 276
    },
    {
      "epoch": 0.9848888888888889,
      "grad_norm": 1.1075791120529175,
      "learning_rate": 0.00015262449528936743,
      "loss": 0.1655,
      "step": 277
    },
    {
      "epoch": 0.9884444444444445,
      "grad_norm": 1.8120288848876953,
      "learning_rate": 0.00015235531628532976,
      "loss": 0.1545,
      "step": 278
    },
    {
      "epoch": 0.992,
      "grad_norm": 2.390756130218506,
      "learning_rate": 0.00015208613728129205,
      "loss": 0.3616,
      "step": 279
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 2.1333436965942383,
      "learning_rate": 0.00015181695827725438,
      "loss": 0.457,
      "step": 280
    },
    {
      "epoch": 0.9991111111111111,
      "grad_norm": 1.1114246845245361,
      "learning_rate": 0.0001515477792732167,
      "loss": 0.1664,
      "step": 281
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.9711302518844604,
      "learning_rate": 0.000151278600269179,
      "loss": 0.1547,
      "step": 282
    },
    {
      "epoch": 1.0035555555555555,
      "grad_norm": 3.3716697692871094,
      "learning_rate": 0.00015100942126514133,
      "loss": 0.3885,
      "step": 283
    },
    {
      "epoch": 1.007111111111111,
      "grad_norm": 1.1015137434005737,
      "learning_rate": 0.00015074024226110363,
      "loss": 0.1535,
      "step": 284
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 1.1631591320037842,
      "learning_rate": 0.00015047106325706596,
      "loss": 0.183,
      "step": 285
    },
    {
      "epoch": 1.0142222222222221,
      "grad_norm": 0.8793090581893921,
      "learning_rate": 0.00015020188425302828,
      "loss": 0.0922,
      "step": 286
    },
    {
      "epoch": 1.0177777777777777,
      "grad_norm": 1.801939845085144,
      "learning_rate": 0.00014993270524899058,
      "loss": 0.1508,
      "step": 287
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 1.9405453205108643,
      "learning_rate": 0.0001496635262449529,
      "loss": 0.2897,
      "step": 288
    },
    {
      "epoch": 1.024888888888889,
      "grad_norm": 1.1193333864212036,
      "learning_rate": 0.0001493943472409152,
      "loss": 0.1557,
      "step": 289
    },
    {
      "epoch": 1.0284444444444445,
      "grad_norm": 0.8549714684486389,
      "learning_rate": 0.00014912516823687753,
      "loss": 0.0846,
      "step": 290
    },
    {
      "epoch": 1.032,
      "grad_norm": 1.2124375104904175,
      "learning_rate": 0.00014885598923283986,
      "loss": 0.1999,
      "step": 291
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 0.9481258392333984,
      "learning_rate": 0.00014858681022880216,
      "loss": 0.1328,
      "step": 292
    },
    {
      "epoch": 1.039111111111111,
      "grad_norm": 1.044527530670166,
      "learning_rate": 0.00014831763122476448,
      "loss": 0.1445,
      "step": 293
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.9684349298477173,
      "learning_rate": 0.00014804845222072678,
      "loss": 0.0502,
      "step": 294
    },
    {
      "epoch": 1.0462222222222222,
      "grad_norm": 2.46555757522583,
      "learning_rate": 0.00014777927321668908,
      "loss": 0.2329,
      "step": 295
    },
    {
      "epoch": 1.0497777777777777,
      "grad_norm": 1.6031893491744995,
      "learning_rate": 0.00014751009421265144,
      "loss": 0.116,
      "step": 296
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 1.2798947095870972,
      "learning_rate": 0.00014724091520861373,
      "loss": 0.1029,
      "step": 297
    },
    {
      "epoch": 1.056888888888889,
      "grad_norm": 1.616414189338684,
      "learning_rate": 0.00014697173620457606,
      "loss": 0.2111,
      "step": 298
    },
    {
      "epoch": 1.0604444444444445,
      "grad_norm": 1.058650016784668,
      "learning_rate": 0.00014670255720053836,
      "loss": 0.093,
      "step": 299
    },
    {
      "epoch": 1.064,
      "grad_norm": 1.2231093645095825,
      "learning_rate": 0.00014643337819650066,
      "loss": 0.1618,
      "step": 300
    },
    {
      "epoch": 1.0675555555555556,
      "grad_norm": 0.7709832787513733,
      "learning_rate": 0.000146164199192463,
      "loss": 0.0685,
      "step": 301
    },
    {
      "epoch": 1.0711111111111111,
      "grad_norm": 0.7199633121490479,
      "learning_rate": 0.0001458950201884253,
      "loss": 0.0699,
      "step": 302
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 1.5841765403747559,
      "learning_rate": 0.0001456258411843876,
      "loss": 0.1503,
      "step": 303
    },
    {
      "epoch": 1.0782222222222222,
      "grad_norm": 1.3483582735061646,
      "learning_rate": 0.00014535666218034994,
      "loss": 0.1812,
      "step": 304
    },
    {
      "epoch": 1.0817777777777777,
      "grad_norm": 1.0037134885787964,
      "learning_rate": 0.00014508748317631224,
      "loss": 0.0815,
      "step": 305
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 1.6594412326812744,
      "learning_rate": 0.0001448183041722746,
      "loss": 0.271,
      "step": 306
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.8589193820953369,
      "learning_rate": 0.0001445491251682369,
      "loss": 0.1438,
      "step": 307
    },
    {
      "epoch": 1.0924444444444443,
      "grad_norm": 2.0304477214813232,
      "learning_rate": 0.0001442799461641992,
      "loss": 0.1891,
      "step": 308
    },
    {
      "epoch": 1.096,
      "grad_norm": 1.148711919784546,
      "learning_rate": 0.0001440107671601615,
      "loss": 0.1279,
      "step": 309
    },
    {
      "epoch": 1.0995555555555556,
      "grad_norm": 1.8858845233917236,
      "learning_rate": 0.0001437415881561238,
      "loss": 0.197,
      "step": 310
    },
    {
      "epoch": 1.1031111111111112,
      "grad_norm": 1.6745566129684448,
      "learning_rate": 0.00014347240915208614,
      "loss": 0.1527,
      "step": 311
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 1.5631842613220215,
      "learning_rate": 0.00014320323014804846,
      "loss": 0.283,
      "step": 312
    },
    {
      "epoch": 1.1102222222222222,
      "grad_norm": 1.47993004322052,
      "learning_rate": 0.00014293405114401076,
      "loss": 0.1774,
      "step": 313
    },
    {
      "epoch": 1.1137777777777778,
      "grad_norm": 2.3405487537384033,
      "learning_rate": 0.0001426648721399731,
      "loss": 0.2671,
      "step": 314
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 1.3646390438079834,
      "learning_rate": 0.0001423956931359354,
      "loss": 0.2025,
      "step": 315
    },
    {
      "epoch": 1.1208888888888888,
      "grad_norm": 1.3259129524230957,
      "learning_rate": 0.00014212651413189771,
      "loss": 0.1483,
      "step": 316
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 3.0947630405426025,
      "learning_rate": 0.00014185733512786004,
      "loss": 0.2617,
      "step": 317
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 1.6526049375534058,
      "learning_rate": 0.00014158815612382234,
      "loss": 0.1936,
      "step": 318
    },
    {
      "epoch": 1.1315555555555556,
      "grad_norm": 1.0764763355255127,
      "learning_rate": 0.00014131897711978467,
      "loss": 0.1483,
      "step": 319
    },
    {
      "epoch": 1.1351111111111112,
      "grad_norm": 1.662041425704956,
      "learning_rate": 0.000141049798115747,
      "loss": 0.1703,
      "step": 320
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.6927356719970703,
      "learning_rate": 0.0001407806191117093,
      "loss": 0.062,
      "step": 321
    },
    {
      "epoch": 1.1422222222222222,
      "grad_norm": 1.347030520439148,
      "learning_rate": 0.00014051144010767162,
      "loss": 0.1069,
      "step": 322
    },
    {
      "epoch": 1.1457777777777778,
      "grad_norm": 1.226531982421875,
      "learning_rate": 0.00014024226110363392,
      "loss": 0.1057,
      "step": 323
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 2.153378963470459,
      "learning_rate": 0.00013997308209959624,
      "loss": 0.256,
      "step": 324
    },
    {
      "epoch": 1.1528888888888889,
      "grad_norm": 0.8056048154830933,
      "learning_rate": 0.00013970390309555857,
      "loss": 0.1065,
      "step": 325
    },
    {
      "epoch": 1.1564444444444444,
      "grad_norm": 1.3128670454025269,
      "learning_rate": 0.00013943472409152087,
      "loss": 0.1289,
      "step": 326
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.7728413343429565,
      "learning_rate": 0.0001391655450874832,
      "loss": 0.1944,
      "step": 327
    },
    {
      "epoch": 1.1635555555555555,
      "grad_norm": 1.1779669523239136,
      "learning_rate": 0.0001388963660834455,
      "loss": 0.1051,
      "step": 328
    },
    {
      "epoch": 1.1671111111111112,
      "grad_norm": 1.0403189659118652,
      "learning_rate": 0.00013862718707940782,
      "loss": 0.1234,
      "step": 329
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.8155553936958313,
      "learning_rate": 0.00013835800807537014,
      "loss": 0.0823,
      "step": 330
    },
    {
      "epoch": 1.1742222222222223,
      "grad_norm": 2.03707218170166,
      "learning_rate": 0.00013808882907133244,
      "loss": 0.308,
      "step": 331
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.8309100866317749,
      "learning_rate": 0.00013781965006729477,
      "loss": 0.0762,
      "step": 332
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 1.4044522047042847,
      "learning_rate": 0.00013755047106325707,
      "loss": 0.1735,
      "step": 333
    },
    {
      "epoch": 1.1848888888888889,
      "grad_norm": 0.9011748433113098,
      "learning_rate": 0.00013728129205921937,
      "loss": 0.1513,
      "step": 334
    },
    {
      "epoch": 1.1884444444444444,
      "grad_norm": 1.2139878273010254,
      "learning_rate": 0.00013701211305518172,
      "loss": 0.1971,
      "step": 335
    },
    {
      "epoch": 1.192,
      "grad_norm": 1.2557393312454224,
      "learning_rate": 0.00013674293405114402,
      "loss": 0.1466,
      "step": 336
    },
    {
      "epoch": 1.1955555555555555,
      "grad_norm": 2.107708215713501,
      "learning_rate": 0.00013647375504710635,
      "loss": 0.2283,
      "step": 337
    },
    {
      "epoch": 1.199111111111111,
      "grad_norm": 1.1470390558242798,
      "learning_rate": 0.00013620457604306865,
      "loss": 0.145,
      "step": 338
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 1.7137854099273682,
      "learning_rate": 0.00013593539703903094,
      "loss": 0.2404,
      "step": 339
    },
    {
      "epoch": 1.2062222222222223,
      "grad_norm": 1.7908140420913696,
      "learning_rate": 0.0001356662180349933,
      "loss": 0.2248,
      "step": 340
    },
    {
      "epoch": 1.2097777777777778,
      "grad_norm": 1.4163470268249512,
      "learning_rate": 0.0001353970390309556,
      "loss": 0.1936,
      "step": 341
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 2.3024520874023438,
      "learning_rate": 0.0001351278600269179,
      "loss": 0.1514,
      "step": 342
    },
    {
      "epoch": 1.216888888888889,
      "grad_norm": 1.6218980550765991,
      "learning_rate": 0.00013485868102288022,
      "loss": 0.1782,
      "step": 343
    },
    {
      "epoch": 1.2204444444444444,
      "grad_norm": 1.2429587841033936,
      "learning_rate": 0.00013458950201884252,
      "loss": 0.2113,
      "step": 344
    },
    {
      "epoch": 1.224,
      "grad_norm": 1.0917668342590332,
      "learning_rate": 0.00013432032301480487,
      "loss": 0.1832,
      "step": 345
    },
    {
      "epoch": 1.2275555555555555,
      "grad_norm": 1.9010086059570312,
      "learning_rate": 0.00013405114401076717,
      "loss": 0.2061,
      "step": 346
    },
    {
      "epoch": 1.231111111111111,
      "grad_norm": 1.3065053224563599,
      "learning_rate": 0.00013378196500672947,
      "loss": 0.1334,
      "step": 347
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 1.7331147193908691,
      "learning_rate": 0.0001335127860026918,
      "loss": 0.149,
      "step": 348
    },
    {
      "epoch": 1.2382222222222223,
      "grad_norm": 0.8300129175186157,
      "learning_rate": 0.0001332436069986541,
      "loss": 0.1152,
      "step": 349
    },
    {
      "epoch": 1.2417777777777779,
      "grad_norm": 0.9100944399833679,
      "learning_rate": 0.00013297442799461642,
      "loss": 0.084,
      "step": 350
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 1.0331075191497803,
      "learning_rate": 0.00013270524899057875,
      "loss": 0.0743,
      "step": 351
    },
    {
      "epoch": 1.248888888888889,
      "grad_norm": 1.6657640933990479,
      "learning_rate": 0.00013243606998654105,
      "loss": 0.1399,
      "step": 352
    },
    {
      "epoch": 1.2524444444444445,
      "grad_norm": 1.9556974172592163,
      "learning_rate": 0.00013216689098250337,
      "loss": 0.2214,
      "step": 353
    },
    {
      "epoch": 1.256,
      "grad_norm": 1.6011335849761963,
      "learning_rate": 0.00013189771197846567,
      "loss": 0.1764,
      "step": 354
    },
    {
      "epoch": 1.2595555555555555,
      "grad_norm": 1.8209842443466187,
      "learning_rate": 0.000131628532974428,
      "loss": 0.2599,
      "step": 355
    },
    {
      "epoch": 1.263111111111111,
      "grad_norm": 0.9177243709564209,
      "learning_rate": 0.00013135935397039033,
      "loss": 0.0989,
      "step": 356
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 2.2565267086029053,
      "learning_rate": 0.00013109017496635262,
      "loss": 0.2732,
      "step": 357
    },
    {
      "epoch": 1.2702222222222221,
      "grad_norm": 1.3750512599945068,
      "learning_rate": 0.00013082099596231495,
      "loss": 0.1637,
      "step": 358
    },
    {
      "epoch": 1.2737777777777777,
      "grad_norm": 1.6841477155685425,
      "learning_rate": 0.00013055181695827725,
      "loss": 0.2141,
      "step": 359
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 1.1940034627914429,
      "learning_rate": 0.00013028263795423958,
      "loss": 0.0772,
      "step": 360
    },
    {
      "epoch": 1.280888888888889,
      "grad_norm": 1.0710903406143188,
      "learning_rate": 0.0001300134589502019,
      "loss": 0.1686,
      "step": 361
    },
    {
      "epoch": 1.2844444444444445,
      "grad_norm": 1.8805502653121948,
      "learning_rate": 0.0001297442799461642,
      "loss": 0.1956,
      "step": 362
    },
    {
      "epoch": 1.288,
      "grad_norm": 2.040799856185913,
      "learning_rate": 0.00012947510094212653,
      "loss": 0.251,
      "step": 363
    },
    {
      "epoch": 1.2915555555555556,
      "grad_norm": 2.046177387237549,
      "learning_rate": 0.00012920592193808883,
      "loss": 0.1642,
      "step": 364
    },
    {
      "epoch": 1.295111111111111,
      "grad_norm": 3.614863395690918,
      "learning_rate": 0.00012893674293405115,
      "loss": 0.2975,
      "step": 365
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 1.1160032749176025,
      "learning_rate": 0.00012866756393001348,
      "loss": 0.1004,
      "step": 366
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 2.0393385887145996,
      "learning_rate": 0.00012839838492597578,
      "loss": 0.0843,
      "step": 367
    },
    {
      "epoch": 1.3057777777777777,
      "grad_norm": 1.4062014818191528,
      "learning_rate": 0.00012812920592193808,
      "loss": 0.0913,
      "step": 368
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 1.2522838115692139,
      "learning_rate": 0.0001278600269179004,
      "loss": 0.2333,
      "step": 369
    },
    {
      "epoch": 1.3128888888888888,
      "grad_norm": 2.1729235649108887,
      "learning_rate": 0.00012759084791386273,
      "loss": 0.0944,
      "step": 370
    },
    {
      "epoch": 1.3164444444444445,
      "grad_norm": 1.9654595851898193,
      "learning_rate": 0.00012732166890982505,
      "loss": 0.2534,
      "step": 371
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.4727864265441895,
      "learning_rate": 0.00012705248990578735,
      "loss": 0.2373,
      "step": 372
    },
    {
      "epoch": 1.3235555555555556,
      "grad_norm": 1.1611695289611816,
      "learning_rate": 0.00012678331090174965,
      "loss": 0.105,
      "step": 373
    },
    {
      "epoch": 1.3271111111111111,
      "grad_norm": 1.8822758197784424,
      "learning_rate": 0.00012651413189771198,
      "loss": 0.2042,
      "step": 374
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.8200066685676575,
      "learning_rate": 0.0001262449528936743,
      "loss": 0.1408,
      "step": 375
    },
    {
      "epoch": 1.3342222222222222,
      "grad_norm": 1.007015585899353,
      "learning_rate": 0.0001259757738896366,
      "loss": 0.0918,
      "step": 376
    },
    {
      "epoch": 1.3377777777777777,
      "grad_norm": 1.3979414701461792,
      "learning_rate": 0.00012570659488559893,
      "loss": 0.1886,
      "step": 377
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 1.5646698474884033,
      "learning_rate": 0.00012543741588156123,
      "loss": 0.1205,
      "step": 378
    },
    {
      "epoch": 1.3448888888888888,
      "grad_norm": 0.9475061893463135,
      "learning_rate": 0.00012516823687752356,
      "loss": 0.0634,
      "step": 379
    },
    {
      "epoch": 1.3484444444444446,
      "grad_norm": 2.2085065841674805,
      "learning_rate": 0.00012489905787348588,
      "loss": 0.2643,
      "step": 380
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 1.0029385089874268,
      "learning_rate": 0.00012462987886944818,
      "loss": 0.1019,
      "step": 381
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 1.6136457920074463,
      "learning_rate": 0.0001243606998654105,
      "loss": 0.2349,
      "step": 382
    },
    {
      "epoch": 1.3591111111111112,
      "grad_norm": 1.5356446504592896,
      "learning_rate": 0.0001240915208613728,
      "loss": 0.1577,
      "step": 383
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 1.5236259698867798,
      "learning_rate": 0.00012382234185733513,
      "loss": 0.184,
      "step": 384
    },
    {
      "epoch": 1.3662222222222222,
      "grad_norm": 2.3564229011535645,
      "learning_rate": 0.00012355316285329746,
      "loss": 0.1207,
      "step": 385
    },
    {
      "epoch": 1.3697777777777778,
      "grad_norm": 1.024688959121704,
      "learning_rate": 0.00012328398384925976,
      "loss": 0.0959,
      "step": 386
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 1.2558876276016235,
      "learning_rate": 0.00012301480484522208,
      "loss": 0.0968,
      "step": 387
    },
    {
      "epoch": 1.3768888888888888,
      "grad_norm": 0.901168704032898,
      "learning_rate": 0.00012274562584118438,
      "loss": 0.0507,
      "step": 388
    },
    {
      "epoch": 1.3804444444444444,
      "grad_norm": 1.2287055253982544,
      "learning_rate": 0.0001224764468371467,
      "loss": 0.1114,
      "step": 389
    },
    {
      "epoch": 1.384,
      "grad_norm": 1.7820336818695068,
      "learning_rate": 0.00012220726783310903,
      "loss": 0.1066,
      "step": 390
    },
    {
      "epoch": 1.3875555555555557,
      "grad_norm": 0.8306026458740234,
      "learning_rate": 0.00012193808882907133,
      "loss": 0.0688,
      "step": 391
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 2.032001256942749,
      "learning_rate": 0.00012166890982503365,
      "loss": 0.2484,
      "step": 392
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 2.016287088394165,
      "learning_rate": 0.00012139973082099596,
      "loss": 0.209,
      "step": 393
    },
    {
      "epoch": 1.3982222222222223,
      "grad_norm": 1.697540283203125,
      "learning_rate": 0.0001211305518169583,
      "loss": 0.2769,
      "step": 394
    },
    {
      "epoch": 1.4017777777777778,
      "grad_norm": 1.461546540260315,
      "learning_rate": 0.0001208613728129206,
      "loss": 0.1021,
      "step": 395
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 1.3945800065994263,
      "learning_rate": 0.00012059219380888291,
      "loss": 0.2549,
      "step": 396
    },
    {
      "epoch": 1.4088888888888889,
      "grad_norm": 0.9186742901802063,
      "learning_rate": 0.00012032301480484522,
      "loss": 0.0695,
      "step": 397
    },
    {
      "epoch": 1.4124444444444444,
      "grad_norm": 1.602012276649475,
      "learning_rate": 0.00012005383580080754,
      "loss": 0.1626,
      "step": 398
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.7302123308181763,
      "learning_rate": 0.00011978465679676986,
      "loss": 0.0424,
      "step": 399
    },
    {
      "epoch": 1.4195555555555557,
      "grad_norm": 1.199017882347107,
      "learning_rate": 0.00011951547779273217,
      "loss": 0.1269,
      "step": 400
    },
    {
      "epoch": 1.423111111111111,
      "grad_norm": 0.8213843703269958,
      "learning_rate": 0.00011924629878869449,
      "loss": 0.0801,
      "step": 401
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 2.7975287437438965,
      "learning_rate": 0.0001189771197846568,
      "loss": 0.2994,
      "step": 402
    },
    {
      "epoch": 1.4302222222222223,
      "grad_norm": 1.5594102144241333,
      "learning_rate": 0.00011870794078061911,
      "loss": 0.1473,
      "step": 403
    },
    {
      "epoch": 1.4337777777777778,
      "grad_norm": 1.2735631465911865,
      "learning_rate": 0.00011843876177658144,
      "loss": 0.2128,
      "step": 404
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 1.0970335006713867,
      "learning_rate": 0.00011816958277254375,
      "loss": 0.1284,
      "step": 405
    },
    {
      "epoch": 1.4408888888888889,
      "grad_norm": 1.2890454530715942,
      "learning_rate": 0.00011790040376850606,
      "loss": 0.1057,
      "step": 406
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.9056270718574524,
      "learning_rate": 0.00011763122476446838,
      "loss": 0.0779,
      "step": 407
    },
    {
      "epoch": 1.448,
      "grad_norm": 1.5866323709487915,
      "learning_rate": 0.00011736204576043069,
      "loss": 0.2397,
      "step": 408
    },
    {
      "epoch": 1.4515555555555555,
      "grad_norm": 0.7520661354064941,
      "learning_rate": 0.00011709286675639301,
      "loss": 0.0737,
      "step": 409
    },
    {
      "epoch": 1.455111111111111,
      "grad_norm": 1.1492524147033691,
      "learning_rate": 0.00011682368775235533,
      "loss": 0.1973,
      "step": 410
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 1.0486136674880981,
      "learning_rate": 0.00011655450874831764,
      "loss": 0.0851,
      "step": 411
    },
    {
      "epoch": 1.462222222222222,
      "grad_norm": 1.432553768157959,
      "learning_rate": 0.00011628532974427995,
      "loss": 0.1175,
      "step": 412
    },
    {
      "epoch": 1.4657777777777778,
      "grad_norm": 2.152362585067749,
      "learning_rate": 0.00011601615074024226,
      "loss": 0.2063,
      "step": 413
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 1.3977584838867188,
      "learning_rate": 0.00011574697173620459,
      "loss": 0.1536,
      "step": 414
    },
    {
      "epoch": 1.472888888888889,
      "grad_norm": 2.736027717590332,
      "learning_rate": 0.0001154777927321669,
      "loss": 0.2868,
      "step": 415
    },
    {
      "epoch": 1.4764444444444444,
      "grad_norm": 1.0270427465438843,
      "learning_rate": 0.00011520861372812922,
      "loss": 0.1214,
      "step": 416
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.2671003341674805,
      "learning_rate": 0.00011493943472409153,
      "loss": 0.1611,
      "step": 417
    },
    {
      "epoch": 1.4835555555555555,
      "grad_norm": 2.512117862701416,
      "learning_rate": 0.00011467025572005383,
      "loss": 0.1459,
      "step": 418
    },
    {
      "epoch": 1.487111111111111,
      "grad_norm": 1.521756649017334,
      "learning_rate": 0.00011440107671601617,
      "loss": 0.0845,
      "step": 419
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.8336865901947021,
      "learning_rate": 0.00011413189771197848,
      "loss": 0.1283,
      "step": 420
    },
    {
      "epoch": 1.4942222222222221,
      "grad_norm": 1.8346341848373413,
      "learning_rate": 0.00011386271870794079,
      "loss": 0.1633,
      "step": 421
    },
    {
      "epoch": 1.4977777777777779,
      "grad_norm": 1.9720593690872192,
      "learning_rate": 0.00011359353970390309,
      "loss": 0.165,
      "step": 422
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 1.344612956047058,
      "learning_rate": 0.0001133243606998654,
      "loss": 0.0633,
      "step": 423
    },
    {
      "epoch": 1.504888888888889,
      "grad_norm": 1.7976975440979004,
      "learning_rate": 0.00011305518169582774,
      "loss": 0.1991,
      "step": 424
    },
    {
      "epoch": 1.5084444444444445,
      "grad_norm": 1.2733609676361084,
      "learning_rate": 0.00011278600269179006,
      "loss": 0.1127,
      "step": 425
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.1763142347335815,
      "learning_rate": 0.00011251682368775235,
      "loss": 0.1427,
      "step": 426
    },
    {
      "epoch": 1.5155555555555555,
      "grad_norm": 0.9423421621322632,
      "learning_rate": 0.00011224764468371467,
      "loss": 0.0949,
      "step": 427
    },
    {
      "epoch": 1.519111111111111,
      "grad_norm": 1.335646390914917,
      "learning_rate": 0.00011197846567967698,
      "loss": 0.2437,
      "step": 428
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 1.1872601509094238,
      "learning_rate": 0.00011170928667563932,
      "loss": 0.0936,
      "step": 429
    },
    {
      "epoch": 1.5262222222222221,
      "grad_norm": 1.1522389650344849,
      "learning_rate": 0.00011144010767160162,
      "loss": 0.224,
      "step": 430
    },
    {
      "epoch": 1.529777777777778,
      "grad_norm": 1.495286226272583,
      "learning_rate": 0.00011117092866756393,
      "loss": 0.2013,
      "step": 431
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 2.4908437728881836,
      "learning_rate": 0.00011090174966352624,
      "loss": 0.2681,
      "step": 432
    },
    {
      "epoch": 1.536888888888889,
      "grad_norm": 2.1335995197296143,
      "learning_rate": 0.00011063257065948856,
      "loss": 0.1558,
      "step": 433
    },
    {
      "epoch": 1.5404444444444443,
      "grad_norm": 1.115425944328308,
      "learning_rate": 0.00011036339165545088,
      "loss": 0.1201,
      "step": 434
    },
    {
      "epoch": 1.544,
      "grad_norm": 2.0194993019104004,
      "learning_rate": 0.0001100942126514132,
      "loss": 0.2211,
      "step": 435
    },
    {
      "epoch": 1.5475555555555556,
      "grad_norm": 1.2186092138290405,
      "learning_rate": 0.00010982503364737551,
      "loss": 0.1378,
      "step": 436
    },
    {
      "epoch": 1.551111111111111,
      "grad_norm": 1.467484712600708,
      "learning_rate": 0.00010955585464333782,
      "loss": 0.1918,
      "step": 437
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 1.3642059564590454,
      "learning_rate": 0.00010928667563930013,
      "loss": 0.187,
      "step": 438
    },
    {
      "epoch": 1.5582222222222222,
      "grad_norm": 1.114992618560791,
      "learning_rate": 0.00010901749663526246,
      "loss": 0.1346,
      "step": 439
    },
    {
      "epoch": 1.561777777777778,
      "grad_norm": 2.3477349281311035,
      "learning_rate": 0.00010874831763122477,
      "loss": 0.2359,
      "step": 440
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 1.3658705949783325,
      "learning_rate": 0.00010847913862718708,
      "loss": 0.2508,
      "step": 441
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 0.8107307553291321,
      "learning_rate": 0.0001082099596231494,
      "loss": 0.1645,
      "step": 442
    },
    {
      "epoch": 1.5724444444444443,
      "grad_norm": 1.0572888851165771,
      "learning_rate": 0.00010794078061911172,
      "loss": 0.1516,
      "step": 443
    },
    {
      "epoch": 1.576,
      "grad_norm": 1.7000030279159546,
      "learning_rate": 0.00010767160161507404,
      "loss": 0.0892,
      "step": 444
    },
    {
      "epoch": 1.5795555555555556,
      "grad_norm": 1.3925490379333496,
      "learning_rate": 0.00010740242261103635,
      "loss": 0.0801,
      "step": 445
    },
    {
      "epoch": 1.5831111111111111,
      "grad_norm": 1.5953987836837769,
      "learning_rate": 0.00010713324360699866,
      "loss": 0.2426,
      "step": 446
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 1.6280221939086914,
      "learning_rate": 0.00010686406460296097,
      "loss": 0.2417,
      "step": 447
    },
    {
      "epoch": 1.5902222222222222,
      "grad_norm": 1.411734938621521,
      "learning_rate": 0.0001065948855989233,
      "loss": 0.239,
      "step": 448
    },
    {
      "epoch": 1.5937777777777777,
      "grad_norm": 3.3871796131134033,
      "learning_rate": 0.00010632570659488561,
      "loss": 0.2686,
      "step": 449
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 1.0268148183822632,
      "learning_rate": 0.00010605652759084792,
      "loss": 0.0579,
      "step": 450
    },
    {
      "epoch": 1.600888888888889,
      "grad_norm": 1.6151080131530762,
      "learning_rate": 0.00010578734858681024,
      "loss": 0.1441,
      "step": 451
    },
    {
      "epoch": 1.6044444444444443,
      "grad_norm": 0.9226762652397156,
      "learning_rate": 0.00010551816958277254,
      "loss": 0.1235,
      "step": 452
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.8754933476448059,
      "learning_rate": 0.00010524899057873488,
      "loss": 0.0985,
      "step": 453
    },
    {
      "epoch": 1.6115555555555554,
      "grad_norm": 1.6619751453399658,
      "learning_rate": 0.00010497981157469719,
      "loss": 0.1533,
      "step": 454
    },
    {
      "epoch": 1.6151111111111112,
      "grad_norm": 1.2760047912597656,
      "learning_rate": 0.0001047106325706595,
      "loss": 0.1627,
      "step": 455
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 1.2620010375976562,
      "learning_rate": 0.0001044414535666218,
      "loss": 0.1538,
      "step": 456
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 2.3732666969299316,
      "learning_rate": 0.00010417227456258411,
      "loss": 0.1458,
      "step": 457
    },
    {
      "epoch": 1.6257777777777778,
      "grad_norm": 1.4666600227355957,
      "learning_rate": 0.00010390309555854645,
      "loss": 0.1511,
      "step": 458
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 2.702733278274536,
      "learning_rate": 0.00010363391655450876,
      "loss": 0.2866,
      "step": 459
    },
    {
      "epoch": 1.6328888888888888,
      "grad_norm": 2.148590087890625,
      "learning_rate": 0.00010336473755047106,
      "loss": 0.2997,
      "step": 460
    },
    {
      "epoch": 1.6364444444444444,
      "grad_norm": 0.5221681594848633,
      "learning_rate": 0.00010309555854643338,
      "loss": 0.0423,
      "step": 461
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.6492117643356323,
      "learning_rate": 0.00010282637954239569,
      "loss": 0.2253,
      "step": 462
    },
    {
      "epoch": 1.6435555555555554,
      "grad_norm": 1.4046692848205566,
      "learning_rate": 0.00010255720053835803,
      "loss": 0.151,
      "step": 463
    },
    {
      "epoch": 1.6471111111111112,
      "grad_norm": 1.6105624437332153,
      "learning_rate": 0.00010228802153432033,
      "loss": 0.1168,
      "step": 464
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 2.041365146636963,
      "learning_rate": 0.00010201884253028264,
      "loss": 0.2385,
      "step": 465
    },
    {
      "epoch": 1.6542222222222223,
      "grad_norm": 1.8676501512527466,
      "learning_rate": 0.00010174966352624495,
      "loss": 0.2294,
      "step": 466
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 1.0306425094604492,
      "learning_rate": 0.00010148048452220727,
      "loss": 0.1308,
      "step": 467
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 1.2227333784103394,
      "learning_rate": 0.00010121130551816959,
      "loss": 0.2938,
      "step": 468
    },
    {
      "epoch": 1.6648888888888889,
      "grad_norm": 2.1581907272338867,
      "learning_rate": 0.0001009421265141319,
      "loss": 0.1996,
      "step": 469
    },
    {
      "epoch": 1.6684444444444444,
      "grad_norm": 2.2457327842712402,
      "learning_rate": 0.00010067294751009422,
      "loss": 0.2035,
      "step": 470
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 1.8724266290664673,
      "learning_rate": 0.00010040376850605653,
      "loss": 0.1394,
      "step": 471
    },
    {
      "epoch": 1.6755555555555555,
      "grad_norm": 1.1751654148101807,
      "learning_rate": 0.00010013458950201884,
      "loss": 0.1307,
      "step": 472
    },
    {
      "epoch": 1.6791111111111112,
      "grad_norm": 1.2832000255584717,
      "learning_rate": 9.986541049798115e-05,
      "loss": 0.1153,
      "step": 473
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 1.8296787738800049,
      "learning_rate": 9.959623149394348e-05,
      "loss": 0.1547,
      "step": 474
    },
    {
      "epoch": 1.6862222222222223,
      "grad_norm": 2.109354019165039,
      "learning_rate": 9.932705248990579e-05,
      "loss": 0.3113,
      "step": 475
    },
    {
      "epoch": 1.6897777777777778,
      "grad_norm": 1.7209941148757935,
      "learning_rate": 9.90578734858681e-05,
      "loss": 0.1609,
      "step": 476
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 1.4978065490722656,
      "learning_rate": 9.878869448183042e-05,
      "loss": 0.1424,
      "step": 477
    },
    {
      "epoch": 1.696888888888889,
      "grad_norm": 0.6257785558700562,
      "learning_rate": 9.851951547779273e-05,
      "loss": 0.0827,
      "step": 478
    },
    {
      "epoch": 1.7004444444444444,
      "grad_norm": 1.503553032875061,
      "learning_rate": 9.825033647375506e-05,
      "loss": 0.1653,
      "step": 479
    },
    {
      "epoch": 1.704,
      "grad_norm": 1.138076901435852,
      "learning_rate": 9.798115746971737e-05,
      "loss": 0.1066,
      "step": 480
    },
    {
      "epoch": 1.7075555555555555,
      "grad_norm": 4.74118185043335,
      "learning_rate": 9.771197846567968e-05,
      "loss": 0.1837,
      "step": 481
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 1.0864936113357544,
      "learning_rate": 9.7442799461642e-05,
      "loss": 0.0815,
      "step": 482
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.9334877133369446,
      "learning_rate": 9.717362045760431e-05,
      "loss": 0.0745,
      "step": 483
    },
    {
      "epoch": 1.7182222222222223,
      "grad_norm": 0.9587839841842651,
      "learning_rate": 9.690444145356663e-05,
      "loss": 0.1094,
      "step": 484
    },
    {
      "epoch": 1.7217777777777776,
      "grad_norm": 1.417466640472412,
      "learning_rate": 9.663526244952895e-05,
      "loss": 0.1841,
      "step": 485
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 1.542955756187439,
      "learning_rate": 9.636608344549124e-05,
      "loss": 0.1395,
      "step": 486
    },
    {
      "epoch": 1.728888888888889,
      "grad_norm": 1.0905280113220215,
      "learning_rate": 9.609690444145357e-05,
      "loss": 0.1623,
      "step": 487
    },
    {
      "epoch": 1.7324444444444445,
      "grad_norm": 1.4304945468902588,
      "learning_rate": 9.582772543741588e-05,
      "loss": 0.1681,
      "step": 488
    },
    {
      "epoch": 1.736,
      "grad_norm": 1.8521716594696045,
      "learning_rate": 9.555854643337821e-05,
      "loss": 0.1793,
      "step": 489
    },
    {
      "epoch": 1.7395555555555555,
      "grad_norm": 1.5729695558547974,
      "learning_rate": 9.528936742934051e-05,
      "loss": 0.1052,
      "step": 490
    },
    {
      "epoch": 1.743111111111111,
      "grad_norm": 2.580075979232788,
      "learning_rate": 9.502018842530283e-05,
      "loss": 0.3423,
      "step": 491
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 2.033419132232666,
      "learning_rate": 9.475100942126515e-05,
      "loss": 0.2046,
      "step": 492
    },
    {
      "epoch": 1.7502222222222223,
      "grad_norm": 3.1218652725219727,
      "learning_rate": 9.448183041722746e-05,
      "loss": 0.2492,
      "step": 493
    },
    {
      "epoch": 1.7537777777777777,
      "grad_norm": 0.9641081094741821,
      "learning_rate": 9.421265141318977e-05,
      "loss": 0.0395,
      "step": 494
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 1.9109647274017334,
      "learning_rate": 9.394347240915209e-05,
      "loss": 0.2447,
      "step": 495
    },
    {
      "epoch": 1.7608888888888887,
      "grad_norm": 1.57454252243042,
      "learning_rate": 9.367429340511441e-05,
      "loss": 0.2261,
      "step": 496
    },
    {
      "epoch": 1.7644444444444445,
      "grad_norm": 0.9481619000434875,
      "learning_rate": 9.340511440107672e-05,
      "loss": 0.0636,
      "step": 497
    },
    {
      "epoch": 1.768,
      "grad_norm": 4.4122819900512695,
      "learning_rate": 9.313593539703904e-05,
      "loss": 0.3003,
      "step": 498
    },
    {
      "epoch": 1.7715555555555556,
      "grad_norm": 3.0347654819488525,
      "learning_rate": 9.286675639300135e-05,
      "loss": 0.1278,
      "step": 499
    },
    {
      "epoch": 1.775111111111111,
      "grad_norm": 3.0616188049316406,
      "learning_rate": 9.259757738896366e-05,
      "loss": 0.327,
      "step": 500
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 1.184653401374817,
      "learning_rate": 9.232839838492599e-05,
      "loss": 0.1864,
      "step": 501
    },
    {
      "epoch": 1.7822222222222224,
      "grad_norm": 2.002923011779785,
      "learning_rate": 9.20592193808883e-05,
      "loss": 0.2289,
      "step": 502
    },
    {
      "epoch": 1.7857777777777777,
      "grad_norm": 1.6765329837799072,
      "learning_rate": 9.17900403768506e-05,
      "loss": 0.1667,
      "step": 503
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 1.648181438446045,
      "learning_rate": 9.152086137281293e-05,
      "loss": 0.1763,
      "step": 504
    },
    {
      "epoch": 1.7928888888888888,
      "grad_norm": 1.4983444213867188,
      "learning_rate": 9.125168236877524e-05,
      "loss": 0.2886,
      "step": 505
    },
    {
      "epoch": 1.7964444444444445,
      "grad_norm": 1.994767189025879,
      "learning_rate": 9.098250336473756e-05,
      "loss": 0.1954,
      "step": 506
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.3800760507583618,
      "learning_rate": 9.071332436069986e-05,
      "loss": 0.131,
      "step": 507
    },
    {
      "epoch": 1.8035555555555556,
      "grad_norm": 1.9274529218673706,
      "learning_rate": 9.044414535666218e-05,
      "loss": 0.2368,
      "step": 508
    },
    {
      "epoch": 1.8071111111111111,
      "grad_norm": 1.3344805240631104,
      "learning_rate": 9.01749663526245e-05,
      "loss": 0.194,
      "step": 509
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 1.3108246326446533,
      "learning_rate": 8.990578734858681e-05,
      "loss": 0.1117,
      "step": 510
    },
    {
      "epoch": 1.8142222222222222,
      "grad_norm": 0.8818859457969666,
      "learning_rate": 8.963660834454913e-05,
      "loss": 0.0864,
      "step": 511
    },
    {
      "epoch": 1.8177777777777777,
      "grad_norm": 2.2129528522491455,
      "learning_rate": 8.936742934051144e-05,
      "loss": 0.2751,
      "step": 512
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 2.4573237895965576,
      "learning_rate": 8.909825033647375e-05,
      "loss": 0.1144,
      "step": 513
    },
    {
      "epoch": 1.8248888888888888,
      "grad_norm": 1.90542733669281,
      "learning_rate": 8.882907133243608e-05,
      "loss": 0.1223,
      "step": 514
    },
    {
      "epoch": 1.8284444444444445,
      "grad_norm": 0.8201715350151062,
      "learning_rate": 8.855989232839839e-05,
      "loss": 0.0915,
      "step": 515
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 1.272074818611145,
      "learning_rate": 8.82907133243607e-05,
      "loss": 0.1402,
      "step": 516
    },
    {
      "epoch": 1.8355555555555556,
      "grad_norm": 1.3868319988250732,
      "learning_rate": 8.802153432032302e-05,
      "loss": 0.1305,
      "step": 517
    },
    {
      "epoch": 1.8391111111111111,
      "grad_norm": 1.3920049667358398,
      "learning_rate": 8.775235531628533e-05,
      "loss": 0.1472,
      "step": 518
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 1.8399959802627563,
      "learning_rate": 8.748317631224765e-05,
      "loss": 0.1909,
      "step": 519
    },
    {
      "epoch": 1.8462222222222222,
      "grad_norm": 1.485796332359314,
      "learning_rate": 8.721399730820995e-05,
      "loss": 0.144,
      "step": 520
    },
    {
      "epoch": 1.8497777777777777,
      "grad_norm": 0.8433488011360168,
      "learning_rate": 8.694481830417228e-05,
      "loss": 0.0699,
      "step": 521
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 2.3628885746002197,
      "learning_rate": 8.667563930013459e-05,
      "loss": 0.3271,
      "step": 522
    },
    {
      "epoch": 1.8568888888888888,
      "grad_norm": 0.8996067643165588,
      "learning_rate": 8.640646029609692e-05,
      "loss": 0.1349,
      "step": 523
    },
    {
      "epoch": 1.8604444444444446,
      "grad_norm": 1.1032671928405762,
      "learning_rate": 8.613728129205922e-05,
      "loss": 0.1425,
      "step": 524
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 1.5323735475540161,
      "learning_rate": 8.586810228802153e-05,
      "loss": 0.1846,
      "step": 525
    },
    {
      "epoch": 1.8675555555555556,
      "grad_norm": 1.5579967498779297,
      "learning_rate": 8.559892328398386e-05,
      "loss": 0.1665,
      "step": 526
    },
    {
      "epoch": 1.871111111111111,
      "grad_norm": 1.4585680961608887,
      "learning_rate": 8.532974427994617e-05,
      "loss": 0.1773,
      "step": 527
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 2.423699140548706,
      "learning_rate": 8.506056527590848e-05,
      "loss": 0.2683,
      "step": 528
    },
    {
      "epoch": 1.8782222222222222,
      "grad_norm": 3.3527004718780518,
      "learning_rate": 8.47913862718708e-05,
      "loss": 0.1835,
      "step": 529
    },
    {
      "epoch": 1.8817777777777778,
      "grad_norm": 1.100487232208252,
      "learning_rate": 8.45222072678331e-05,
      "loss": 0.116,
      "step": 530
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 1.2115709781646729,
      "learning_rate": 8.425302826379543e-05,
      "loss": 0.1791,
      "step": 531
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 1.3818542957305908,
      "learning_rate": 8.398384925975775e-05,
      "loss": 0.0951,
      "step": 532
    },
    {
      "epoch": 1.8924444444444446,
      "grad_norm": 0.9887164235115051,
      "learning_rate": 8.371467025572006e-05,
      "loss": 0.1893,
      "step": 533
    },
    {
      "epoch": 1.896,
      "grad_norm": 2.3058793544769287,
      "learning_rate": 8.344549125168237e-05,
      "loss": 0.2203,
      "step": 534
    },
    {
      "epoch": 1.8995555555555557,
      "grad_norm": 1.3906505107879639,
      "learning_rate": 8.317631224764468e-05,
      "loss": 0.2313,
      "step": 535
    },
    {
      "epoch": 1.903111111111111,
      "grad_norm": 1.695953369140625,
      "learning_rate": 8.290713324360701e-05,
      "loss": 0.1796,
      "step": 536
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 1.552889347076416,
      "learning_rate": 8.263795423956932e-05,
      "loss": 0.3292,
      "step": 537
    },
    {
      "epoch": 1.9102222222222223,
      "grad_norm": 1.408850908279419,
      "learning_rate": 8.236877523553163e-05,
      "loss": 0.0674,
      "step": 538
    },
    {
      "epoch": 1.9137777777777778,
      "grad_norm": 1.2066015005111694,
      "learning_rate": 8.209959623149395e-05,
      "loss": 0.1259,
      "step": 539
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.8827457427978516,
      "learning_rate": 8.183041722745626e-05,
      "loss": 0.0954,
      "step": 540
    },
    {
      "epoch": 1.9208888888888889,
      "grad_norm": 1.0463529825210571,
      "learning_rate": 8.156123822341859e-05,
      "loss": 0.1221,
      "step": 541
    },
    {
      "epoch": 1.9244444444444444,
      "grad_norm": 0.8514145016670227,
      "learning_rate": 8.129205921938088e-05,
      "loss": 0.1441,
      "step": 542
    },
    {
      "epoch": 1.928,
      "grad_norm": 1.836064100265503,
      "learning_rate": 8.102288021534321e-05,
      "loss": 0.2055,
      "step": 543
    },
    {
      "epoch": 1.9315555555555557,
      "grad_norm": 1.3391817808151245,
      "learning_rate": 8.075370121130552e-05,
      "loss": 0.1555,
      "step": 544
    },
    {
      "epoch": 1.935111111111111,
      "grad_norm": 0.8442276120185852,
      "learning_rate": 8.048452220726784e-05,
      "loss": 0.078,
      "step": 545
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 1.4063385725021362,
      "learning_rate": 8.021534320323015e-05,
      "loss": 0.1318,
      "step": 546
    },
    {
      "epoch": 1.942222222222222,
      "grad_norm": 1.385428547859192,
      "learning_rate": 7.994616419919246e-05,
      "loss": 0.1766,
      "step": 547
    },
    {
      "epoch": 1.9457777777777778,
      "grad_norm": 1.1738684177398682,
      "learning_rate": 7.967698519515479e-05,
      "loss": 0.1617,
      "step": 548
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 2.126255512237549,
      "learning_rate": 7.94078061911171e-05,
      "loss": 0.1559,
      "step": 549
    },
    {
      "epoch": 1.952888888888889,
      "grad_norm": 1.653240442276001,
      "learning_rate": 7.913862718707941e-05,
      "loss": 0.1921,
      "step": 550
    },
    {
      "epoch": 1.9564444444444444,
      "grad_norm": 1.7109923362731934,
      "learning_rate": 7.886944818304172e-05,
      "loss": 0.2168,
      "step": 551
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.5800122022628784,
      "learning_rate": 7.860026917900404e-05,
      "loss": 0.169,
      "step": 552
    },
    {
      "epoch": 1.9635555555555557,
      "grad_norm": 1.4767696857452393,
      "learning_rate": 7.833109017496636e-05,
      "loss": 0.2153,
      "step": 553
    },
    {
      "epoch": 1.967111111111111,
      "grad_norm": 1.5462446212768555,
      "learning_rate": 7.806191117092868e-05,
      "loss": 0.198,
      "step": 554
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.9375074505805969,
      "learning_rate": 7.779273216689099e-05,
      "loss": 0.1535,
      "step": 555
    },
    {
      "epoch": 1.974222222222222,
      "grad_norm": 1.6797490119934082,
      "learning_rate": 7.75235531628533e-05,
      "loss": 0.2208,
      "step": 556
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 1.083420991897583,
      "learning_rate": 7.725437415881561e-05,
      "loss": 0.0824,
      "step": 557
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 1.4314377307891846,
      "learning_rate": 7.698519515477794e-05,
      "loss": 0.1584,
      "step": 558
    },
    {
      "epoch": 1.984888888888889,
      "grad_norm": 1.8141368627548218,
      "learning_rate": 7.671601615074024e-05,
      "loss": 0.2178,
      "step": 559
    },
    {
      "epoch": 1.9884444444444445,
      "grad_norm": 1.3042551279067993,
      "learning_rate": 7.644683714670257e-05,
      "loss": 0.246,
      "step": 560
    },
    {
      "epoch": 1.992,
      "grad_norm": 1.6837431192398071,
      "learning_rate": 7.617765814266488e-05,
      "loss": 0.1236,
      "step": 561
    },
    {
      "epoch": 1.9955555555555555,
      "grad_norm": 1.187569260597229,
      "learning_rate": 7.590847913862719e-05,
      "loss": 0.1509,
      "step": 562
    },
    {
      "epoch": 1.999111111111111,
      "grad_norm": 1.126617670059204,
      "learning_rate": 7.56393001345895e-05,
      "loss": 0.1095,
      "step": 563
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.10119884461164474,
      "learning_rate": 7.537012113055182e-05,
      "loss": 0.0034,
      "step": 564
    },
    {
      "epoch": 2.0035555555555558,
      "grad_norm": 0.8706568479537964,
      "learning_rate": 7.510094212651414e-05,
      "loss": 0.0788,
      "step": 565
    },
    {
      "epoch": 2.007111111111111,
      "grad_norm": 1.1369128227233887,
      "learning_rate": 7.483176312247645e-05,
      "loss": 0.1048,
      "step": 566
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 1.371267318725586,
      "learning_rate": 7.456258411843877e-05,
      "loss": 0.0481,
      "step": 567
    },
    {
      "epoch": 2.014222222222222,
      "grad_norm": 0.9680373668670654,
      "learning_rate": 7.429340511440108e-05,
      "loss": 0.134,
      "step": 568
    },
    {
      "epoch": 2.017777777777778,
      "grad_norm": 0.9059025049209595,
      "learning_rate": 7.402422611036339e-05,
      "loss": 0.0915,
      "step": 569
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.6962891221046448,
      "learning_rate": 7.375504710632572e-05,
      "loss": 0.0804,
      "step": 570
    },
    {
      "epoch": 2.024888888888889,
      "grad_norm": 0.7342047691345215,
      "learning_rate": 7.348586810228803e-05,
      "loss": 0.0671,
      "step": 571
    },
    {
      "epoch": 2.0284444444444443,
      "grad_norm": 1.1855642795562744,
      "learning_rate": 7.321668909825033e-05,
      "loss": 0.103,
      "step": 572
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.9182586669921875,
      "learning_rate": 7.294751009421266e-05,
      "loss": 0.0988,
      "step": 573
    },
    {
      "epoch": 2.0355555555555553,
      "grad_norm": 1.4086748361587524,
      "learning_rate": 7.267833109017497e-05,
      "loss": 0.1493,
      "step": 574
    },
    {
      "epoch": 2.039111111111111,
      "grad_norm": 0.5585055947303772,
      "learning_rate": 7.24091520861373e-05,
      "loss": 0.0638,
      "step": 575
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.574632465839386,
      "learning_rate": 7.21399730820996e-05,
      "loss": 0.0498,
      "step": 576
    },
    {
      "epoch": 2.046222222222222,
      "grad_norm": 0.7207630276679993,
      "learning_rate": 7.18707940780619e-05,
      "loss": 0.0759,
      "step": 577
    },
    {
      "epoch": 2.049777777777778,
      "grad_norm": 1.0629652738571167,
      "learning_rate": 7.160161507402423e-05,
      "loss": 0.0438,
      "step": 578
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.7399232387542725,
      "learning_rate": 7.133243606998654e-05,
      "loss": 0.0664,
      "step": 579
    },
    {
      "epoch": 2.056888888888889,
      "grad_norm": 1.0197125673294067,
      "learning_rate": 7.106325706594886e-05,
      "loss": 0.0516,
      "step": 580
    },
    {
      "epoch": 2.0604444444444443,
      "grad_norm": 1.6929513216018677,
      "learning_rate": 7.079407806191117e-05,
      "loss": 0.1564,
      "step": 581
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.6961727738380432,
      "learning_rate": 7.05248990578735e-05,
      "loss": 0.0435,
      "step": 582
    },
    {
      "epoch": 2.0675555555555554,
      "grad_norm": 1.1894667148590088,
      "learning_rate": 7.025572005383581e-05,
      "loss": 0.0736,
      "step": 583
    },
    {
      "epoch": 2.071111111111111,
      "grad_norm": 1.0361806154251099,
      "learning_rate": 6.998654104979812e-05,
      "loss": 0.0966,
      "step": 584
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.7605608105659485,
      "learning_rate": 6.971736204576043e-05,
      "loss": 0.0496,
      "step": 585
    },
    {
      "epoch": 2.078222222222222,
      "grad_norm": 1.3032022714614868,
      "learning_rate": 6.944818304172275e-05,
      "loss": 0.1282,
      "step": 586
    },
    {
      "epoch": 2.081777777777778,
      "grad_norm": 1.6117148399353027,
      "learning_rate": 6.917900403768507e-05,
      "loss": 0.2617,
      "step": 587
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 1.2355557680130005,
      "learning_rate": 6.890982503364738e-05,
      "loss": 0.114,
      "step": 588
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 0.8234753608703613,
      "learning_rate": 6.864064602960968e-05,
      "loss": 0.0984,
      "step": 589
    },
    {
      "epoch": 2.0924444444444443,
      "grad_norm": 1.6786861419677734,
      "learning_rate": 6.837146702557201e-05,
      "loss": 0.1097,
      "step": 590
    },
    {
      "epoch": 2.096,
      "grad_norm": 1.5828872919082642,
      "learning_rate": 6.810228802153432e-05,
      "loss": 0.0755,
      "step": 591
    },
    {
      "epoch": 2.0995555555555554,
      "grad_norm": 1.2291594743728638,
      "learning_rate": 6.783310901749665e-05,
      "loss": 0.1267,
      "step": 592
    },
    {
      "epoch": 2.103111111111111,
      "grad_norm": 1.2537816762924194,
      "learning_rate": 6.756393001345895e-05,
      "loss": 0.1102,
      "step": 593
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 1.1646337509155273,
      "learning_rate": 6.729475100942126e-05,
      "loss": 0.0627,
      "step": 594
    },
    {
      "epoch": 2.110222222222222,
      "grad_norm": 2.8586843013763428,
      "learning_rate": 6.702557200538359e-05,
      "loss": 0.108,
      "step": 595
    },
    {
      "epoch": 2.113777777777778,
      "grad_norm": 1.0983308553695679,
      "learning_rate": 6.67563930013459e-05,
      "loss": 0.1124,
      "step": 596
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 1.884576678276062,
      "learning_rate": 6.648721399730821e-05,
      "loss": 0.099,
      "step": 597
    },
    {
      "epoch": 2.120888888888889,
      "grad_norm": 0.8542487025260925,
      "learning_rate": 6.621803499327052e-05,
      "loss": 0.0835,
      "step": 598
    },
    {
      "epoch": 2.1244444444444444,
      "grad_norm": 1.094889760017395,
      "learning_rate": 6.594885598923284e-05,
      "loss": 0.1159,
      "step": 599
    },
    {
      "epoch": 2.128,
      "grad_norm": 2.434468984603882,
      "learning_rate": 6.567967698519516e-05,
      "loss": 0.1271,
      "step": 600
    },
    {
      "epoch": 2.1315555555555554,
      "grad_norm": 1.6136976480484009,
      "learning_rate": 6.541049798115748e-05,
      "loss": 0.1088,
      "step": 601
    },
    {
      "epoch": 2.135111111111111,
      "grad_norm": 2.3535330295562744,
      "learning_rate": 6.514131897711979e-05,
      "loss": 0.0528,
      "step": 602
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 1.3712154626846313,
      "learning_rate": 6.48721399730821e-05,
      "loss": 0.1071,
      "step": 603
    },
    {
      "epoch": 2.1422222222222222,
      "grad_norm": 1.3224716186523438,
      "learning_rate": 6.460296096904441e-05,
      "loss": 0.1238,
      "step": 604
    },
    {
      "epoch": 2.145777777777778,
      "grad_norm": 2.045788526535034,
      "learning_rate": 6.433378196500674e-05,
      "loss": 0.1344,
      "step": 605
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 1.391029715538025,
      "learning_rate": 6.406460296096904e-05,
      "loss": 0.0681,
      "step": 606
    },
    {
      "epoch": 2.152888888888889,
      "grad_norm": 1.8194743394851685,
      "learning_rate": 6.379542395693136e-05,
      "loss": 0.0632,
      "step": 607
    },
    {
      "epoch": 2.1564444444444444,
      "grad_norm": 0.8407613635063171,
      "learning_rate": 6.352624495289368e-05,
      "loss": 0.0596,
      "step": 608
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.0081957578659058,
      "learning_rate": 6.325706594885599e-05,
      "loss": 0.1019,
      "step": 609
    },
    {
      "epoch": 2.1635555555555555,
      "grad_norm": 1.2643566131591797,
      "learning_rate": 6.29878869448183e-05,
      "loss": 0.0681,
      "step": 610
    },
    {
      "epoch": 2.167111111111111,
      "grad_norm": 0.7953142523765564,
      "learning_rate": 6.271870794078061e-05,
      "loss": 0.076,
      "step": 611
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 1.4209973812103271,
      "learning_rate": 6.244952893674294e-05,
      "loss": 0.0796,
      "step": 612
    },
    {
      "epoch": 2.1742222222222223,
      "grad_norm": 0.5013522505760193,
      "learning_rate": 6.218034993270525e-05,
      "loss": 0.0319,
      "step": 613
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 0.6266130208969116,
      "learning_rate": 6.191117092866757e-05,
      "loss": 0.0452,
      "step": 614
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.8026432991027832,
      "learning_rate": 6.164199192462988e-05,
      "loss": 0.0614,
      "step": 615
    },
    {
      "epoch": 2.1848888888888887,
      "grad_norm": 0.9156327843666077,
      "learning_rate": 6.137281292059219e-05,
      "loss": 0.0712,
      "step": 616
    },
    {
      "epoch": 2.1884444444444444,
      "grad_norm": 1.442946195602417,
      "learning_rate": 6.110363391655452e-05,
      "loss": 0.1655,
      "step": 617
    },
    {
      "epoch": 2.192,
      "grad_norm": 1.1829710006713867,
      "learning_rate": 6.083445491251682e-05,
      "loss": 0.1615,
      "step": 618
    },
    {
      "epoch": 2.1955555555555555,
      "grad_norm": 1.2385207414627075,
      "learning_rate": 6.056527590847915e-05,
      "loss": 0.1201,
      "step": 619
    },
    {
      "epoch": 2.1991111111111112,
      "grad_norm": 1.6405504941940308,
      "learning_rate": 6.0296096904441455e-05,
      "loss": 0.1059,
      "step": 620
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 3.2029976844787598,
      "learning_rate": 6.002691790040377e-05,
      "loss": 0.1051,
      "step": 621
    },
    {
      "epoch": 2.2062222222222223,
      "grad_norm": 1.3287659883499146,
      "learning_rate": 5.975773889636609e-05,
      "loss": 0.1281,
      "step": 622
    },
    {
      "epoch": 2.2097777777777776,
      "grad_norm": 1.259123682975769,
      "learning_rate": 5.94885598923284e-05,
      "loss": 0.1775,
      "step": 623
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.9878265261650085,
      "learning_rate": 5.921938088829072e-05,
      "loss": 0.0969,
      "step": 624
    },
    {
      "epoch": 2.2168888888888887,
      "grad_norm": 0.6870311498641968,
      "learning_rate": 5.895020188425303e-05,
      "loss": 0.0247,
      "step": 625
    },
    {
      "epoch": 2.2204444444444444,
      "grad_norm": 0.9499244093894958,
      "learning_rate": 5.8681022880215344e-05,
      "loss": 0.0766,
      "step": 626
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.5080267190933228,
      "learning_rate": 5.841184387617766e-05,
      "loss": 0.0258,
      "step": 627
    },
    {
      "epoch": 2.2275555555555555,
      "grad_norm": 0.940287172794342,
      "learning_rate": 5.8142664872139976e-05,
      "loss": 0.0768,
      "step": 628
    },
    {
      "epoch": 2.2311111111111113,
      "grad_norm": 1.2856229543685913,
      "learning_rate": 5.7873485868102295e-05,
      "loss": 0.1038,
      "step": 629
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 2.025667428970337,
      "learning_rate": 5.760430686406461e-05,
      "loss": 0.094,
      "step": 630
    },
    {
      "epoch": 2.2382222222222223,
      "grad_norm": 0.6730647683143616,
      "learning_rate": 5.7335127860026914e-05,
      "loss": 0.0363,
      "step": 631
    },
    {
      "epoch": 2.2417777777777776,
      "grad_norm": 1.1142735481262207,
      "learning_rate": 5.706594885598924e-05,
      "loss": 0.0911,
      "step": 632
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 0.9347535371780396,
      "learning_rate": 5.6796769851951546e-05,
      "loss": 0.1081,
      "step": 633
    },
    {
      "epoch": 2.2488888888888887,
      "grad_norm": 1.590835690498352,
      "learning_rate": 5.652759084791387e-05,
      "loss": 0.2392,
      "step": 634
    },
    {
      "epoch": 2.2524444444444445,
      "grad_norm": 1.5925270318984985,
      "learning_rate": 5.625841184387618e-05,
      "loss": 0.1234,
      "step": 635
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 1.4887731075286865,
      "learning_rate": 5.598923283983849e-05,
      "loss": 0.1448,
      "step": 636
    },
    {
      "epoch": 2.2595555555555555,
      "grad_norm": 1.6863447427749634,
      "learning_rate": 5.572005383580081e-05,
      "loss": 0.1965,
      "step": 637
    },
    {
      "epoch": 2.2631111111111113,
      "grad_norm": 1.8761290311813354,
      "learning_rate": 5.545087483176312e-05,
      "loss": 0.0675,
      "step": 638
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 1.0521684885025024,
      "learning_rate": 5.518169582772544e-05,
      "loss": 0.0508,
      "step": 639
    },
    {
      "epoch": 2.2702222222222224,
      "grad_norm": 1.6928766965866089,
      "learning_rate": 5.4912516823687754e-05,
      "loss": 0.2458,
      "step": 640
    },
    {
      "epoch": 2.2737777777777777,
      "grad_norm": 0.6134319305419922,
      "learning_rate": 5.4643337819650066e-05,
      "loss": 0.0492,
      "step": 641
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.9176712036132812,
      "learning_rate": 5.4374158815612386e-05,
      "loss": 0.0832,
      "step": 642
    },
    {
      "epoch": 2.2808888888888887,
      "grad_norm": 1.345012903213501,
      "learning_rate": 5.41049798115747e-05,
      "loss": 0.0908,
      "step": 643
    },
    {
      "epoch": 2.2844444444444445,
      "grad_norm": 1.0230966806411743,
      "learning_rate": 5.383580080753702e-05,
      "loss": 0.0942,
      "step": 644
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.7420652508735657,
      "learning_rate": 5.356662180349933e-05,
      "loss": 0.0485,
      "step": 645
    },
    {
      "epoch": 2.2915555555555556,
      "grad_norm": 1.2836990356445312,
      "learning_rate": 5.329744279946165e-05,
      "loss": 0.0947,
      "step": 646
    },
    {
      "epoch": 2.295111111111111,
      "grad_norm": 0.7750752568244934,
      "learning_rate": 5.302826379542396e-05,
      "loss": 0.0826,
      "step": 647
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 1.4061037302017212,
      "learning_rate": 5.275908479138627e-05,
      "loss": 0.0849,
      "step": 648
    },
    {
      "epoch": 2.3022222222222224,
      "grad_norm": 1.8094685077667236,
      "learning_rate": 5.2489905787348594e-05,
      "loss": 0.0546,
      "step": 649
    },
    {
      "epoch": 2.3057777777777777,
      "grad_norm": 0.7102688550949097,
      "learning_rate": 5.22207267833109e-05,
      "loss": 0.0546,
      "step": 650
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 1.2471654415130615,
      "learning_rate": 5.1951547779273226e-05,
      "loss": 0.0385,
      "step": 651
    },
    {
      "epoch": 2.3128888888888888,
      "grad_norm": 1.332103967666626,
      "learning_rate": 5.168236877523553e-05,
      "loss": 0.0554,
      "step": 652
    },
    {
      "epoch": 2.3164444444444445,
      "grad_norm": 0.7596908807754517,
      "learning_rate": 5.1413189771197844e-05,
      "loss": 0.0466,
      "step": 653
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.9827924370765686,
      "learning_rate": 5.1144010767160164e-05,
      "loss": 0.0977,
      "step": 654
    },
    {
      "epoch": 2.3235555555555556,
      "grad_norm": 2.3473942279815674,
      "learning_rate": 5.0874831763122476e-05,
      "loss": 0.1362,
      "step": 655
    },
    {
      "epoch": 2.327111111111111,
      "grad_norm": 1.6559003591537476,
      "learning_rate": 5.0605652759084796e-05,
      "loss": 0.0526,
      "step": 656
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.26047906279563904,
      "learning_rate": 5.033647375504711e-05,
      "loss": 0.0088,
      "step": 657
    },
    {
      "epoch": 2.3342222222222224,
      "grad_norm": 2.041921377182007,
      "learning_rate": 5.006729475100942e-05,
      "loss": 0.1405,
      "step": 658
    },
    {
      "epoch": 2.3377777777777777,
      "grad_norm": 0.9252012372016907,
      "learning_rate": 4.979811574697174e-05,
      "loss": 0.0631,
      "step": 659
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 1.173903226852417,
      "learning_rate": 4.952893674293405e-05,
      "loss": 0.1029,
      "step": 660
    },
    {
      "epoch": 2.344888888888889,
      "grad_norm": 2.22711443901062,
      "learning_rate": 4.9259757738896365e-05,
      "loss": 0.1015,
      "step": 661
    },
    {
      "epoch": 2.3484444444444446,
      "grad_norm": 1.5787495374679565,
      "learning_rate": 4.8990578734858685e-05,
      "loss": 0.1731,
      "step": 662
    },
    {
      "epoch": 2.352,
      "grad_norm": 1.1225072145462036,
      "learning_rate": 4.8721399730821e-05,
      "loss": 0.1179,
      "step": 663
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 1.5338854789733887,
      "learning_rate": 4.845222072678332e-05,
      "loss": 0.1436,
      "step": 664
    },
    {
      "epoch": 2.359111111111111,
      "grad_norm": 1.1024175882339478,
      "learning_rate": 4.818304172274562e-05,
      "loss": 0.0435,
      "step": 665
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.9762207269668579,
      "learning_rate": 4.791386271870794e-05,
      "loss": 0.0653,
      "step": 666
    },
    {
      "epoch": 2.3662222222222224,
      "grad_norm": 1.688027024269104,
      "learning_rate": 4.7644683714670254e-05,
      "loss": 0.0786,
      "step": 667
    },
    {
      "epoch": 2.3697777777777778,
      "grad_norm": 0.8182756900787354,
      "learning_rate": 4.7375504710632574e-05,
      "loss": 0.0575,
      "step": 668
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 1.1907010078430176,
      "learning_rate": 4.7106325706594886e-05,
      "loss": 0.071,
      "step": 669
    },
    {
      "epoch": 2.376888888888889,
      "grad_norm": 1.7742042541503906,
      "learning_rate": 4.6837146702557206e-05,
      "loss": 0.1308,
      "step": 670
    },
    {
      "epoch": 2.3804444444444446,
      "grad_norm": 0.5116295218467712,
      "learning_rate": 4.656796769851952e-05,
      "loss": 0.0249,
      "step": 671
    },
    {
      "epoch": 2.384,
      "grad_norm": 1.2658002376556396,
      "learning_rate": 4.629878869448183e-05,
      "loss": 0.1174,
      "step": 672
    },
    {
      "epoch": 2.3875555555555557,
      "grad_norm": 1.9784737825393677,
      "learning_rate": 4.602960969044415e-05,
      "loss": 0.1602,
      "step": 673
    },
    {
      "epoch": 2.391111111111111,
      "grad_norm": 0.6623943448066711,
      "learning_rate": 4.576043068640646e-05,
      "loss": 0.0259,
      "step": 674
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 3.0838375091552734,
      "learning_rate": 4.549125168236878e-05,
      "loss": 0.1732,
      "step": 675
    },
    {
      "epoch": 2.398222222222222,
      "grad_norm": 1.7623822689056396,
      "learning_rate": 4.522207267833109e-05,
      "loss": 0.0732,
      "step": 676
    },
    {
      "epoch": 2.401777777777778,
      "grad_norm": 0.6868700981140137,
      "learning_rate": 4.495289367429341e-05,
      "loss": 0.0558,
      "step": 677
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 0.9920293688774109,
      "learning_rate": 4.468371467025572e-05,
      "loss": 0.0883,
      "step": 678
    },
    {
      "epoch": 2.408888888888889,
      "grad_norm": 1.5738587379455566,
      "learning_rate": 4.441453566621804e-05,
      "loss": 0.0938,
      "step": 679
    },
    {
      "epoch": 2.4124444444444446,
      "grad_norm": 0.955913245677948,
      "learning_rate": 4.414535666218035e-05,
      "loss": 0.0397,
      "step": 680
    },
    {
      "epoch": 2.416,
      "grad_norm": 1.4765360355377197,
      "learning_rate": 4.3876177658142664e-05,
      "loss": 0.15,
      "step": 681
    },
    {
      "epoch": 2.4195555555555557,
      "grad_norm": 0.7800235748291016,
      "learning_rate": 4.360699865410498e-05,
      "loss": 0.0727,
      "step": 682
    },
    {
      "epoch": 2.423111111111111,
      "grad_norm": 2.7913455963134766,
      "learning_rate": 4.3337819650067296e-05,
      "loss": 0.1691,
      "step": 683
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 1.6665685176849365,
      "learning_rate": 4.306864064602961e-05,
      "loss": 0.101,
      "step": 684
    },
    {
      "epoch": 2.430222222222222,
      "grad_norm": 0.8197429180145264,
      "learning_rate": 4.279946164199193e-05,
      "loss": 0.0537,
      "step": 685
    },
    {
      "epoch": 2.433777777777778,
      "grad_norm": 1.5834773778915405,
      "learning_rate": 4.253028263795424e-05,
      "loss": 0.1223,
      "step": 686
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 1.362532138824463,
      "learning_rate": 4.226110363391655e-05,
      "loss": 0.0648,
      "step": 687
    },
    {
      "epoch": 2.440888888888889,
      "grad_norm": 4.420805931091309,
      "learning_rate": 4.199192462987887e-05,
      "loss": 0.1775,
      "step": 688
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.7487745881080627,
      "learning_rate": 4.1722745625841185e-05,
      "loss": 0.0342,
      "step": 689
    },
    {
      "epoch": 2.448,
      "grad_norm": 1.5226596593856812,
      "learning_rate": 4.1453566621803505e-05,
      "loss": 0.0704,
      "step": 690
    },
    {
      "epoch": 2.4515555555555557,
      "grad_norm": 0.9365220665931702,
      "learning_rate": 4.118438761776582e-05,
      "loss": 0.0421,
      "step": 691
    },
    {
      "epoch": 2.455111111111111,
      "grad_norm": 1.2995586395263672,
      "learning_rate": 4.091520861372813e-05,
      "loss": 0.068,
      "step": 692
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 2.2859761714935303,
      "learning_rate": 4.064602960969044e-05,
      "loss": 0.1282,
      "step": 693
    },
    {
      "epoch": 2.462222222222222,
      "grad_norm": 1.5200841426849365,
      "learning_rate": 4.037685060565276e-05,
      "loss": 0.087,
      "step": 694
    },
    {
      "epoch": 2.465777777777778,
      "grad_norm": 1.3464503288269043,
      "learning_rate": 4.0107671601615074e-05,
      "loss": 0.0844,
      "step": 695
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 1.1968107223510742,
      "learning_rate": 3.9838492597577394e-05,
      "loss": 0.0674,
      "step": 696
    },
    {
      "epoch": 2.472888888888889,
      "grad_norm": 1.853470802307129,
      "learning_rate": 3.9569313593539706e-05,
      "loss": 0.17,
      "step": 697
    },
    {
      "epoch": 2.4764444444444447,
      "grad_norm": 1.1526483297348022,
      "learning_rate": 3.930013458950202e-05,
      "loss": 0.0469,
      "step": 698
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.7980966567993164,
      "learning_rate": 3.903095558546434e-05,
      "loss": 0.1143,
      "step": 699
    },
    {
      "epoch": 2.4835555555555557,
      "grad_norm": 0.8979002237319946,
      "learning_rate": 3.876177658142665e-05,
      "loss": 0.0694,
      "step": 700
    },
    {
      "epoch": 2.487111111111111,
      "grad_norm": 0.9290558695793152,
      "learning_rate": 3.849259757738897e-05,
      "loss": 0.0403,
      "step": 701
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 0.8891337513923645,
      "learning_rate": 3.822341857335128e-05,
      "loss": 0.0414,
      "step": 702
    },
    {
      "epoch": 2.494222222222222,
      "grad_norm": 1.8369618654251099,
      "learning_rate": 3.7954239569313595e-05,
      "loss": 0.1785,
      "step": 703
    },
    {
      "epoch": 2.497777777777778,
      "grad_norm": 0.3443918824195862,
      "learning_rate": 3.768506056527591e-05,
      "loss": 0.0181,
      "step": 704
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 1.322347640991211,
      "learning_rate": 3.741588156123823e-05,
      "loss": 0.0898,
      "step": 705
    },
    {
      "epoch": 2.504888888888889,
      "grad_norm": 1.3770900964736938,
      "learning_rate": 3.714670255720054e-05,
      "loss": 0.096,
      "step": 706
    },
    {
      "epoch": 2.5084444444444447,
      "grad_norm": 1.0731245279312134,
      "learning_rate": 3.687752355316286e-05,
      "loss": 0.0298,
      "step": 707
    },
    {
      "epoch": 2.512,
      "grad_norm": 1.087598443031311,
      "learning_rate": 3.6608344549125165e-05,
      "loss": 0.0807,
      "step": 708
    },
    {
      "epoch": 2.5155555555555553,
      "grad_norm": 1.1644139289855957,
      "learning_rate": 3.6339165545087484e-05,
      "loss": 0.1114,
      "step": 709
    },
    {
      "epoch": 2.519111111111111,
      "grad_norm": 1.9040272235870361,
      "learning_rate": 3.60699865410498e-05,
      "loss": 0.0748,
      "step": 710
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.6074279546737671,
      "learning_rate": 3.5800807537012116e-05,
      "loss": 0.0287,
      "step": 711
    },
    {
      "epoch": 2.526222222222222,
      "grad_norm": 2.0436816215515137,
      "learning_rate": 3.553162853297443e-05,
      "loss": 0.1482,
      "step": 712
    },
    {
      "epoch": 2.529777777777778,
      "grad_norm": 2.1873908042907715,
      "learning_rate": 3.526244952893675e-05,
      "loss": 0.0542,
      "step": 713
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 1.4910662174224854,
      "learning_rate": 3.499327052489906e-05,
      "loss": 0.1081,
      "step": 714
    },
    {
      "epoch": 2.536888888888889,
      "grad_norm": 0.880824863910675,
      "learning_rate": 3.472409152086137e-05,
      "loss": 0.1011,
      "step": 715
    },
    {
      "epoch": 2.5404444444444443,
      "grad_norm": 1.4592690467834473,
      "learning_rate": 3.445491251682369e-05,
      "loss": 0.0803,
      "step": 716
    },
    {
      "epoch": 2.544,
      "grad_norm": 1.2153538465499878,
      "learning_rate": 3.4185733512786005e-05,
      "loss": 0.0813,
      "step": 717
    },
    {
      "epoch": 2.5475555555555554,
      "grad_norm": 1.06681227684021,
      "learning_rate": 3.3916554508748324e-05,
      "loss": 0.0623,
      "step": 718
    },
    {
      "epoch": 2.551111111111111,
      "grad_norm": 0.35865917801856995,
      "learning_rate": 3.364737550471063e-05,
      "loss": 0.0091,
      "step": 719
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 1.0984532833099365,
      "learning_rate": 3.337819650067295e-05,
      "loss": 0.0235,
      "step": 720
    },
    {
      "epoch": 2.558222222222222,
      "grad_norm": 1.9746860265731812,
      "learning_rate": 3.310901749663526e-05,
      "loss": 0.1695,
      "step": 721
    },
    {
      "epoch": 2.561777777777778,
      "grad_norm": 2.0795464515686035,
      "learning_rate": 3.283983849259758e-05,
      "loss": 0.0919,
      "step": 722
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 1.825704574584961,
      "learning_rate": 3.2570659488559894e-05,
      "loss": 0.2612,
      "step": 723
    },
    {
      "epoch": 2.568888888888889,
      "grad_norm": 1.5279099941253662,
      "learning_rate": 3.2301480484522207e-05,
      "loss": 0.0561,
      "step": 724
    },
    {
      "epoch": 2.5724444444444443,
      "grad_norm": 0.6517682075500488,
      "learning_rate": 3.203230148048452e-05,
      "loss": 0.0194,
      "step": 725
    },
    {
      "epoch": 2.576,
      "grad_norm": 1.3228981494903564,
      "learning_rate": 3.176312247644684e-05,
      "loss": 0.0944,
      "step": 726
    },
    {
      "epoch": 2.5795555555555554,
      "grad_norm": 1.4559507369995117,
      "learning_rate": 3.149394347240915e-05,
      "loss": 0.0474,
      "step": 727
    },
    {
      "epoch": 2.583111111111111,
      "grad_norm": 1.2006840705871582,
      "learning_rate": 3.122476446837147e-05,
      "loss": 0.0506,
      "step": 728
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 1.2069389820098877,
      "learning_rate": 3.095558546433378e-05,
      "loss": 0.0634,
      "step": 729
    },
    {
      "epoch": 2.590222222222222,
      "grad_norm": 1.3170785903930664,
      "learning_rate": 3.0686406460296096e-05,
      "loss": 0.0559,
      "step": 730
    },
    {
      "epoch": 2.5937777777777775,
      "grad_norm": 2.697249412536621,
      "learning_rate": 3.041722745625841e-05,
      "loss": 0.0643,
      "step": 731
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 2.281811475753784,
      "learning_rate": 3.0148048452220727e-05,
      "loss": 0.0914,
      "step": 732
    },
    {
      "epoch": 2.600888888888889,
      "grad_norm": 1.4226083755493164,
      "learning_rate": 2.9878869448183043e-05,
      "loss": 0.0769,
      "step": 733
    },
    {
      "epoch": 2.6044444444444443,
      "grad_norm": 1.1735223531723022,
      "learning_rate": 2.960969044414536e-05,
      "loss": 0.0763,
      "step": 734
    },
    {
      "epoch": 2.608,
      "grad_norm": 1.6049821376800537,
      "learning_rate": 2.9340511440107672e-05,
      "loss": 0.1151,
      "step": 735
    },
    {
      "epoch": 2.6115555555555554,
      "grad_norm": 1.219574213027954,
      "learning_rate": 2.9071332436069988e-05,
      "loss": 0.0762,
      "step": 736
    },
    {
      "epoch": 2.615111111111111,
      "grad_norm": 1.497314214706421,
      "learning_rate": 2.8802153432032304e-05,
      "loss": 0.0553,
      "step": 737
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.3542507588863373,
      "learning_rate": 2.853297442799462e-05,
      "loss": 0.0109,
      "step": 738
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 1.0423471927642822,
      "learning_rate": 2.8263795423956936e-05,
      "loss": 0.0741,
      "step": 739
    },
    {
      "epoch": 2.6257777777777775,
      "grad_norm": 1.7582552433013916,
      "learning_rate": 2.7994616419919245e-05,
      "loss": 0.1203,
      "step": 740
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 1.0029165744781494,
      "learning_rate": 2.772543741588156e-05,
      "loss": 0.0618,
      "step": 741
    },
    {
      "epoch": 2.632888888888889,
      "grad_norm": 1.488563060760498,
      "learning_rate": 2.7456258411843877e-05,
      "loss": 0.0681,
      "step": 742
    },
    {
      "epoch": 2.6364444444444444,
      "grad_norm": 2.3988420963287354,
      "learning_rate": 2.7187079407806193e-05,
      "loss": 0.0719,
      "step": 743
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.4346307516098022,
      "learning_rate": 2.691790040376851e-05,
      "loss": 0.0347,
      "step": 744
    },
    {
      "epoch": 2.6435555555555554,
      "grad_norm": 0.46160688996315,
      "learning_rate": 2.6648721399730825e-05,
      "loss": 0.0171,
      "step": 745
    },
    {
      "epoch": 2.647111111111111,
      "grad_norm": 1.1759883165359497,
      "learning_rate": 2.6379542395693134e-05,
      "loss": 0.1007,
      "step": 746
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 2.4576098918914795,
      "learning_rate": 2.611036339165545e-05,
      "loss": 0.2644,
      "step": 747
    },
    {
      "epoch": 2.6542222222222223,
      "grad_norm": 2.5341856479644775,
      "learning_rate": 2.5841184387617766e-05,
      "loss": 0.1352,
      "step": 748
    },
    {
      "epoch": 2.6577777777777776,
      "grad_norm": 0.8712289333343506,
      "learning_rate": 2.5572005383580082e-05,
      "loss": 0.0746,
      "step": 749
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 1.5071669816970825,
      "learning_rate": 2.5302826379542398e-05,
      "loss": 0.0885,
      "step": 750
    },
    {
      "epoch": 2.664888888888889,
      "grad_norm": 1.3730664253234863,
      "learning_rate": 2.503364737550471e-05,
      "loss": 0.0897,
      "step": 751
    },
    {
      "epoch": 2.6684444444444444,
      "grad_norm": 1.3312499523162842,
      "learning_rate": 2.4764468371467026e-05,
      "loss": 0.1595,
      "step": 752
    },
    {
      "epoch": 2.672,
      "grad_norm": 2.7819626331329346,
      "learning_rate": 2.4495289367429342e-05,
      "loss": 0.1494,
      "step": 753
    },
    {
      "epoch": 2.6755555555555555,
      "grad_norm": 1.0173486471176147,
      "learning_rate": 2.422611036339166e-05,
      "loss": 0.0437,
      "step": 754
    },
    {
      "epoch": 2.679111111111111,
      "grad_norm": 0.6546502113342285,
      "learning_rate": 2.395693135935397e-05,
      "loss": 0.0159,
      "step": 755
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 1.4419037103652954,
      "learning_rate": 2.3687752355316287e-05,
      "loss": 0.058,
      "step": 756
    },
    {
      "epoch": 2.6862222222222223,
      "grad_norm": 1.09169340133667,
      "learning_rate": 2.3418573351278603e-05,
      "loss": 0.0544,
      "step": 757
    },
    {
      "epoch": 2.6897777777777776,
      "grad_norm": 2.1763932704925537,
      "learning_rate": 2.3149394347240915e-05,
      "loss": 0.0978,
      "step": 758
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 1.1241074800491333,
      "learning_rate": 2.288021534320323e-05,
      "loss": 0.0484,
      "step": 759
    },
    {
      "epoch": 2.696888888888889,
      "grad_norm": 2.5885021686553955,
      "learning_rate": 2.2611036339165544e-05,
      "loss": 0.1051,
      "step": 760
    },
    {
      "epoch": 2.7004444444444444,
      "grad_norm": 1.4494290351867676,
      "learning_rate": 2.234185733512786e-05,
      "loss": 0.0959,
      "step": 761
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 1.9756511449813843,
      "learning_rate": 2.2072678331090176e-05,
      "loss": 0.1129,
      "step": 762
    },
    {
      "epoch": 2.7075555555555555,
      "grad_norm": 1.0369113683700562,
      "learning_rate": 2.180349932705249e-05,
      "loss": 0.092,
      "step": 763
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 1.295352578163147,
      "learning_rate": 2.1534320323014804e-05,
      "loss": 0.0511,
      "step": 764
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 0.764076828956604,
      "learning_rate": 2.126514131897712e-05,
      "loss": 0.0467,
      "step": 765
    },
    {
      "epoch": 2.7182222222222223,
      "grad_norm": 2.6328415870666504,
      "learning_rate": 2.0995962314939436e-05,
      "loss": 0.0639,
      "step": 766
    },
    {
      "epoch": 2.7217777777777776,
      "grad_norm": 0.5707887411117554,
      "learning_rate": 2.0726783310901752e-05,
      "loss": 0.0406,
      "step": 767
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.9235355854034424,
      "learning_rate": 2.0457604306864065e-05,
      "loss": 0.0964,
      "step": 768
    },
    {
      "epoch": 2.728888888888889,
      "grad_norm": 0.38006341457366943,
      "learning_rate": 2.018842530282638e-05,
      "loss": 0.0146,
      "step": 769
    },
    {
      "epoch": 2.7324444444444445,
      "grad_norm": 1.7487127780914307,
      "learning_rate": 1.9919246298788697e-05,
      "loss": 0.1509,
      "step": 770
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 1.4876567125320435,
      "learning_rate": 1.965006729475101e-05,
      "loss": 0.0961,
      "step": 771
    },
    {
      "epoch": 2.7395555555555555,
      "grad_norm": 1.587349534034729,
      "learning_rate": 1.9380888290713325e-05,
      "loss": 0.1197,
      "step": 772
    },
    {
      "epoch": 2.7431111111111113,
      "grad_norm": 1.5937870740890503,
      "learning_rate": 1.911170928667564e-05,
      "loss": 0.0913,
      "step": 773
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.8922122120857239,
      "learning_rate": 1.8842530282637954e-05,
      "loss": 0.0623,
      "step": 774
    },
    {
      "epoch": 2.7502222222222223,
      "grad_norm": 1.204110026359558,
      "learning_rate": 1.857335127860027e-05,
      "loss": 0.0994,
      "step": 775
    },
    {
      "epoch": 2.7537777777777777,
      "grad_norm": 0.7135637402534485,
      "learning_rate": 1.8304172274562582e-05,
      "loss": 0.0425,
      "step": 776
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.49945250153541565,
      "learning_rate": 1.80349932705249e-05,
      "loss": 0.0139,
      "step": 777
    },
    {
      "epoch": 2.7608888888888887,
      "grad_norm": 1.7274045944213867,
      "learning_rate": 1.7765814266487214e-05,
      "loss": 0.1005,
      "step": 778
    },
    {
      "epoch": 2.7644444444444445,
      "grad_norm": 1.0422323942184448,
      "learning_rate": 1.749663526244953e-05,
      "loss": 0.0603,
      "step": 779
    },
    {
      "epoch": 2.768,
      "grad_norm": 1.285196304321289,
      "learning_rate": 1.7227456258411846e-05,
      "loss": 0.0359,
      "step": 780
    },
    {
      "epoch": 2.7715555555555556,
      "grad_norm": 1.6139415502548218,
      "learning_rate": 1.6958277254374162e-05,
      "loss": 0.1075,
      "step": 781
    },
    {
      "epoch": 2.7751111111111113,
      "grad_norm": 1.3802398443222046,
      "learning_rate": 1.6689098250336475e-05,
      "loss": 0.0809,
      "step": 782
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.6723813414573669,
      "learning_rate": 1.641991924629879e-05,
      "loss": 0.0304,
      "step": 783
    },
    {
      "epoch": 2.7822222222222224,
      "grad_norm": 1.2016538381576538,
      "learning_rate": 1.6150740242261103e-05,
      "loss": 0.0956,
      "step": 784
    },
    {
      "epoch": 2.7857777777777777,
      "grad_norm": 1.5321441888809204,
      "learning_rate": 1.588156123822342e-05,
      "loss": 0.069,
      "step": 785
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 3.3806099891662598,
      "learning_rate": 1.5612382234185735e-05,
      "loss": 0.1113,
      "step": 786
    },
    {
      "epoch": 2.7928888888888888,
      "grad_norm": 1.0809357166290283,
      "learning_rate": 1.5343203230148048e-05,
      "loss": 0.0926,
      "step": 787
    },
    {
      "epoch": 2.7964444444444445,
      "grad_norm": 2.295201539993286,
      "learning_rate": 1.5074024226110364e-05,
      "loss": 0.0811,
      "step": 788
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.5873521566390991,
      "learning_rate": 1.480484522207268e-05,
      "loss": 0.1303,
      "step": 789
    },
    {
      "epoch": 2.8035555555555556,
      "grad_norm": 1.0373404026031494,
      "learning_rate": 1.4535666218034994e-05,
      "loss": 0.072,
      "step": 790
    },
    {
      "epoch": 2.8071111111111113,
      "grad_norm": 0.48574501276016235,
      "learning_rate": 1.426648721399731e-05,
      "loss": 0.0233,
      "step": 791
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 3.3036069869995117,
      "learning_rate": 1.3997308209959623e-05,
      "loss": 0.1091,
      "step": 792
    },
    {
      "epoch": 2.814222222222222,
      "grad_norm": 0.8105558753013611,
      "learning_rate": 1.3728129205921938e-05,
      "loss": 0.0359,
      "step": 793
    },
    {
      "epoch": 2.8177777777777777,
      "grad_norm": 0.3033435046672821,
      "learning_rate": 1.3458950201884254e-05,
      "loss": 0.0119,
      "step": 794
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 1.599869728088379,
      "learning_rate": 1.3189771197846567e-05,
      "loss": 0.0407,
      "step": 795
    },
    {
      "epoch": 2.824888888888889,
      "grad_norm": 1.9337413311004639,
      "learning_rate": 1.2920592193808883e-05,
      "loss": 0.1055,
      "step": 796
    },
    {
      "epoch": 2.8284444444444445,
      "grad_norm": 0.851069450378418,
      "learning_rate": 1.2651413189771199e-05,
      "loss": 0.0361,
      "step": 797
    },
    {
      "epoch": 2.832,
      "grad_norm": 2.3414368629455566,
      "learning_rate": 1.2382234185733513e-05,
      "loss": 0.136,
      "step": 798
    },
    {
      "epoch": 2.8355555555555556,
      "grad_norm": 0.8602596521377563,
      "learning_rate": 1.211305518169583e-05,
      "loss": 0.0253,
      "step": 799
    },
    {
      "epoch": 2.8391111111111114,
      "grad_norm": 1.9483788013458252,
      "learning_rate": 1.1843876177658143e-05,
      "loss": 0.1049,
      "step": 800
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 1.5762876272201538,
      "learning_rate": 1.1574697173620458e-05,
      "loss": 0.0987,
      "step": 801
    },
    {
      "epoch": 2.846222222222222,
      "grad_norm": 1.7302523851394653,
      "learning_rate": 1.1305518169582772e-05,
      "loss": 0.169,
      "step": 802
    },
    {
      "epoch": 2.8497777777777777,
      "grad_norm": 2.157559394836426,
      "learning_rate": 1.1036339165545088e-05,
      "loss": 0.1704,
      "step": 803
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 1.1580021381378174,
      "learning_rate": 1.0767160161507402e-05,
      "loss": 0.1038,
      "step": 804
    },
    {
      "epoch": 2.856888888888889,
      "grad_norm": 1.7151577472686768,
      "learning_rate": 1.0497981157469718e-05,
      "loss": 0.103,
      "step": 805
    },
    {
      "epoch": 2.8604444444444446,
      "grad_norm": 1.5948514938354492,
      "learning_rate": 1.0228802153432032e-05,
      "loss": 0.0985,
      "step": 806
    },
    {
      "epoch": 2.864,
      "grad_norm": 1.3522173166275024,
      "learning_rate": 9.959623149394348e-06,
      "loss": 0.0623,
      "step": 807
    },
    {
      "epoch": 2.8675555555555556,
      "grad_norm": 1.4637513160705566,
      "learning_rate": 9.690444145356663e-06,
      "loss": 0.0832,
      "step": 808
    },
    {
      "epoch": 2.871111111111111,
      "grad_norm": 0.70966637134552,
      "learning_rate": 9.421265141318977e-06,
      "loss": 0.0393,
      "step": 809
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.7374342083930969,
      "learning_rate": 9.152086137281291e-06,
      "loss": 0.0252,
      "step": 810
    },
    {
      "epoch": 2.878222222222222,
      "grad_norm": 1.3639509677886963,
      "learning_rate": 8.882907133243607e-06,
      "loss": 0.121,
      "step": 811
    },
    {
      "epoch": 2.8817777777777778,
      "grad_norm": 1.4746707677841187,
      "learning_rate": 8.613728129205923e-06,
      "loss": 0.1052,
      "step": 812
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 0.6467505097389221,
      "learning_rate": 8.344549125168237e-06,
      "loss": 0.0307,
      "step": 813
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 1.5288777351379395,
      "learning_rate": 8.075370121130552e-06,
      "loss": 0.1046,
      "step": 814
    },
    {
      "epoch": 2.8924444444444446,
      "grad_norm": 1.4378036260604858,
      "learning_rate": 7.806191117092868e-06,
      "loss": 0.062,
      "step": 815
    },
    {
      "epoch": 2.896,
      "grad_norm": 1.0199180841445923,
      "learning_rate": 7.537012113055182e-06,
      "loss": 0.064,
      "step": 816
    },
    {
      "epoch": 2.8995555555555557,
      "grad_norm": 1.5433858633041382,
      "learning_rate": 7.267833109017497e-06,
      "loss": 0.0612,
      "step": 817
    },
    {
      "epoch": 2.903111111111111,
      "grad_norm": 0.7451590299606323,
      "learning_rate": 6.998654104979811e-06,
      "loss": 0.0294,
      "step": 818
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.7944672703742981,
      "learning_rate": 6.729475100942127e-06,
      "loss": 0.0747,
      "step": 819
    },
    {
      "epoch": 2.910222222222222,
      "grad_norm": 0.7063425183296204,
      "learning_rate": 6.4602960969044415e-06,
      "loss": 0.0437,
      "step": 820
    },
    {
      "epoch": 2.913777777777778,
      "grad_norm": 1.0499048233032227,
      "learning_rate": 6.191117092866757e-06,
      "loss": 0.1501,
      "step": 821
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 1.368249535560608,
      "learning_rate": 5.921938088829072e-06,
      "loss": 0.0993,
      "step": 822
    },
    {
      "epoch": 2.920888888888889,
      "grad_norm": 1.302138328552246,
      "learning_rate": 5.652759084791386e-06,
      "loss": 0.0679,
      "step": 823
    },
    {
      "epoch": 2.924444444444444,
      "grad_norm": 1.3973259925842285,
      "learning_rate": 5.383580080753701e-06,
      "loss": 0.1193,
      "step": 824
    },
    {
      "epoch": 2.928,
      "grad_norm": 1.347078800201416,
      "learning_rate": 5.114401076716016e-06,
      "loss": 0.0975,
      "step": 825
    },
    {
      "epoch": 2.9315555555555557,
      "grad_norm": 0.9939260482788086,
      "learning_rate": 4.845222072678331e-06,
      "loss": 0.0562,
      "step": 826
    },
    {
      "epoch": 2.935111111111111,
      "grad_norm": 1.2278351783752441,
      "learning_rate": 4.576043068640646e-06,
      "loss": 0.093,
      "step": 827
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 1.4076682329177856,
      "learning_rate": 4.3068640646029616e-06,
      "loss": 0.0583,
      "step": 828
    },
    {
      "epoch": 2.942222222222222,
      "grad_norm": 3.2241780757904053,
      "learning_rate": 4.037685060565276e-06,
      "loss": 0.0596,
      "step": 829
    },
    {
      "epoch": 2.945777777777778,
      "grad_norm": 2.3728153705596924,
      "learning_rate": 3.768506056527591e-06,
      "loss": 0.0976,
      "step": 830
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 1.6735824346542358,
      "learning_rate": 3.4993270524899056e-06,
      "loss": 0.1244,
      "step": 831
    },
    {
      "epoch": 2.952888888888889,
      "grad_norm": 2.6341230869293213,
      "learning_rate": 3.2301480484522207e-06,
      "loss": 0.1132,
      "step": 832
    },
    {
      "epoch": 2.956444444444444,
      "grad_norm": 0.9413036704063416,
      "learning_rate": 2.960969044414536e-06,
      "loss": 0.0776,
      "step": 833
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.8890191912651062,
      "learning_rate": 2.6917900403768505e-06,
      "loss": 0.0339,
      "step": 834
    },
    {
      "epoch": 2.9635555555555557,
      "grad_norm": 1.1422797441482544,
      "learning_rate": 2.4226110363391657e-06,
      "loss": 0.0339,
      "step": 835
    },
    {
      "epoch": 2.967111111111111,
      "grad_norm": 1.1979786157608032,
      "learning_rate": 2.1534320323014808e-06,
      "loss": 0.058,
      "step": 836
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 0.5336737036705017,
      "learning_rate": 1.8842530282637955e-06,
      "loss": 0.0227,
      "step": 837
    },
    {
      "epoch": 2.974222222222222,
      "grad_norm": 0.7785890698432922,
      "learning_rate": 1.6150740242261104e-06,
      "loss": 0.0827,
      "step": 838
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 1.4767528772354126,
      "learning_rate": 1.3458950201884253e-06,
      "loss": 0.1009,
      "step": 839
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 2.068991184234619,
      "learning_rate": 1.0767160161507404e-06,
      "loss": 0.2074,
      "step": 840
    },
    {
      "epoch": 2.984888888888889,
      "grad_norm": 1.9891022443771362,
      "learning_rate": 8.075370121130552e-07,
      "loss": 0.1202,
      "step": 841
    },
    {
      "epoch": 2.9884444444444442,
      "grad_norm": 0.940477728843689,
      "learning_rate": 5.383580080753702e-07,
      "loss": 0.0452,
      "step": 842
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.7552913427352905,
      "learning_rate": 2.691790040376851e-07,
      "loss": 0.017,
      "step": 843
    }
  ],
  "logging_steps": 1,
  "max_steps": 843,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0390369947771494e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
